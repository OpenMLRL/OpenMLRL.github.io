[{"id":0,"href":"/docs/examples/quick-demo/","title":"CoMLRL Quick Demo","section":"Examples","content":"This tutorial demonstrates how to train two LLM agents to collaborate to tell a story. The first agent generates a compact story setup, while the second agent produces a longer version. The reward function encourages the second agent\u0026rsquo;s output to be 2–3× longer than the first agent\u0026rsquo;s.\nTo run this demo, please have at least 24 GB of GPU memory available. You can also visualize the training process by setting up your WandB dashboard.\nImport Libraries# import math from functools import partial from datasets import Dataset from transformers import AutoModelForCausalLM, AutoTokenizer from comlrl.utils.reward_processor import RewardProcessors from comlrl.trainers.magrpo import MAGRPOConfig, MAGRPOTrainerDataset Preparation# We first create a dataset of creative prompts for the agents to work on.\ntrain_data = { \u0026#34;prompt\u0026#34;: [ \u0026#34;Describe a city in the clouds:\u0026#34;, \u0026#34;Invent a new holiday and explain it:\u0026#34;, \u0026#34;Write a bedtime story for a dragon:\u0026#34;, \u0026#34;Explain how teleportation might work:\u0026#34;, \u0026#34;Tell a joke about dinosaurs:\u0026#34;, \u0026#34;Describe a world without electricity:\u0026#34;, \u0026#34;Create a superhero with a unique power:\u0026#34;, \u0026#34;Write a scene where the moon talks:\u0026#34;, \u0026#34;Invent a new type of fruit:\u0026#34;, \u0026#34;Design a playground on Mars:\u0026#34;, ] } train_dataset = Dataset.from_dict(train_data)Agent Initialization# We load a tokenizer to convert text into tokens that the model can process and initialize two separate instances.\nmodel_name = \u0026#34;Qwen/Qwen2.5-0.5B\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_name) agents = [AutoModelForCausalLM.from_pretrained(model_name) for _ in range(2)]Define the Reward Function# The reward function measures how well the agents collaborate. It gives maximum reward (1.0) when the second agent\u0026rsquo;s output is 2–3× longer than the first agent\u0026rsquo;s. If the length ratio falls outside this range, the reward decays exponentially based on how far it deviates.\ndef proper_length_ratio_reward( completions1, completions2, target_min=2.0, target_max=3.0 ): rewards = [] for c1, c2 in zip(completions1, completions2): len1, len2 = len(c1), len(c2) if len1 == 0: rewards.append(0.0) continue ratio = len2 / len1 if target_min \u0026lt;= ratio \u0026lt;= target_max: reward = 1.0 else: if ratio \u0026lt; target_min: distance = target_min - ratio else: distance = ratio - target_max reward = math.exp(-distance) rewards.append(float(reward)) return rewardsConfigure Training# We set up the training configuration with hyperparameters like learning rate, batch size, and the number of generations each agent produces per prompt.\nconfig = MAGRPOConfig( output_dir=\u0026#34;./magrpo_multi_reward_output\u0026#34;, num_train_epochs=3, per_device_train_batch_size=1, learning_rate=5e-5, logging_steps=10, save_steps=100, num_generations=8, max_new_tokens=128, )Create the Trainer# We instantiate the MAGRPO trainer with our agents, reward function, and configuration. The reward is scaled by 100× to provide a stronger learning signal.\nwandb_config = { \u0026#34;project\u0026#34;: \u0026lt;your-project-name\u0026gt;, \u0026#34;entity\u0026#34;: \u0026lt;your-entity-name\u0026gt;, \u0026#34;name\u0026#34;: \u0026#34;length-ratio-demo\u0026#34;, } configured_reward_func = partial( proper_length_ratio_reward, target_min=2, target_max=3 ) trainer = MAGRPOTrainer( agents=agents, reward_func=configured_reward_func, reward_processor=RewardProcessors.scale(factor=100.0), args=config, train_dataset=train_dataset, tokenizer=tokenizer, wandb_config=wandb_config, )Run Training# Finally, we start the training process. The trainer will optimize both agents to maximize the collaborative reward, then save the trained models.\ntrainer.train() trainer.save_model(f\u0026#34;{config.output_dir}/models\u0026#34;)"},{"id":1,"href":"/docs/user-guide/installation/","title":"Installation","section":"User Guide","content":"You can create a venv or conda environment with Python 3.10+ and install CoMLRL as follows.\nInstall from PyPI# pip install comlrl # Install PyTorch compatible with your deviceInstall from conda-forge# conda install -c conda-forge comlrl # Install PyTorch compatible with your deviceInstall from source# To access the latest features of CoMLRL, clone this repository and install in editable mode:\ngit clone https://github.com/OpenMLRL/CoMLRL.git cd CoMLRL pip install -e . # Install PyTorch compatible with your device"},{"id":2,"href":"/docs/dev/support/","title":"Support","section":"Developers","content":"We are willing to help you with any issues you encounter while using CoMLRL.\nReport Issues# If you are stuck with a problem using CoMLRL, please follow this procedure:\nRead the documentation first, including using the search feature (Ctrl + K).\nSearch the GitHub Issues archives to see if someone else already had the same problem.\nBefore writing, try to create a minimal example that reproduces the problem. You\u0026rsquo;ll get the fastest response if you can send just a handful of lines of code that show what isn\u0026rsquo;t working.\nCitation# Please cite our paper if you find this library useful in your research:\n@misc{liu2025comlrl, title={LLM Collaboration With Multi-Agent Reinforcement Learning}, author={Shuo Liu and Tianle Chen and Zeyu Liang and Xueguang Lyu and Christopher Amato}, year={2025}, eprint={2508.04652}, archivePrefix={arXiv}, primaryClass={cs.AI}, url={https://arxiv.org/abs/2508.04652}, }"},{"id":3,"href":"/docs/dev/contributing/","title":"Contributing","section":"Developers","content":"Thanks for your interest in helping build CoMLRL! This guide walks you through reporting issues, contributing changes, and keeping the codebase healthy.\nDevelopment Guidelines# Fork the upstream repository. Clone your fork and synchronize with upstream: git clone https://github.com/\u0026lt;your-username\u0026gt;/CoMLRL.git cd CoMLRL git remote add upstream https://github.com/OpenMLRL/CoMLRL.git git fetch upstream git checkout -b feature/\u0026lt;short-description\u0026gt; upstream/main git fetch upstream \u0026amp;\u0026amp; git rebase upstream/main Implement new features or fix bugs, updating documentation as needed. Open a pull request to the upstream repository and wait for review. "},{"id":4,"href":"/docs/user-guide/reinforce-finetuning/","title":"Multi-Agent REINFORCE","section":"User Guide","content":"REINFORCE optimizes the policy directly using sampled returns. An action-independent baseline can be included to reduce variance for REINFORCE methods. REINFORCE methods have been widely used to fine-tune LLMs because of their simplicity and effectiveness, e.g., GRPO, Dr. GRPO, RLOO, ReMax, TreeRPO, and REINFORCE++.\nMAREINFORCE# In the LLM collaboration setting, REINFORCE can be extended to optimize each agent\u0026rsquo;s policy with joint returns from multiple agents.\nMAREINFORCE: The naive Multi‑Agent REINFORCE without a baseline can be expressed by: \\[ J(\\theta_i) = \\mathbb{E}_{\\mathbf{o}_0 \\sim \\mathcal{D}, \\mathbf{h}^\\mathcal{G} \\sim \\mathbf{\\pi}_{\\mathbf{\\theta}}} \\Bigg[\\frac{1}{|\\mathcal{G}|}\\sum_{g \\in \\mathcal{G}} R^{(g)}_t \\cdot \\log \\pi_{\\theta_i}(a^{(g)}_{i,t}\\mid h_{i,t})\\Bigg]. \\] These classes are derived from comlrl.trainers.magrpo.MAGRPOTrainer. Interfaces for the trainer and configuration classes are the same as MAGRPOTrainer and MAGRPOConfig.\nMAGRPO# Multi‑Agent Group‑Relative Policy Optimization optimizes each agent with a group‑relative baseline computed among sibling joint actions at the same node.\n\\[ J(\\theta_i) = \\mathbb{E}_{\\mathbf{o}_0 \\sim \\mathcal{D}, \\mathbf{h}^\\mathcal{G} \\sim \\mathbf{\\pi}_{\\mathbf{\\theta}}}\\left[ \\frac{1}{|\\mathcal{G}|}\\sum_{g \\in \\mathcal{G}} \\Big(R^{(g)}_t - \\operatorname{mean}(R^{\\mathcal{G}}_t)\\Big) \\cdot \\log \\pi_{\\theta_i}\\big(a^{(g)}_{i,t} \\mid h_{i,t}\\big) \\right]. \\] MAGRPOConfig inherits from TrainingArguments and provides parameters for both single-turn and multi-turn training:\nnum_agents: Number of agents (default: 2) num_generations: Number of generations to sample per prompt for each agent (default: 4) max_new_tokens: Maximum number of new tokens to generate (default: 256) temperature: Temperature for sampling (default: 0.7) top_p: Top-p for sampling (default: 0.9) num_turns: Number of turns per episode; set \u0026gt;1 for multi-turn (default: 1) discount: Discount factor gamma over turns for returns (default: 0.9) joint_mode: Joint action composition - 'aligned' (index-aligned, default) or 'cross' (Cartesian product) termination_threshold: Early stop a branch if mean reward exceeds this threshold (default: None) eval_interval: Run evaluation every N training batches (default: 4) eval_num_samples: Number of samples to evaluate per evaluation run (default: 4) MAGRPOTrainer accepts either a model string/object for homogeneous agents or a list of agents for heterogeneous setups:\nmodel or agents: Model string/object for homogeneous agents, or list of agent models num_agents: Number of agents (default: 2) tokenizer: The tokenizer (required) train_dataset: Training dataset (required) reward_func: Callable that returns a list of floats (required) reward_processor: Optional processor to apply to rewards (e.g., scaling) formatters: Single callable or list of callables for each agent to format dataset items into prompts external_transition: Function providing transitions between turns (required for multi-turn training) eval_dataset: Evaluation dataset (optional) eval_logger: Evaluation logger function (optional) eval_aggregator: Evaluation aggregator function (optional) wandb_config: Configuration for Weights \u0026amp; Biases logging (optional) model_config: Model configuration dict (optional) args: Instance of MAGRPOConfig (optional) CoMLRL implements on-policy GRPO, which computes the policy gradient using the current policy\u0026rsquo;s samples without importance sampling or ratio clipping.\nThe trainer enforces per_device_train_batch_size=1 and requires at least 2 generations for group baseline computation.\nOther Variants# CoMLRL also implements other Multi-Agent REINFORCE variants with different baselines:\nMARLOO: Multi‑Agent REINFORCE Leave‑One‑Out. Baseline is the mean return of other agents (leave‑one‑out) at the same step. \\[ J(\\theta_i) = \\mathbb{E}_{\\mathbf{o}_0 \\sim \\mathcal{D}, \\mathbf{h}^\\mathcal{G} \\sim \\mathbf{\\pi}_{\\mathbf{\\theta}}} \\Bigg[\\frac{1}{|\\mathcal{G}|}\\sum_{g \\in \\mathcal{G}} \\Big( R^{(g)}_t - \\sum_{k\\in \\mathcal{G},\\, k\\neq g}\\tfrac{R^{(k)}_t}{|\\mathcal{G}|-1} \\Big) \\cdot \\log \\pi_{\\theta_i}(a^{(g)}_{i,t}\\mid h_{i,t}) \\Bigg]; \\] MAREMAX: Multi‑Agent REINFORCE with Group Max. Baseline is the maximum group return at the step. \\[ J(\\theta_i) = \\mathbb{E}_{\\mathbf{o}_0 \\sim \\mathcal{D}, \\mathbf{h}^\\mathcal{G} \\sim \\mathbf{\\pi}_{\\mathbf{\\theta}}} \\Bigg[\\frac{1}{|\\mathcal{G}|}\\sum_{g \\in \\mathcal{G}} \\Big( R^{(g)}_t - \\max(R_t^{\\mathcal{G}}) \\Big) \\cdot \\log \\pi_{\\theta_i}(a^{(g)}_{i,t}\\mid h_{i,t}) \\Bigg]. \\] These classes are derived from comlrl.trainers.magrpo.MAGRPOTrainer. Interfaces for the trainer and configuration classes are the same as MAGRPOTrainer and MAGRPOConfig.\n"},{"id":5,"href":"/docs/dev/changelog/","title":"Changelog","section":"Developers","content":" Version 2.0.0# First release of CoMLRL.\n"},{"id":6,"href":"/docs/user-guide/ppo-finetuning/","title":"Multi-Agent PPO","section":"User Guide","content":"PPO is a widely used policy gradient method that employs generalized advantage estimation to estimate advantages, reducing the high variance and long rollout times in Monte Carlo methods, e.g., REINFORCE. PPO has also been used for LLM fine-tuning, e.g., trl, verl, LLaMA Factory.\nIPPO# Independent PPO (IPPO) optimizes each agent\u0026rsquo;s policy independently while using joint returns from multiple agents. Each agent maintains its own actor and critic, other agents serve as part of the environment. The policy objective is:\n\\[ J(\\theta_i) = \\mathbb{E}_{o_{i,0} \\sim \\mathcal{D}, h_i \\sim \\pi_{\\theta_i}}\\left[\\log \\pi_{\\theta_i}(a_{i,t}|h_{i,t}) \\cdot \\sum_{l=0}^{\\infty} (\\gamma\\lambda)^l \\delta_{i,t\u0026#43;l} \u0026#43; \\beta \\mathcal{H}(\\pi_{\\theta_i})\\right] \\] where \\( \\delta_{i,t} = r_{i,t} \u0026#43; \\gamma V_{\\phi_i}(h_{i,t\u0026#43;1}) - V_{\\phi_i}(h_{i,t}) \\) is the temporal difference error, \\( \\gamma \\) is the discount factor, \\( \\lambda \\) is the GAE parameter that balances bias and variance, and \\( \\mathcal{H}(\\pi_{\\theta_i}) \\) is the entropy bonus with coefficient \\( \\beta \\).\nCoMLRL supports two IPPO architectures for critic implementation:\nSeparate Critic: Uses an independent model dedicated to value estimation, completely separate from the actor. It provides more stable training but requires longer training time and larger VRAM usage.\nValue Head: Attaches a small value prediction head directly to the actor model, sharing the base model\u0026rsquo;s representations. It reduces VRAM usage, but since both actor and critic share the same model, gradient errors can be amplified during training.\nIPPOConfig provides parameters for configuring the PPO training:\noutput_dir: Directory to save outputs actor_learning_rate: Learning rate for actor critic_learning_rate: Learning rate for critic weight_decay: Weight decay for AdamW optimizer adam_beta1, adam_beta2, adam_epsilon: Adam optimizer parameters max_grad_norm: Maximum gradient norm for clipping rollout_buffer_size: Number of samples to collect before update mini_batch_size: Mini-batch size for PPO updates ppo_epochs: Number of optimization epochs per rollout value_clip_range: Clipping range for value function value_loss_coef: Coefficient for value loss entropy_coef: Coefficient for entropy bonus advantage_normalization: Whether to normalize advantages max_new_tokens: Maximum new tokens to generate temperature: Temperature for sampling top_p: Top-p for nucleus sampling top_k: Top-k for sampling do_sample: Whether to use sampling num_train_epochs: Number of training epochs per_device_train_batch_size: Batch size per device, must be 1 use_separate_critic: Whether to use separate critic model critic_model_name_or_path: Model identifier for separate critic critic_value_head_hidden_dim: Hidden dimension for critic value head value_head_hidden_dim: Hidden dimension for actor value head num_agents: Number of agents num_turns: Number of turns, currently only supports 1 reward_norm_eps: Epsilon for reward normalization IPPOTrainer trains agents using Independent PPO:\nmodel: Model string or PreTrainedModel instance (required for single-agent, must be string for multi-agent) tokenizer: The tokenizer (required) reward_func: Callable that returns a list of floats (required) reward_processor: Optional processor to apply to rewards formatters: Single callable or list of callables for each agent to format dataset items into prompts args: Instance of IPPOConfig (optional) train_dataset: Training dataset (required) eval_dataset: Evaluation dataset (optional) model_config: Model configuration dict (optional) wandb_config: Configuration for Weights \u0026amp; Biases logging (optional) metrics_callback: Optional callback for custom metrics CoMLRL implements on-policy IPPO, which computes the policy gradient using the current policy\u0026rsquo;s samples without importance sampling or ratio clipping.\nThe trainer enforces per_device_train_batch_size=1 and currently only supports single-turn training (num_turns=1).\n"},{"id":7,"href":"/docs/user-guide/multi-turn/","title":"Multi-Turn Training","section":"User Guide","content":"Many complex problems cannot be solved in a single turn. Agents need to interact with the environment to obtain useful feedback from other models or tools involved in the system, enabling iterative refinement and exploration of multiple solution paths.\nMulti-Turn MAGRPO# MAGRPO in the multi-turn setting (MAGRPO-MT) forms a tree-structured rollout expansion where branches represent different joint responses (TreeRPO).\nIn each episode, a task is sampled from the dataset to construct initial observations \\( \\mathbf{o}_0=\\{o_{1, 0}, \\cdots, o_{n, 0}\\} \\) and histories \\( \\mathbf{h}_0=\\{h_{1, 0}, \\cdots, h_{n, 0}\\} \\) for all agents. At each turn, agents generate a group of joint responses \\( \\mathbf{a}^{\\mathcal{G}}_t\\gets\\boldsymbol{\\pi}^{\\mathcal{G}}(\\cdot|\\mathbf{h}_t) \\) from their current observation-action history \\( \\mathbf{h}_t \\), with each response initiating a distinct rollout. Agents receive joint rewards \\( r^{(g)}_{t} \\) for each response based on the accumulated history \\( \\mathbf{a}^{(g)}_{t} \\in \\mathbf{a}^{\\mathcal{G}}_{t} \\) and current action. Each rollout then evolves independently, producing new joint observations \\( \\mathbf{o}^{\\mathcal{G}}_{t\u0026#43;1} \\) as the environment dynamics unfold and spawning more rollouts at the next turn \\( t\u0026#43;1 \\). This process continues until the terminal turn is reached \\( H \\).\nJoint Mode# MAGRPO supports two modes for forming joint responses at each turn:\nAlign: Provides flexibility in the number of joint responses generated per turn, allowing any number of generations at each turn. However, generations are not fully utilized since only aligned responses across agents are combined. As training progresses over \\( T \\) turns with \\( N \\) agents, the total number of leaves grows as \\( G^T \\), where \\( G \\) is the number of generations per turn. Cross: Maximizes the utilization of generations and provides more accurate value estimation with more samples by forming the Cartesian product of all agent responses. As training progresses over \\( T \\) turns with \\( N \\) agents, the total number of leaves grows as \\( G^{N \\cdot T} \\), where each node has \\( G^N \\) sibling joint actions. Note that only responses originating from the same rollout can be combined, as rollouts evolve independently.\nEnvironment Transition# External feedback mechanisms control how environment observations are incorporated into prompts for subsequent turns.\nCustom External Feedback# Users can implement custom external feedback by defining a function with the following interface:\nCustom External Feedback Interface:\nprompt: Original task prompt/problem description (required) agent_completions: List or tuple of completions from the previous turn, one per agent (required) num_agents: Number of agents in the system (required) prompt_history_per_agent: List of prompt histories for each agent, where each history is a list of prompts from previous turns (optional) response_history_per_agent: List of response histories for each agent, where each history is a list of responses from previous turns (optional) **kwargs: Additional mode-specific parameters (optional) The function should return a list or tuple of formatted prompts for the next turn, one for each agent.\nFor example:\ndef custom_external( prompt: str, agent_completions: List[str], num_agents: int, prompt_history_per_agent: Optional[List[List[str]]] = None, response_history_per_agent: Optional[List[List[str]]] = None, **kwargs ) -\u0026gt; List[str]: # Custom logic to format next-turn prompts # Access environment feedback, tool outputs, etc. next_turn_prompts = [] for i in range(num_agents): # Format prompt for agent i based on history and feedback next_prompt = f\u0026#34;{prompt}\\nPrevious attempt: {agent_completions[i]}\\nPlease revise.\u0026#34; next_turn_prompts.append(next_prompt) return next_turn_promptsThis interface allows full flexibility in how environment feedback, tool outputs, or other contextual information is integrated into the multi-turn training loop.\nExample Modes (Expert, Diagnosis, Self-Improvement)# An environment for code generation includes 3 example external transition modes:\nexternal.mode=expert_edits: Uses an external LLM (default: DeepSeek-Coder) to propose code edits. Follow-up prompts include edit suggestions with context from previous turns. It can be configured via expert_model for different experts (e.g., Claude, GPT) when API keys are available.\nexternal.mode=level_feedback: Static AST checks and dynamically executes code to provide diagnosis. The default sandbox test includes the first test; configurable via sandbox_slice to include all tests (0, None, or \u0026lsquo;all\u0026rsquo;), specific number of tests (negative values enabled).\nexternal.mode=plain: Self-improvement mode that just includes prompts and responses in the previous turns and a revision instruction.\n"},{"id":8,"href":"/docs/env/","title":"Environments","section":"Docs","content":"Environments that simulate real-world tasks for training and evaluating LLM collaboration:\nWriting Collaboration: Multiple LLM agents collaborate on processing articles.\nTLDR - Summarizing Reddit posts. ArXiv - Expanding abstracts into introductions. Code Generation: Generate code solutions for programming problems.\nMBPP - Mostly basic python problems. HumanEval - Handwritten evaluation problems CoopHumanEval - HumanEval with cooperative nature. Code Completion: Complete code snippets based on given contexts.\nClassEval - Complete class-level code based on method stubs and docstrings. "}]