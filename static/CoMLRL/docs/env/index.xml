<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Environments on CoMLRL</title><link>/docs/env/</link><description>Recent content in Environments on CoMLRL</description><generator>Hugo</generator><language>en-us</language><atom:link href="/docs/env/index.xml" rel="self" type="application/rss+xml"/><item><title>Article Writing</title><link>/docs/env/writing-collaboration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/env/writing-collaboration/</guid><description>&lt;p&gt;Collaborative summarization and expansion tasks for pairs (or teams) of LLMs.
The reference implementation lives in the
&lt;a href="https://github.com/OpenMLRL/LLM_Collab_Writing"&gt;LLM_Collab_Writing&lt;/a&gt; repository
and includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;TLDR&lt;/strong&gt; – distills Reddit threads into concise summaries.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ArXiv Introductions&lt;/strong&gt; – grows short abstracts into multi-paragraph drafts.&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Code Generation</title><link>/docs/env/code-generation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/env/code-generation/</guid><description>&lt;p&gt;A suite of cooperative programming benchmarks where agents propose, critique, and
refine solutions. The environments shipped in
&lt;a href="https://github.com/OpenMLRL/LLM_Collab_Code_Generation"&gt;LLM_Collab_Code_Generation&lt;/a&gt;
cover:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MBPP&lt;/strong&gt; – mostly basic Python problems for rapid iteration.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HumanEval&lt;/strong&gt; – handwritten tasks from OpenAI for exact-match grading.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CoopHumanEval&lt;/strong&gt; – HumanEval variants that explicitly require collaboration.&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Code Completion</title><link>/docs/env/code-completion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/env/code-completion/</guid><description>&lt;p&gt;Multi-agent autocompletion tasks where each model fills in part of a codebase.
The &lt;a href="https://github.com/OpenMLRL/LLM_Collab_Code_Completion"&gt;LLM_Collab_Code_Completion&lt;/a&gt;
project currently focuses on &lt;strong&gt;ClassEval&lt;/strong&gt;, which asks teams of LLMs to finish
class skeletons based on docstrings and partially implemented methods.&lt;/p&gt;</description></item></channel></rss>