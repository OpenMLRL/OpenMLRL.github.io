<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="This tutorial demonstrates how to train two LLM agents to collaborate to tell a story. The first agent generates a compact story setup, while the second agent produces a longer version. The reward function encourages the second agent’s output to be 2–3× longer than the first agent’s.
To run this demo, please have at least 24 GB of GPU memory available. You can also visualize the training process by setting up your WandB dashboard.
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="/docs/examples/quick-demo/"><meta property="og:site_name" content="CoMLRL"><meta property="og:title" content="CoMLRL Quick Demo"><meta property="og:description" content="This tutorial demonstrates how to train two LLM agents to collaborate to tell a story. The first agent generates a compact story setup, while the second agent produces a longer version. The reward function encourages the second agent’s output to be 2–3× longer than the first agent’s.
To run this demo, please have at least 24 GB of GPU memory available. You can also visualize the training process by setting up your WandB dashboard."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2025-11-10T22:00:29-05:00"><meta itemprop=name content="CoMLRL Quick Demo"><meta itemprop=description content="This tutorial demonstrates how to train two LLM agents to collaborate to tell a story. The first agent generates a compact story setup, while the second agent produces a longer version. The reward function encourages the second agent’s output to be 2–3× longer than the first agent’s.
To run this demo, please have at least 24 GB of GPU memory available. You can also visualize the training process by setting up your WandB dashboard."><meta itemprop=dateModified content="2025-11-10T22:00:29-05:00"><meta itemprop=wordCount content="443"><title>CoMLRL Quick Demo | CoMLRL</title><link rel=icon href=../../../favicon.png><link rel=manifest href=../../../manifest.json><link rel=canonical href=../../../docs/examples/quick-demo/><link rel=stylesheet href=../../../book.min.24e9ec7868251e24f5826abe6a916a5214836089a7e7088dfca0e3a55c2e4f7b.css><script defer src=../../../fuse.min.js></script><script defer src=../../../en.search.min.9f792a4777562ba5e886b116faf0f5e39df89be87377b2ebe9d84a47b8dca4e7.js></script><link rel=stylesheet href=../../../katex/katex.min.css><script defer src=../../../katex/katex.min.js></script><script defer src=../../../katex/auto-render.min.js onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"\\begin{gather}",right:"\\end{gather}",display:!0}],throwOnError:!1})'></script><link rel=stylesheet href=../../../css/sidebar.css><link rel=icon type=image/svg+xml href=../../../img/comlrl-icon.svg><link rel=icon type=image/png href=../../../img/comlrl-icon.png><link rel="shortcut icon" type=image/png href=../../../img/comlrl-icon.png><link rel=apple-touch-icon href=../../../img/comlrl-icon.png><script>document.addEventListener("DOMContentLoaded",function(){try{var t,n,s,o,e=document.querySelector("article.book-article");if(!e)return;if(n=document.querySelector(".book-header h3"),t=n?n.textContent.trim():(document.title||"").split("|")[0].trim(),!t)return;s=e.querySelector("h1"),(!s||s.textContent.trim()!==t)&&(o=document.createElement("h1"),o.textContent=t,e.insertBefore(o,e.firstChild)),function(){var t,e=document.getElementById("omlrl-footer");e||(e=document.createElement("div"),e.id="omlrl-footer",e.className="omlrl-footer",document.body.appendChild(e)),e.textContent="",t=document.createElement("strong"),t.textContent="© OpenMLRL. All rights reserved.",e.appendChild(t)}()}catch{}}),document.addEventListener("keydown",function(e){if(!e.metaKey&&!e.ctrlKey)return;var n,t=(e.key||"").toLowerCase();if(t==="k"){e.preventDefault(),n=document.querySelector(".book-search input"),n&&n.focus();return}if(t==="o"){e.preventDefault(),window.location.href="https://openmlrl.github.io";return}if(t==="g"){e.preventDefault(),window.location.href="https://github.com/OpenMLRL/CoMLRL";return}}),document.addEventListener("DOMContentLoaded",function(){var e,t=document.querySelectorAll("pre");t.forEach(function(e){var t=document.createElement("button");t.className="copy-code-button",t.textContent="Copy",t.addEventListener("click",function(){var n=e.querySelector("code"),s=n.textContent;navigator.clipboard.writeText(s).then(function(){t.textContent="Copied!",setTimeout(function(){t.textContent="Copy"},2e3)})}),e.style.position="relative",e.appendChild(t)}),e=document.createElement("a"),e.href="https://github.com/OpenMLRL/CoMLRL",e.className="github-corner",e.target="_blank",e.rel="noopener noreferrer",e.setAttribute("aria-label","View source on GitHub"),e.innerHTML='<svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><defs><linearGradient id="github-gradient" x1="0%" y1="0%" x2="100%" y2="100%"><stop offset="0%" style="stop-color:#9555af;stop-opacity:1" /><stop offset="100%" style="stop-color:#e091c4;stop-opacity:1" /></linearGradient></defs><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z" fill="url(#github-gradient)"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="#fff" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="#fff" class="octo-body"></path></svg>',document.body.appendChild(e)})</script></head><body dir=ltr class="book-kind-page book-type-docs"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=../../../><img src=../../../img/comlrl-logo.png alt=Logo><span>CoMLRL</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a>User Guide</a><ul><li><a href=../../../docs/user-guide/installation/>Installation</a></li><li><a href=../../../docs/user-guide/reinforce-finetuning/>Multi-Agent REINFORCE</a></li><li><a href=../../../docs/user-guide/ppo-finetuning/>Multi-Agent PPO</a></li><li><a href=../../../docs/user-guide/multi-turn/>Multi-Turn Training</a></li></ul></li><li class=book-section-flat><a>Examples</a><ul><li><a href=../../../docs/examples/quick-demo/ class=active>CoMLRL Quick Demo</a></li></ul></li><li class=book-section-flat><a>Developers</a><ul><li><a href=../../../docs/dev/support/>Support</a></li><li><a href=../../../docs/dev/contributing/>Contributing</a></li><li><a href=../../../docs/dev/changelog/>Changelog</a></li></ul></li><li class=book-section-flat><a href=../../../docs/env/>Environments</a><ul><li><a href=https://github.com/OpenMLRL/LLM_Collab_Writing target=_blank rel=noopener>Article Writing</a></li><li><a href=https://github.com/OpenMLRL/LLM_Collab_Code_Generation target=_blank rel=noopener>Code Generation</a></li><li><a href=https://github.com/OpenMLRL/LLM_Collab_Code_Completion target=_blank rel=noopener>Code Completion</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=../../../icons/menu.svg class=book-icon alt=Menu></label><h3>CoMLRL Quick Demo</h3><label for=toc-control></label></div></header><article class="markdown book-article"><p>This tutorial demonstrates how to train two LLM agents to collaborate to tell a story. The first agent generates a compact story setup, while the second agent produces a longer version. The reward function encourages the second agent&rsquo;s output to be 2–3× longer than the first agent&rsquo;s.</p><p>To run this demo, please have at least 24 GB of GPU memory available. You can also visualize the training process by setting up your WandB dashboard.</p><h2 id=import-libraries>Import Libraries<a class=anchor href=#import-libraries>#</a></h2><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#007020;font-weight:700>import</span> <span style=color:#0e84b5;font-weight:700>math</span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>from</span> <span style=color:#0e84b5;font-weight:700>functools</span> <span style=color:#007020;font-weight:700>import</span> partial
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>from</span> <span style=color:#0e84b5;font-weight:700>datasets</span> <span style=color:#007020;font-weight:700>import</span> Dataset
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>from</span> <span style=color:#0e84b5;font-weight:700>transformers</span> <span style=color:#007020;font-weight:700>import</span> AutoModelForCausalLM, AutoTokenizer
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>from</span> <span style=color:#0e84b5;font-weight:700>comlrl.utils.reward_processor</span> <span style=color:#007020;font-weight:700>import</span> RewardProcessors
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>from</span> <span style=color:#0e84b5;font-weight:700>comlrl.trainers.magrpo</span> <span style=color:#007020;font-weight:700>import</span> MAGRPOConfig, MAGRPOTrainer</span></span></code></pre></div><h2 id=dataset-preparation>Dataset Preparation<a class=anchor href=#dataset-preparation>#</a></h2><p>We first create a dataset of creative prompts for the agents to work on.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>train_data <span style=color:#666>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#4070a0>&#34;prompt&#34;</span>: [
</span></span><span style=display:flex><span>        <span style=color:#4070a0>&#34;Describe a city in the clouds:&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#4070a0>&#34;Invent a new holiday and explain it:&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#4070a0>&#34;Write a bedtime story for a dragon:&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#4070a0>&#34;Explain how teleportation might work:&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#4070a0>&#34;Tell a joke about dinosaurs:&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#4070a0>&#34;Describe a world without electricity:&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#4070a0>&#34;Create a superhero with a unique power:&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#4070a0>&#34;Write a scene where the moon talks:&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#4070a0>&#34;Invent a new type of fruit:&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#4070a0>&#34;Design a playground on Mars:&#34;</span>,
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>train_dataset <span style=color:#666>=</span> Dataset<span style=color:#666>.</span>from_dict(train_data)</span></span></code></pre></div><h2 id=agent-initialization>Agent Initialization<a class=anchor href=#agent-initialization>#</a></h2><p>We load a tokenizer to convert text into tokens that the model can process and initialize two separate instances.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model_name <span style=color:#666>=</span> <span style=color:#4070a0>&#34;Qwen/Qwen2.5-0.5B&#34;</span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#666>=</span> AutoTokenizer<span style=color:#666>.</span>from_pretrained(model_name)
</span></span><span style=display:flex><span>agents <span style=color:#666>=</span> [AutoModelForCausalLM<span style=color:#666>.</span>from_pretrained(model_name) <span style=color:#007020;font-weight:700>for</span> _ <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(<span style=color:#40a070>2</span>)]</span></span></code></pre></div><h2 id=define-the-reward-function>Define the Reward Function<a class=anchor href=#define-the-reward-function>#</a></h2><p>The reward function measures how well the agents collaborate. It gives maximum reward (1.0) when the second agent&rsquo;s output is 2–3× longer than the first agent&rsquo;s. If the length ratio falls outside this range, the reward decays exponentially based on how far it deviates.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#007020;font-weight:700>def</span> <span style=color:#06287e>proper_length_ratio_reward</span>(
</span></span><span style=display:flex><span>    completions1, completions2, target_min<span style=color:#666>=</span><span style=color:#40a070>2.0</span>, target_max<span style=color:#666>=</span><span style=color:#40a070>3.0</span>
</span></span><span style=display:flex><span>):
</span></span><span style=display:flex><span>    rewards <span style=color:#666>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> c1, c2 <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>zip</span>(completions1, completions2):
</span></span><span style=display:flex><span>        len1, len2 <span style=color:#666>=</span> <span style=color:#007020>len</span>(c1), <span style=color:#007020>len</span>(c2)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>if</span> len1 <span style=color:#666>==</span> <span style=color:#40a070>0</span>:
</span></span><span style=display:flex><span>            rewards<span style=color:#666>.</span>append(<span style=color:#40a070>0.0</span>)
</span></span><span style=display:flex><span>            <span style=color:#007020;font-weight:700>continue</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        ratio <span style=color:#666>=</span> len2 <span style=color:#666>/</span> len1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>if</span> target_min <span style=color:#666>&lt;=</span> ratio <span style=color:#666>&lt;=</span> target_max:
</span></span><span style=display:flex><span>            reward <span style=color:#666>=</span> <span style=color:#40a070>1.0</span>
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>else</span>:
</span></span><span style=display:flex><span>            <span style=color:#007020;font-weight:700>if</span> ratio <span style=color:#666>&lt;</span> target_min:
</span></span><span style=display:flex><span>                distance <span style=color:#666>=</span> target_min <span style=color:#666>-</span> ratio
</span></span><span style=display:flex><span>            <span style=color:#007020;font-weight:700>else</span>:
</span></span><span style=display:flex><span>                distance <span style=color:#666>=</span> ratio <span style=color:#666>-</span> target_max
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            reward <span style=color:#666>=</span> math<span style=color:#666>.</span>exp(<span style=color:#666>-</span>distance)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        rewards<span style=color:#666>.</span>append(<span style=color:#007020>float</span>(reward))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>return</span> rewards</span></span></code></pre></div><h2 id=configure-training>Configure Training<a class=anchor href=#configure-training>#</a></h2><p>We set up the training configuration with hyperparameters like learning rate, batch size, and the number of generations each agent produces per prompt.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>config <span style=color:#666>=</span> MAGRPOConfig(
</span></span><span style=display:flex><span>    output_dir<span style=color:#666>=</span><span style=color:#4070a0>&#34;./magrpo_multi_reward_output&#34;</span>,
</span></span><span style=display:flex><span>    num_train_epochs<span style=color:#666>=</span><span style=color:#40a070>3</span>,
</span></span><span style=display:flex><span>    per_device_train_batch_size<span style=color:#666>=</span><span style=color:#40a070>1</span>,
</span></span><span style=display:flex><span>    learning_rate<span style=color:#666>=</span><span style=color:#40a070>5e-5</span>,
</span></span><span style=display:flex><span>    logging_steps<span style=color:#666>=</span><span style=color:#40a070>10</span>,
</span></span><span style=display:flex><span>    save_steps<span style=color:#666>=</span><span style=color:#40a070>100</span>,
</span></span><span style=display:flex><span>    num_generations<span style=color:#666>=</span><span style=color:#40a070>8</span>,
</span></span><span style=display:flex><span>    max_new_tokens<span style=color:#666>=</span><span style=color:#40a070>128</span>,
</span></span><span style=display:flex><span>)</span></span></code></pre></div><h2 id=create-the-trainer>Create the Trainer<a class=anchor href=#create-the-trainer>#</a></h2><p>We instantiate the MAGRPO trainer with our agents, reward function, and configuration. The reward is scaled by 100× to provide a stronger learning signal.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>wandb_config <span style=color:#666>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#4070a0>&#34;project&#34;</span>: <span style=color:#666>&lt;</span>your<span style=color:#666>-</span>project<span style=color:#666>-</span>name<span style=color:#666>&gt;</span>,
</span></span><span style=display:flex><span>    <span style=color:#4070a0>&#34;entity&#34;</span>: <span style=color:#666>&lt;</span>your<span style=color:#666>-</span>entity<span style=color:#666>-</span>name<span style=color:#666>&gt;</span>,
</span></span><span style=display:flex><span>    <span style=color:#4070a0>&#34;name&#34;</span>: <span style=color:#4070a0>&#34;length-ratio-demo&#34;</span>,
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>configured_reward_func <span style=color:#666>=</span> partial(
</span></span><span style=display:flex><span>    proper_length_ratio_reward, target_min<span style=color:#666>=</span><span style=color:#40a070>2</span>, target_max<span style=color:#666>=</span><span style=color:#40a070>3</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>trainer <span style=color:#666>=</span> MAGRPOTrainer(
</span></span><span style=display:flex><span>    agents<span style=color:#666>=</span>agents,
</span></span><span style=display:flex><span>    reward_func<span style=color:#666>=</span>configured_reward_func,
</span></span><span style=display:flex><span>    reward_processor<span style=color:#666>=</span>RewardProcessors<span style=color:#666>.</span>scale(factor<span style=color:#666>=</span><span style=color:#40a070>100.0</span>),
</span></span><span style=display:flex><span>    args<span style=color:#666>=</span>config,
</span></span><span style=display:flex><span>    train_dataset<span style=color:#666>=</span>train_dataset,
</span></span><span style=display:flex><span>    tokenizer<span style=color:#666>=</span>tokenizer,
</span></span><span style=display:flex><span>    wandb_config<span style=color:#666>=</span>wandb_config,
</span></span><span style=display:flex><span>)</span></span></code></pre></div><h2 id=run-training>Run Training<a class=anchor href=#run-training>#</a></h2><p>Finally, we start the training process. The trainer will optimize both agents to maximize the collaborative reward, then save the trained models.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>trainer<span style=color:#666>.</span>train()
</span></span><span style=display:flex><span>trainer<span style=color:#666>.</span>save_model(<span style=color:#4070a0>f</span><span style=color:#4070a0>&#34;</span><span style=color:#70a0d0>{</span>config<span style=color:#666>.</span>output_dir<span style=color:#70a0d0>}</span><span style=color:#4070a0>/models&#34;</span>)</span></span></code></pre></div></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=../../../docs/user-guide/multi-turn/ class="flex align-center"><img src=../../../icons/backward.svg class=book-icon alt=Backward>
<span>Multi-Turn Training</span>
</a></span><span><a href=../../../docs/dev/support/ class="flex align-center"><span>Support</span>
<img src=../../../icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div></main></body></html>