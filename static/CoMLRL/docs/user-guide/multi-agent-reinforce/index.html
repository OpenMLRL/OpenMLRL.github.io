<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="REINFORCE is a class of policy gradient methods that optimize the policy directly using sampled returns. It has been widely used to fine-tune LLMs because of its simplicity and efficiency, e.g., GRPO, Dr. GRPO, RLOO, ReMax, TreeRPO, and REINFORCE++. REINFORCE can be extended to multi-agent settings, where multiple LLM agents response synchronously and their joint responses form a solution at each turn to receive a shared reward at each turn.
MA-REINFORCE# The naive Multi‑Agent REINFORCE (MA-REINFORCE) can be expressed as:
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="/docs/user-guide/multi-agent-reinforce/"><meta property="og:site_name" content="CoMLRL"><meta property="og:title" content="Multi-Agent REINFORCE"><meta property="og:description" content="REINFORCE is a class of policy gradient methods that optimize the policy directly using sampled returns. It has been widely used to fine-tune LLMs because of its simplicity and efficiency, e.g., GRPO, Dr. GRPO, RLOO, ReMax, TreeRPO, and REINFORCE++. REINFORCE can be extended to multi-agent settings, where multiple LLM agents response synchronously and their joint responses form a solution at each turn to receive a shared reward at each turn.
MA-REINFORCE# The naive Multi‑Agent REINFORCE (MA-REINFORCE) can be expressed as:"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2026-02-13T16:23:08-05:00"><meta itemprop=name content="Multi-Agent REINFORCE"><meta itemprop=description content="REINFORCE is a class of policy gradient methods that optimize the policy directly using sampled returns. It has been widely used to fine-tune LLMs because of its simplicity and efficiency, e.g., GRPO, Dr. GRPO, RLOO, ReMax, TreeRPO, and REINFORCE++. REINFORCE can be extended to multi-agent settings, where multiple LLM agents response synchronously and their joint responses form a solution at each turn to receive a shared reward at each turn.
MA-REINFORCE# The naive Multi‑Agent REINFORCE (MA-REINFORCE) can be expressed as:"><meta itemprop=dateModified content="2026-02-13T16:23:08-05:00"><meta itemprop=wordCount content="693"><title>Multi-Agent REINFORCE | CoMLRL</title><link rel=icon href=../../../favicon.png><link rel=manifest href=../../../manifest.json><link rel=canonical href=../../../docs/user-guide/multi-agent-reinforce/><link rel=stylesheet href=../../../book.min.cf77f74a94d5d5797fce47774a1371342c8a7d81e76bbc31dd6c3ba4e268e9a2.css><script defer src=../../../fuse.min.js></script><script defer src=../../../en.search.min.a2d091a143a3683cc0ac8052c8fc4c5efb5598a776bc019e77cd12eceaa93413.js></script><link rel=stylesheet href=../../../css/sidebar.css><link rel=icon type=image/svg+xml href=../../../img/comlrl-icon.svg><link rel=icon type=image/png href=../../../img/comlrl-icon.png><link rel="shortcut icon" type=image/png href=../../../img/comlrl-icon.png><link rel=apple-touch-icon href=../../../img/comlrl-icon.png><script>document.addEventListener("DOMContentLoaded",function(){try{var t,n,s,o,e=document.querySelector("article.book-article");if(!e)return;if(n=document.querySelector(".book-header h3"),t=n?n.textContent.trim():(document.title||"").split("|")[0].trim(),!t)return;s=e.querySelector("h1"),(!s||s.textContent.trim()!==t)&&(o=document.createElement("h1"),o.textContent=t,e.insertBefore(o,e.firstChild)),function(){var t,e=document.getElementById("omlrl-footer");e||(e=document.createElement("div"),e.id="omlrl-footer",e.className="omlrl-footer",document.body.appendChild(e)),e.textContent="",t=document.createElement("strong"),t.textContent="© OpenMLRL. All rights reserved.",e.appendChild(t)}()}catch{}}),document.addEventListener("keydown",function(e){if(!e.metaKey&&!e.ctrlKey)return;var n,t=(e.key||"").toLowerCase();if(t==="k"){e.preventDefault(),n=document.querySelector(".book-search input"),n&&n.focus();return}if(t==="o"){e.preventDefault(),window.location.href="https://openmlrl.github.io";return}if(t==="g"){e.preventDefault(),window.location.href="https://github.com/OpenMLRL/CoMLRL";return}}),document.addEventListener("DOMContentLoaded",function(){var e,t=document.querySelectorAll("pre");t.forEach(function(e){var t=document.createElement("button");t.className="copy-code-button",t.textContent="Copy",t.addEventListener("click",function(){var n=e.querySelector("code"),s=n.textContent;navigator.clipboard.writeText(s).then(function(){t.textContent="Copied!",setTimeout(function(){t.textContent="Copy"},2e3)})}),e.style.position="relative",e.appendChild(t)}),e=document.createElement("a"),e.href="https://github.com/OpenMLRL/CoMLRL",e.className="github-corner",e.target="_blank",e.rel="noopener noreferrer",e.setAttribute("aria-label","View source on GitHub"),e.innerHTML='<svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><defs><linearGradient id="github-gradient" x1="0%" y1="0%" x2="100%" y2="100%"><stop offset="0%" style="stop-color:#9555af;stop-opacity:1" /><stop offset="100%" style="stop-color:#e091c4;stop-opacity:1" /></linearGradient></defs><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z" fill="url(#github-gradient)"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="#fff" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="#fff" class="octo-body"></path></svg>',document.body.appendChild(e)})</script></head><body dir=ltr class="book-kind-page book-type-docs"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=../../../><img src=../../../img/comlrl-logo.png alt=Logo><span>CoMLRL</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a>User Guide</a><ul><li><a href=../../../docs/user-guide/installation/>Installation</a></li><li><a href=../../../docs/user-guide/model-loading/>Model Loading</a></li><li><a href=../../../docs/user-guide/multi-agent-reinforce/ class=active>Multi-Agent REINFORCE</a></li><li><a href=../../../docs/user-guide/multi-agent-actor-critic/>Multi-Agent Actor-Critic</a></li><li><a href=../../../docs/user-guide/multi-turn-training/>Multi-Turn Training</a></li></ul></li><li class=book-section-flat><a>Examples</a><ul><li><a href=../../../docs/examples/comlrl-quick-start/>CoMLRL Quick Start</a></li></ul></li><li class=book-section-flat><a>Developers</a><ul><li><a href=../../../docs/dev/support/>Support</a></li><li><a href=../../../docs/dev/contributing/>Contributing</a></li><li><a href=../../../docs/dev/changelog/>Changelog</a></li></ul></li><li class=book-section-flat><a href=../../../docs/env/>Environments</a><ul><li><a href=https://github.com/OpenMLRL/LLM_Collab_Writing target=_blank rel=noopener>Writing</a></li><li><a href=https://github.com/OpenMLRL/LLM_Collab_Code_Generation target=_blank rel=noopener>Coding</a></li><li><a href=https://github.com/OpenMLRL/LLM_Collab_Minecraft target=_blank rel=noopener>Minecraft</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=../../../icons/menu.svg class=book-icon alt=Menu></label><h3>Multi-Agent REINFORCE</h3><label for=toc-control></label></div></header><article class="markdown book-article"><p>REINFORCE is a class of policy gradient methods that optimize the policy directly using sampled returns.
It has been widely used to fine-tune LLMs because of its simplicity and efficiency, e.g., <a href=https://arxiv.org/pdf/2402.03300>GRPO</a>, <a href=https://arxiv.org/abs/2503.20783>Dr. GRPO</a>, <a href="https://openreview.net/forum?id=r1lgTGL5DE">RLOO</a>, <a href=https://arxiv.org/abs/2310.1050>ReMax</a>, <a href=https://arxiv.org/abs/2506.05183>TreeRPO</a>, and <a href=https://arxiv.org/abs/2501.03262>REINFORCE++</a>.
REINFORCE can be extended to multi-agent settings, where multiple LLM agents response synchronously and their joint responses form a solution at each turn to receive a shared reward at each turn.</p><h2 id=ma-reinforce>MA-REINFORCE<a class=anchor href=#ma-reinforce>#</a></h2><p>The naive Multi‑Agent REINFORCE (MA-REINFORCE) can be expressed as:</p><p><div class=book-katex>\[
J(\theta_i) = \mathbb{E}_{\mathbf{o}_0 \sim \mathcal{D}, \mathbf{h}_t \sim \boldsymbol{\pi}_{\boldsymbol{\theta}}}
\Bigg[\sum_{t=0}^{H-1} R_t \cdot \log \pi_{\theta_i}(a_{i,t}\mid h_{i,t})\Bigg],
\]</div><link rel=stylesheet href=../../../katex/katex.min.css><script defer src=../../../katex/katex.min.js></script><script defer src=../../../katex/auto-render.min.js onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"\\begin{gather}",right:"\\end{gather}",display:!0}],throwOnError:!1})'></script>where <span class=book-katex>\( R_t \)</span> is the return at turn <span class=book-katex>\( t \)</span> and <span class=book-katex>\( H \)</span> is the horizon (i.e., number of dialog turns).
The expectation is taken over initial observations from the dataset <span class=book-katex>\( \mathcal{D} \)</span> and the joint action history of all episodes following policy <span class=book-katex>\( \boldsymbol{\pi}_{\boldsymbol{\theta}} \)</span>.</p><p>REINFORCE methods do not use a critic model for value estimation. Their policy gradients estimation can have high variance, due to the stochasticity of the environment and the long-term credit assignment.
There are two common approaches to reduce the variance: using an action-independent baseline or update with more samples, e.g., using <span class=book-katex>\( K \)</span> samples for value estimation of each joint history <span class=book-katex>\( \mathbf{h}_t \)</span>.</p><p><div class=book-katex>\[
J(\theta_i) = \mathbb{E}_{\mathbf{o}_0 \sim \mathcal{D}, \mathbf{h}_t \sim \boldsymbol{\pi}_{\boldsymbol{\theta}}}
\Bigg[\frac{1}{K}\sum_{k=1}^{K} \sum_{t=0}^{H-1} \left(R^{k}_t - b(\mathbf{h}_t)\right) \cdot \log \pi_{\theta_i}(a^{k}_{i,t}\mid h_{i,t})\Bigg],
\]</div>where the baseline <span class=book-katex>\( b(\mathbf{h}_t) \)</span> is action-independent.</p><h2 id=magrpo>MAGRPO<a class=anchor href=#magrpo>#</a></h2><p>Multi‑Agent Group‑Relative Policy Optimization (MAGRPO) is an instantiation of MA-REINFORCE inspired from GRPO, where the group-average baseline is the mean return of <span class=book-katex>\( K \)</span> samples:</p><div class=book-katex>\[
J(\theta_i) = \mathbb{E}_{\mathbf{o}_0 \sim \mathcal{D}, \mathbf{h}_t \sim \boldsymbol{\pi}_{\boldsymbol{\theta}}}
\Bigg[\frac{1}{K}\sum_{k=1}^{K}\sum_{t=0}^{H-1} \left(R^{k}_t - \frac{1}{K}\sum_{l=1}^{K}R^{l}_t\right) \cdot \log \pi_{\theta_i}(a^{k}_{i,t}\mid h_{i,t})\Bigg].
\]</div><blockquote class="book-hint info"><p><strong>MAGRPOConfig</strong> parameters:</p><ul><li><code>num_agents</code>: Number of agents</li><li><code>num_turns</code>: Number of turns per episode</li><li><code>num_train_epochs</code>: Number of training epochs</li><li><code>agent_learning_rate</code>: Learning rate</li><li><code>logging_steps</code>: Log every N steps</li><li><code>num_generations</code>: Number of generations to sample per prompt for each agent</li><li><code>max_new_tokens</code>: Maximum number of new tokens to generate</li><li><code>temperature</code>: Temperature for sampling</li><li><code>top_p</code>: Top-p for sampling</li><li><code>top_k</code>: Top-k for sampling</li><li><code>discount</code>: Discount factor gamma over turns for returns</li><li><code>joint_mode</code>: Joint action composition (<code>aligned</code> for index-aligned, <code>cross</code> for Cartesian product)</li><li><code>early_termination_threshold</code>: Stop rollouts with mean reward exceeds a threshold</li><li><code>rollout_buffer_size</code>: Number of node samples to buffer before update</li><li><code>train_batch_size</code>: Mini-batch size within each update</li><li><code>advantage_normalization</code>: Whether to normalize advantages</li><li><code>eval_interval</code>: Run evaluation every N training batches</li><li><code>eval_num_samples</code>: Number of samples to evaluate per evaluation run</li><li><code>eval_batch_size</code>: Eval dataloader batch size</li><li><code>external_prompt_passthrough</code>: Use external prompts directly in multi-turn</li><li><code>advantage_mode</code>: Baseline mode (<code>mean</code>, <code>max</code>, <code>rloo</code>, <code>raw</code>)</li></ul></blockquote><blockquote class="book-hint info"><p><strong>MAGRPOTrainer</strong> setup:</p><ul><li><code>agent_model</code> or <code>agents</code>: Model identifier string for homogeneous agents, or list of agent models (multi-agent <code>agent_model</code> must be a string)</li><li><code>num_agents</code>: Number of agents</li><li><code>tokenizer</code>: The tokenizer (required)</li><li><code>reward_func</code>: Callable that returns a list of floats (required)</li><li><code>reward_processor</code>: Optional processor to apply to rewards (e.g., scaling)</li><li><code>formatters</code>: Single callable or list of callables for each agent to format prompts</li><li><code>args</code>: Instance of <code>MAGRPOConfig</code> (optional)</li><li><code>train_dataset</code>: Training dataset (required)</li><li><code>eval_dataset</code>: Evaluation dataset (optional)</li><li><code>model_config</code>: Model configuration dict (optional)</li><li><code>wandb_config</code>: Configuration for Weights & Biases logging (optional)</li><li><code>external_transition</code>: Function providing transitions between turns</li><li><code>eval_logger</code>: Evaluation logger function (optional)</li><li><code>eval_aggregator</code>: Evaluation aggregator function (optional)</li></ul></blockquote><blockquote class="book-hint warning"><p>For simplicity, MAGRPO computes the policy gradient using the current policy&rsquo;s samples without importance sampling or ratio clipping. And since it does not use a critic model, there is no <code>value_clip_range</code> applicable.</p></blockquote><blockquote class="book-hint warning"><p>The trainer uses a fixed training DataLoader batch size of 1 and requires at least <code>num_generations=2</code> generations for group baseline computation. The training use batch gradient descent by default, where <code>train_batch_size</code>=<code>rollout_buffer_size</code>.</p></blockquote><h2 id=other-variants>Other Variants<a class=anchor href=#other-variants>#</a></h2><p>CoMLRL also provides other MA-REINFORCE variants with different baselines:</p><ul><li><strong>MARLOO</strong>: Multi‑Agent REINFORCE Leave‑One‑Out. Baseline is the mean return of other agents (leave‑one‑out) at the same step.</li></ul><div class=book-katex>\[
J(\theta_i) = \mathbb{E}_{\mathbf{o}_0 \sim \mathcal{D}, \mathbf{h}_t \sim \boldsymbol{\pi}_{\boldsymbol{\theta}}}
\Bigg[\frac{1}{K}\sum_{k=1}^{K}\sum_{t=0}^{H-1} \left(R^{k}_t - \frac{1}{K-1}\sum_{l=1, l\neq k}^{K}R^{l}_t\right) \cdot \log \pi_{\theta_i}(a^{k}_{i,t}\mid h_{i,t})\Bigg].
\]</div><ul><li><strong>MAREMAX</strong>: Multi‑Agent REINFORCE with Group Max. Baseline is the maximum group return at the step.</li></ul><div class=book-katex>\[
J(\theta_i) = \mathbb{E}_{\mathbf{o}_0 \sim \mathcal{D}, \mathbf{h}_t \sim \boldsymbol{\pi}_{\boldsymbol{\theta}}}
\Bigg[\frac{1}{K}\sum_{k=1}^{K}\sum_{t=0}^{H-1} \left(R^{k}_t - \mathrm{max}_l\, R^l_t \right) \cdot \log \pi_{\theta_i}(a^{k}_{i,t}\mid h_{i,t})\Bigg].
\]</div><blockquote class="book-hint success"><p>These classes and MA-REINFORCE are derived from <code>comlrl.trainers.reinforce.MAGRPOTrainer</code>. Interfaces for the trainer and configuration classes are the same as <code>MAGRPOTrainer</code> and <code>MAGRPOConfig</code>.</p></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=../../../docs/user-guide/model-loading/ class="flex align-center"><img src=../../../icons/backward.svg class=book-icon alt=Backward>
<span>Model Loading</span>
</a></span><span><a href=../../../docs/user-guide/multi-agent-actor-critic/ class="flex align-center"><span>Multi-Agent Actor-Critic</span>
<img src=../../../icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div></main></body></html>