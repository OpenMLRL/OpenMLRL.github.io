<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>User Guide on CoMLRL</title><link>/docs/user-guide/</link><description>Recent content in User Guide on CoMLRL</description><generator>Hugo</generator><language>en-us</language><atom:link href="/docs/user-guide/index.xml" rel="self" type="application/rss+xml"/><item><title>Installation</title><link>/docs/user-guide/installation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/user-guide/installation/</guid><description>&lt;p&gt;CoMLRL provides several different ways for installation.&lt;/p&gt;
&lt;h2 id="install-from-pypi"&gt;Install from PyPI&lt;a class="anchor" href="#install-from-pypi"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;pip install comlrl
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;# install PyTorch compatible with your device&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="install-from-conda-forge"&gt;Install from conda-forge&lt;a class="anchor" href="#install-from-conda-forge"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;conda install -c conda-forge comlrl
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;# install PyTorch compatible with your device&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="install-from-source"&gt;Install from source&lt;a class="anchor" href="#install-from-source"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;To access the latest features of CoMLRL or to develop CoMLRL, clone this repository and install in editable mode:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;git clone https://github.com/OpenMLRL/CoMLRL.git
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;cd&lt;/span&gt; CoMLRL
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;pip install -e .
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;# install PyTorch compatible with your device&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description></item><item><title>Model Loading</title><link>/docs/user-guide/model-loading/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/user-guide/model-loading/</guid><description>&lt;p&gt;CoMLRL supports both homogeneous and heterogeneous models.
Users can assign &lt;code&gt;agent_model&lt;/code&gt;/&lt;code&gt;critic_model&lt;/code&gt; with &lt;a href="https://huggingface.co/models"&gt;HuggingFace model identifiers&lt;/a&gt; for homogeneous setups, or provide &lt;code&gt;agents&lt;/code&gt;/&lt;code&gt;critics&lt;/code&gt; lists for heterogeneous setups.&lt;/p&gt;
&lt;h2 id="homogeneous-agents"&gt;Homogeneous Agents&lt;a class="anchor" href="#homogeneous-agents"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The easiest way to start the journey of CoMLRL is to load &lt;code&gt;num_agents&lt;/code&gt; homogeneous agents with a single model identifier.
Users can set &lt;code&gt;agent_model.name&lt;/code&gt; to a single model identifier while keeping &lt;code&gt;agents: null&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example, to load 3 &lt;em&gt;Qwen/Qwen2.5-1.5B&lt;/em&gt; agents:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;trainer &lt;span style="color:#666"&gt;=&lt;/span&gt; MAGRPOTrainer(
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; agent_model&lt;span style="color:#666"&gt;=&lt;/span&gt;&lt;span style="color:#4070a0"&gt;&amp;#34;Qwen/Qwen2.5-1.5B&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; agents&lt;span style="color:#666"&gt;=&lt;/span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; args&lt;span style="color:#666"&gt;=&lt;/span&gt;MAGRPOConfig(num_agents&lt;span style="color:#666"&gt;=&lt;/span&gt;&lt;span style="color:#40a070"&gt;3&lt;/span&gt;, temperature&lt;span style="color:#666"&gt;=&lt;/span&gt;&lt;span style="color:#40a070"&gt;0.7&lt;/span&gt;, top_p&lt;span style="color:#666"&gt;=&lt;/span&gt;&lt;span style="color:#40a070"&gt;0.9&lt;/span&gt;, top_k&lt;span style="color:#666"&gt;=&lt;/span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;None&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="heterogeneous-agents"&gt;Heterogeneous Agents&lt;a class="anchor" href="#heterogeneous-agents"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Although homogeneous LLM agents can be specified into different roles by prompting, using heterogeneous LLMs with different skills can further unleash the potential of multi-agent collaboration.
Users can load a list of heterogeneous agents in &lt;code&gt;agents&lt;/code&gt;, where the length of the list should match &lt;code&gt;num_agents&lt;/code&gt;. Each entry should specify a model identifier and optional tokenizer/model kwargs.
When &lt;code&gt;agents&lt;/code&gt; is provided, &lt;code&gt;agent_model&lt;/code&gt; should be set to null or ignored; if both are provided, they must match (same names, correct length) or training will raise an error.&lt;/p&gt;</description></item><item><title>Multi-Agent REINFORCE</title><link>/docs/user-guide/multi-agent-reinforce/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/user-guide/multi-agent-reinforce/</guid><description>&lt;p&gt;REINFORCE is a class of policy gradient methods that optimize the policy directly using sampled returns.
It has been widely used to fine-tune LLMs because of its simplicity and efficiency, e.g., &lt;a href="https://arxiv.org/pdf/2402.03300"&gt;GRPO&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2503.20783"&gt;Dr. GRPO&lt;/a&gt;, &lt;a href="https://openreview.net/forum?id=r1lgTGL5DE"&gt;RLOO&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2310.1050"&gt;ReMax&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2506.05183"&gt;TreeRPO&lt;/a&gt;, and &lt;a href="https://arxiv.org/abs/2501.03262"&gt;REINFORCE++&lt;/a&gt;.
REINFORCE can be extended to multi-agent settings, where multiple LLM agents response synchronously and their joint responses form a solution at each turn to receive a shared reward at each turn.&lt;/p&gt;
&lt;h2 id="ma-reinforce"&gt;MA-REINFORCE&lt;a class="anchor" href="#ma-reinforce"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The naive Multiâ€‘Agent REINFORCE (MA-REINFORCE) can be expressed as:&lt;/p&gt;</description></item><item><title>Multi-Agent Actor-Critic</title><link>/docs/user-guide/multi-agent-actor-critic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/user-guide/multi-agent-actor-critic/</guid><description>&lt;p&gt;Actor-Critic (AC) methods are widely-used policy gradient that employ critics to facilitate training.
AC methods can achieve lower variance and better sample efficiency than REINFORCE, but this requires careful design and tuning of the critic to ensure stable training.
In Multi-Agent Reinforcement Learning (MARL), Actor-Critic methods can be instantiated as Multi-Agent Actor-Critic (MAAC) and Independent Actor-Critic (IAC).&lt;/p&gt;
&lt;h2 id="maac"&gt;MAAC&lt;a class="anchor" href="#maac"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Multi-Agent Actor-Critic (MAAC) uses a Centralized Critic (CC) across agents to evaluate the values of joint histories &lt;span class="book-katex"&gt;\( V_{\boldsymbol{\phi}}(\mathbf{h}_t) \)&lt;/span&gt;&lt;link rel="stylesheet" href="../../katex/katex.min.css" /&gt;&lt;script defer src="../../katex/katex.min.js"&gt;&lt;/script&gt;&lt;script defer src="../../katex/auto-render.min.js" onload="renderMathInElement(document.body, {&amp;#34;delimiters&amp;#34;:[{&amp;#34;left&amp;#34;:&amp;#34;$$&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;$$&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\(&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\)&amp;#34;,&amp;#34;display&amp;#34;:false},{&amp;#34;left&amp;#34;:&amp;#34;\\[&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\]&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\begin{equation}&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\end{equation}&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\begin{align}&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\end{align}&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\begin{gather}&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\end{gather}&amp;#34;,&amp;#34;display&amp;#34;:true}],&amp;#34;throwOnError&amp;#34;:false});"&gt;&lt;/script&gt; or joint history-action pairs &lt;span class="book-katex"&gt;\( Q_{\boldsymbol{\psi}}(\mathbf{h}_t, \mathbf{a}_t) \)&lt;/span&gt;.
The policy gradient of each agent is:&lt;/p&gt;</description></item><item><title>Multi-Turn Interaction</title><link>/docs/user-guide/multi-turn-training/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/user-guide/multi-turn-training/</guid><description>&lt;p&gt;Many complex problems cannot be solved in a single turn. LLM agents need to interact with the environment to obtain useful feedback from other models or tools involved in the system.&lt;/p&gt;
&lt;h2 id="multi-turn-magrpo"&gt;Multi-Turn MAGRPO&lt;a class="anchor" href="#multi-turn-magrpo"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;MAGRPO in the multi-turn setting forms a tree-structured rollout expansion where branches represent different joint responses (&lt;a href="https://arxiv.org/abs/2506.05183"&gt;TreeRPO&lt;/a&gt;).&lt;/p&gt;
&lt;p align="center"&gt;
 &lt;img src="../../img/joint-tree.svg" width="450px;" alt=""/&gt;
&lt;/p&gt;
&lt;p&gt;In each episode, a task is sampled from the dataset to construct initial observations &lt;span class="book-katex"&gt;\( \mathbf{o}_0=\{o_{1, 0}, \cdots, o_{n, 0}\} \)&lt;/span&gt;&lt;link rel="stylesheet" href="../../katex/katex.min.css" /&gt;&lt;script defer src="../../katex/katex.min.js"&gt;&lt;/script&gt;&lt;script defer src="../../katex/auto-render.min.js" onload="renderMathInElement(document.body, {&amp;#34;delimiters&amp;#34;:[{&amp;#34;left&amp;#34;:&amp;#34;$$&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;$$&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\(&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\)&amp;#34;,&amp;#34;display&amp;#34;:false},{&amp;#34;left&amp;#34;:&amp;#34;\\[&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\]&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\begin{equation}&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\end{equation}&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\begin{align}&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\end{align}&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\begin{gather}&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\end{gather}&amp;#34;,&amp;#34;display&amp;#34;:true}],&amp;#34;throwOnError&amp;#34;:false});"&gt;&lt;/script&gt; and histories &lt;span class="book-katex"&gt;\( \mathbf{h}_0=\{h_{1, 0}, \cdots, h_{n, 0}\} \)&lt;/span&gt; for all agents. At each turn, agents generate a group of joint responses &lt;span class="book-katex"&gt;\( \mathbf{a}^{\mathcal{G}}_t\gets\boldsymbol{\pi}^{\mathcal{G}}(\cdot|\mathbf{h}_t) \)&lt;/span&gt; from their current observation-action history &lt;span class="book-katex"&gt;\( \mathbf{h}_t \)&lt;/span&gt;, with each response initiating a distinct rollout. Agents receive joint rewards &lt;span class="book-katex"&gt;\( r^{(g)}_{t} \)&lt;/span&gt; for each response based on the accumulated history &lt;span class="book-katex"&gt;\( \mathbf{a}^{(g)}_{t} \in \mathbf{a}^{\mathcal{G}}_{t} \)&lt;/span&gt; and current action. &lt;strong&gt;Each rollout then evolves independently&lt;/strong&gt;, producing new joint observations &lt;span class="book-katex"&gt;\( \mathbf{o}^{\mathcal{G}}_{t&amp;#43;1} \)&lt;/span&gt; as the environment dynamics unfold and spawning more rollouts at the next turn &lt;span class="book-katex"&gt;\( t&amp;#43;1 \)&lt;/span&gt;. This process continues until the terminal turn is reached &lt;span class="book-katex"&gt;\( H \)&lt;/span&gt;.&lt;/p&gt;</description></item><item><title>Training Parallelization</title><link>/docs/user-guide/training-parallelization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/user-guide/training-parallelization/</guid><description>&lt;p&gt;CoMLRL supports fine-tuning multi-LLM systems with larger models and more agents when multiple GPUs are available.
Users can configure the parallelization training with &lt;code&gt;iac.parallel_training&lt;/code&gt;.
Currently, &lt;code&gt;parallel_training&lt;/code&gt; supports two modes: &lt;code&gt;none&lt;/code&gt; or &lt;code&gt;null&lt;/code&gt; is the default mode for single-device training; &lt;code&gt;mp&lt;/code&gt;is the model parallel scheduling across explicit agent/critic devices.&lt;/p&gt;
&lt;blockquote class="book-hint success"&gt;
&lt;p&gt;We will support more parallelization modes (e.g., &lt;a href="https://docs.pytorch.org/docs/stable/elastic/run.html"&gt;data parallelization&lt;/a&gt;, &lt;a href="ray.io"&gt;multi-node training&lt;/a&gt;) in the future.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="model-parallelization"&gt;Model Parallelization&lt;a class="anchor" href="#model-parallelization"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;When &lt;code&gt;parallel_training=mp&lt;/code&gt;, CoMLRL requires explicit &lt;code&gt;agent_devices&lt;/code&gt; / &lt;code&gt;critic_devices&lt;/code&gt; configuration and deploys the agents and critics accordingly.
The training and inference for each model (agent/critic) are running separately on its assigned device.
The responses are aggregated on the CPU and pass to the reward function. The reward is then broadcast back to all devices for training.
MP supports training larger and more models than a single GPU can hold, but the training throughput is limited by the slowest model.&lt;/p&gt;</description></item></channel></rss>