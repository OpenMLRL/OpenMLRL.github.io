<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Many complex problems cannot be solved in a single turn. Agents need to interact with the environment to obtain useful feedback from other models or tools involved in the system, enabling iterative refinement and exploration of multiple solution paths.
Multi-Turn MAGRPO# MAGRPO in the multi-turn setting (MAGRPO-MT) forms a tree-structured rollout expansion where branches represent different joint responses (TreeRPO).
In each episode, a task is sampled from the dataset to construct initial observations \( \mathbf{o}_0=\{o_{1, 0}, \cdots, o_{n, 0}\} \) and histories \( \mathbf{h}_0=\{h_{1, 0}, \cdots, h_{n, 0}\} \) for all agents. At each turn, agents generate a group of joint responses \( \mathbf{a}^{\mathcal{G}}_t\gets\boldsymbol{\pi}^{\mathcal{G}}(\cdot|\mathbf{h}_t) \) from their current observation-action history \( \mathbf{h}_t \), with each response initiating a distinct rollout. Agents receive joint rewards \( r^{(g)}_{t} \) for each response based on the accumulated history \( \mathbf{a}^{(g)}_{t} \in \mathbf{a}^{\mathcal{G}}_{t} \) and current action. Each rollout then evolves independently, producing new joint observations \( \mathbf{o}^{\mathcal{G}}_{t+1} \) as the environment dynamics unfold and spawning more rollouts at the next turn \( t+1 \). This process continues until the terminal turn is reached \( H \).
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="/docs/user-guide/multi-turn/"><meta property="og:site_name" content="CoMLRL"><meta property="og:title" content="Multi-Turn Training"><meta property="og:description" content="Many complex problems cannot be solved in a single turn. Agents need to interact with the environment to obtain useful feedback from other models or tools involved in the system, enabling iterative refinement and exploration of multiple solution paths.
Multi-Turn MAGRPO# MAGRPO in the multi-turn setting (MAGRPO-MT) forms a tree-structured rollout expansion where branches represent different joint responses (TreeRPO).
In each episode, a task is sampled from the dataset to construct initial observations \( \mathbf{o}_0=\{o_{1, 0}, \cdots, o_{n, 0}\} \) and histories \( \mathbf{h}_0=\{h_{1, 0}, \cdots, h_{n, 0}\} \) for all agents. At each turn, agents generate a group of joint responses \( \mathbf{a}^{\mathcal{G}}_t\gets\boldsymbol{\pi}^{\mathcal{G}}(\cdot|\mathbf{h}_t) \) from their current observation-action history \( \mathbf{h}_t \), with each response initiating a distinct rollout. Agents receive joint rewards \( r^{(g)}_{t} \) for each response based on the accumulated history \( \mathbf{a}^{(g)}_{t} \in \mathbf{a}^{\mathcal{G}}_{t} \) and current action. Each rollout then evolves independently, producing new joint observations \( \mathbf{o}^{\mathcal{G}}_{t+1} \) as the environment dynamics unfold and spawning more rollouts at the next turn \( t+1 \). This process continues until the terminal turn is reached \( H \)."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2026-02-06T20:50:25-05:00"><meta itemprop=name content="Multi-Turn Training"><meta itemprop=description content="Many complex problems cannot be solved in a single turn. Agents need to interact with the environment to obtain useful feedback from other models or tools involved in the system, enabling iterative refinement and exploration of multiple solution paths.
Multi-Turn MAGRPO# MAGRPO in the multi-turn setting (MAGRPO-MT) forms a tree-structured rollout expansion where branches represent different joint responses (TreeRPO).
In each episode, a task is sampled from the dataset to construct initial observations \( \mathbf{o}_0=\{o_{1, 0}, \cdots, o_{n, 0}\} \) and histories \( \mathbf{h}_0=\{h_{1, 0}, \cdots, h_{n, 0}\} \) for all agents. At each turn, agents generate a group of joint responses \( \mathbf{a}^{\mathcal{G}}_t\gets\boldsymbol{\pi}^{\mathcal{G}}(\cdot|\mathbf{h}_t) \) from their current observation-action history \( \mathbf{h}_t \), with each response initiating a distinct rollout. Agents receive joint rewards \( r^{(g)}_{t} \) for each response based on the accumulated history \( \mathbf{a}^{(g)}_{t} \in \mathbf{a}^{\mathcal{G}}_{t} \) and current action. Each rollout then evolves independently, producing new joint observations \( \mathbf{o}^{\mathcal{G}}_{t+1} \) as the environment dynamics unfold and spawning more rollouts at the next turn \( t+1 \). This process continues until the terminal turn is reached \( H \)."><meta itemprop=dateModified content="2026-02-06T20:50:25-05:00"><meta itemprop=wordCount content="706"><title>Multi-Turn Training | CoMLRL</title><link rel=icon href=../../../favicon.png><link rel=manifest href=../../../manifest.json><link rel=canonical href=../../../docs/user-guide/multi-turn/><link rel=stylesheet href=../../../book.min.84995bf041dbb6656f4541996b392ba4d647753aa90c42a8483168566c27189d.css><script defer src=../../../fuse.min.js></script><script defer src=../../../en.search.min.70e3fdf5f315d2c45bded58b459caec812cbc9d37db2bff9050f3e53f8c75e93.js></script><link rel=stylesheet href=../../../css/sidebar.css><link rel=icon type=image/svg+xml href=../../../img/comlrl-icon.svg><link rel=icon type=image/png href=../../../img/comlrl-icon.png><link rel="shortcut icon" type=image/png href=../../../img/comlrl-icon.png><link rel=apple-touch-icon href=../../../img/comlrl-icon.png><script>document.addEventListener("DOMContentLoaded",function(){try{var t,n,s,o,e=document.querySelector("article.book-article");if(!e)return;if(n=document.querySelector(".book-header h3"),t=n?n.textContent.trim():(document.title||"").split("|")[0].trim(),!t)return;s=e.querySelector("h1"),(!s||s.textContent.trim()!==t)&&(o=document.createElement("h1"),o.textContent=t,e.insertBefore(o,e.firstChild)),function(){var t,e=document.getElementById("omlrl-footer");e||(e=document.createElement("div"),e.id="omlrl-footer",e.className="omlrl-footer",document.body.appendChild(e)),e.textContent="",t=document.createElement("strong"),t.textContent="Â© OpenMLRL. All rights reserved.",e.appendChild(t)}()}catch{}}),document.addEventListener("keydown",function(e){if(!e.metaKey&&!e.ctrlKey)return;var n,t=(e.key||"").toLowerCase();if(t==="k"){e.preventDefault(),n=document.querySelector(".book-search input"),n&&n.focus();return}if(t==="o"){e.preventDefault(),window.location.href="https://openmlrl.github.io";return}if(t==="g"){e.preventDefault(),window.location.href="https://github.com/OpenMLRL/CoMLRL";return}}),document.addEventListener("DOMContentLoaded",function(){var e,t=document.querySelectorAll("pre");t.forEach(function(e){var t=document.createElement("button");t.className="copy-code-button",t.textContent="Copy",t.addEventListener("click",function(){var n=e.querySelector("code"),s=n.textContent;navigator.clipboard.writeText(s).then(function(){t.textContent="Copied!",setTimeout(function(){t.textContent="Copy"},2e3)})}),e.style.position="relative",e.appendChild(t)}),e=document.createElement("a"),e.href="https://github.com/OpenMLRL/CoMLRL",e.className="github-corner",e.target="_blank",e.rel="noopener noreferrer",e.setAttribute("aria-label","View source on GitHub"),e.innerHTML='<svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><defs><linearGradient id="github-gradient" x1="0%" y1="0%" x2="100%" y2="100%"><stop offset="0%" style="stop-color:#9555af;stop-opacity:1" /><stop offset="100%" style="stop-color:#e091c4;stop-opacity:1" /></linearGradient></defs><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z" fill="url(#github-gradient)"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="#fff" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="#fff" class="octo-body"></path></svg>',document.body.appendChild(e)})</script></head><body dir=ltr class="book-kind-page book-type-docs"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=../../../><img src=../../../img/comlrl-logo.png alt=Logo><span>CoMLRL</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a>User Guide</a><ul><li><a href=../../../docs/user-guide/installation/>Installation</a></li><li><a href=../../../docs/user-guide/reinforce-finetuning/>Multi-Agent REINFORCE</a></li><li><a href=../../../docs/user-guide/ac-finetuning/>Multi-Agent Actor-Critic</a></li><li><a href=../../../docs/user-guide/multi-turn/ class=active>Multi-Turn Training</a></li></ul></li><li class=book-section-flat><a>Examples</a><ul><li><a href=../../../docs/examples/quick-demo/>CoMLRL Quick Demo</a></li></ul></li><li class=book-section-flat><a>Developers</a><ul><li><a href=../../../docs/dev/support/>Support</a></li><li><a href=../../../docs/dev/contributing/>Contributing</a></li><li><a href=../../../docs/dev/changelog/>Changelog</a></li></ul></li><li class=book-section-flat><a href=../../../docs/env/>Environments</a><ul><li><a href=https://github.com/OpenMLRL/LLM_Collab_Writing target=_blank rel=noopener>Writing</a></li><li><a href=https://github.com/OpenMLRL/LLM_Collab_Code_Generation target=_blank rel=noopener>Coding</a></li><li><a href=https://github.com/OpenMLRL/LLM_Collab_Minecraft target=_blank rel=noopener>Minecraft</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=../../../icons/menu.svg class=book-icon alt=Menu></label><h3>Multi-Turn Training</h3><label for=toc-control></label></div></header><article class="markdown book-article"><p>Many complex problems cannot be solved in a single turn. Agents need to interact with the environment to obtain useful feedback from other models or tools involved in the system, enabling iterative refinement and exploration of multiple solution paths.</p><h2 id=multi-turn-magrpo>Multi-Turn MAGRPO<a class=anchor href=#multi-turn-magrpo>#</a></h2><p>MAGRPO in the multi-turn setting (<strong>MAGRPO-MT</strong>) forms a tree-structured rollout expansion where branches represent different joint responses (<a href=https://arxiv.org/abs/2506.05183>TreeRPO</a>).</p><p align=center><img src=../../../img/joint-tree.svg width=450px; alt></p><p>In each episode, a task is sampled from the dataset to construct initial observations <span class=book-katex>\( \mathbf{o}_0=\{o_{1, 0}, \cdots, o_{n, 0}\} \)</span><link rel=stylesheet href=../../../katex/katex.min.css><script defer src=../../../katex/katex.min.js></script><script defer src=../../../katex/auto-render.min.js onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"\\begin{gather}",right:"\\end{gather}",display:!0}],throwOnError:!1})'></script> and histories <span class=book-katex>\( \mathbf{h}_0=\{h_{1, 0}, \cdots, h_{n, 0}\} \)</span> for all agents. At each turn, agents generate a group of joint responses <span class=book-katex>\( \mathbf{a}^{\mathcal{G}}_t\gets\boldsymbol{\pi}^{\mathcal{G}}(\cdot|\mathbf{h}_t) \)</span> from their current observation-action history <span class=book-katex>\( \mathbf{h}_t \)</span>, with each response initiating a distinct rollout. Agents receive joint rewards <span class=book-katex>\( r^{(g)}_{t} \)</span> for each response based on the accumulated history <span class=book-katex>\( \mathbf{a}^{(g)}_{t} \in \mathbf{a}^{\mathcal{G}}_{t} \)</span> and current action. <strong>Each rollout then evolves independently</strong>, producing new joint observations <span class=book-katex>\( \mathbf{o}^{\mathcal{G}}_{t+1} \)</span> as the environment dynamics unfold and spawning more rollouts at the next turn <span class=book-katex>\( t+1 \)</span>. This process continues until the terminal turn is reached <span class=book-katex>\( H \)</span>.</p><h3 id=joint-mode>Joint Mode<a class=anchor href=#joint-mode>#</a></h3><p>MAGRPO supports two modes for forming joint responses at each turn:</p><ul><li><strong>Align</strong>: Provides flexibility in the number of joint responses generated per turn, allowing any number of generations at each turn. However, generations are not fully utilized since only aligned responses across agents are combined. As training progresses over <span class=book-katex>\( T \)</span> turns with <span class=book-katex>\( N \)</span> agents, the total number of leaves grows as <span class=book-katex>\( G^T \)</span>, where <span class=book-katex>\( G \)</span> is the number of generations per turn.</li></ul><p align=center><img src=../../../img/align-tree.svg width=500px; alt></p><ul><li><strong>Cross</strong>: Maximizes the utilization of generations and provides more accurate value estimation with more samples by forming the Cartesian product of all agent responses. As training progresses over <span class=book-katex>\( T \)</span> turns with <span class=book-katex>\( N \)</span> agents, the total number of leaves grows as <span class=book-katex>\( G^{N \cdot T} \)</span>, where each node has <span class=book-katex>\( G^N \)</span> sibling joint actions.</li></ul><p align=center><img src=../../../img/cross-tree.svg width=500px; alt></p><blockquote class="book-hint warning"><p>Note that only responses originating from the same rollout can be combined, as rollouts evolve independently.</p></blockquote><h2 id=environment-transition>Environment Transition<a class=anchor href=#environment-transition>#</a></h2><p>External feedback mechanisms control how environment observations are incorporated into prompts for subsequent turns.</p><h3 id=custom-external-feedback>Custom External Feedback<a class=anchor href=#custom-external-feedback>#</a></h3><p>Users can implement custom external feedback by defining a function with the following interface:</p><blockquote class="book-hint info"><p>Custom External Feedback Interface:</p><ul><li><code>prompt</code>: Original task prompt/problem description (required)</li><li><code>agent_completions</code>: List or tuple of completions from the previous turn, one per agent (required)</li><li><code>num_agents</code>: Number of agents in the system (required)</li><li><code>prompt_history_per_agent</code>: List of prompt histories for each agent, where each history is a list of prompts from previous turns (optional)</li><li><code>response_history_per_agent</code>: List of response histories for each agent, where each history is a list of responses from previous turns (optional)</li></ul><p>The function must return a list or tuple of prompts for the next turn, one for each agent.
The trainer only passes the arguments above (no extra kwargs), so any mode-specific parameters should be captured via closure or <code>functools.partial</code>.</p></blockquote><p>By default, returned prompts are inserted as the new <code>prompt</code> field and then passed through each agent&rsquo;s formatter.
If <code>external_prompt_passthrough=true</code>, the returned prompts are used directly without re-formatting.
In MAGRPO, the external transition is called per rollout branch with that branch&rsquo;s histories.</p><p>For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#007020;font-weight:700>def</span> <span style=color:#06287e>custom_external</span>(
</span></span><span style=display:flex><span>    prompt: <span style=color:#007020>str</span>,
</span></span><span style=display:flex><span>    agent_completions: List[<span style=color:#007020>str</span>],
</span></span><span style=display:flex><span>    num_agents: <span style=color:#007020>int</span>,
</span></span><span style=display:flex><span>    prompt_history_per_agent: Optional[List[List[<span style=color:#007020>str</span>]]] <span style=color:#666>=</span> <span style=color:#007020;font-weight:700>None</span>,
</span></span><span style=display:flex><span>    response_history_per_agent: Optional[List[List[<span style=color:#007020>str</span>]]] <span style=color:#666>=</span> <span style=color:#007020;font-weight:700>None</span>,
</span></span><span style=display:flex><span>) <span style=color:#666>-&gt;</span> List[<span style=color:#007020>str</span>]:
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic># Custom logic to format next-turn prompts</span>
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic># Access environment feedback, tool outputs, etc.</span>
</span></span><span style=display:flex><span>    next_turn_prompts <span style=color:#666>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> i <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(num_agents):
</span></span><span style=display:flex><span>        <span style=color:#60a0b0;font-style:italic># Format prompt for agent i based on history and feedback</span>
</span></span><span style=display:flex><span>        next_prompt <span style=color:#666>=</span> <span style=color:#4070a0>f</span><span style=color:#4070a0>&#34;</span><span style=color:#70a0d0>{</span>prompt<span style=color:#70a0d0>}</span><span style=color:#4070a0;font-weight:700>\n</span><span style=color:#4070a0>Previous attempt: </span><span style=color:#70a0d0>{</span>agent_completions[i]<span style=color:#70a0d0>}</span><span style=color:#4070a0;font-weight:700>\n</span><span style=color:#4070a0>Please revise.&#34;</span>
</span></span><span style=display:flex><span>        next_turn_prompts<span style=color:#666>.</span>append(next_prompt)
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>return</span> next_turn_prompts</span></span></code></pre></div><blockquote class="book-hint warning"><p>For IAC/MAAC multi-turn training, <code>num_generations</code> must be set to 1.</p></blockquote><h3 id=example-modes-expert-diagnosis-and-self-improvement>Example Modes (Expert, Diagnosis, and Self-Improvement)<a class=anchor href=#example-modes-expert-diagnosis-and-self-improvement>#</a></h3><p>An <a href=https://github.com/OpenMLRL/LLM_Collab_Code_Generation>environment for code generation</a> includes 3 example external transition modes:</p><blockquote class="book-hint success"><ul><li><p><code>external.mode=expert_edits</code>: Uses an external LLM (default: DeepSeek-Coder) to propose code edits. Follow-up prompts include edit suggestions with context from previous turns. It can be configured via <code>expert_model</code> for different experts (e.g., Claude, GPT) when API keys are available.</p></li><li><p><code>external.mode=level_feedback</code>: Static AST checks and dynamically executes code to provide diagnosis. The default sandbox test includes the first test; configurable via <code>sandbox_slice</code> to include all tests (0, None, or &lsquo;all&rsquo;), specific number of tests (negative values enabled).</p></li><li><p><code>external.mode=plain</code>: Self-improvement mode that just includes prompts and responses in the previous turns and a revision instruction.</p></li></ul></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=../../../docs/user-guide/ac-finetuning/ class="flex align-center"><img src=../../../icons/backward.svg class=book-icon alt=Backward>
<span>Multi-Agent Actor-Critic</span>
</a></span><span><a href=../../../docs/examples/quick-demo/ class="flex align-center"><span>CoMLRL Quick Demo</span>
<img src=../../../icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div></main></body></html>