<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Actor-Critic methods are widely used policy gradient approaches that employ generalized advantage estimation to estimate advantages, reducing the high variance and long rollout times in Monte Carlo methods, e.g., REINFORCE. Many LLM fine-tuning frameworks implement actor-critic training (e.g., trl, verl, LLaMA Factory).
IAC# Independent Actor-Critic (IAC) optimizes each agent’s policy independently while using joint returns from multiple agents. Each agent maintains its own actor and critic, other agents serve as part of the environment. The policy objective is:
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="/docs/user-guide/ac-finetuning/"><meta property="og:site_name" content="CoMLRL"><meta property="og:title" content="Multi-Agent Actor-Critic"><meta property="og:description" content="Actor-Critic methods are widely used policy gradient approaches that employ generalized advantage estimation to estimate advantages, reducing the high variance and long rollout times in Monte Carlo methods, e.g., REINFORCE. Many LLM fine-tuning frameworks implement actor-critic training (e.g., trl, verl, LLaMA Factory).
IAC# Independent Actor-Critic (IAC) optimizes each agent’s policy independently while using joint returns from multiple agents. Each agent maintains its own actor and critic, other agents serve as part of the environment. The policy objective is:"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2025-11-24T21:47:32-05:00"><meta itemprop=name content="Multi-Agent Actor-Critic"><meta itemprop=description content="Actor-Critic methods are widely used policy gradient approaches that employ generalized advantage estimation to estimate advantages, reducing the high variance and long rollout times in Monte Carlo methods, e.g., REINFORCE. Many LLM fine-tuning frameworks implement actor-critic training (e.g., trl, verl, LLaMA Factory).
IAC# Independent Actor-Critic (IAC) optimizes each agent’s policy independently while using joint returns from multiple agents. Each agent maintains its own actor and critic, other agents serve as part of the environment. The policy objective is:"><meta itemprop=dateModified content="2025-11-24T21:47:32-05:00"><meta itemprop=wordCount content="488"><title>Multi-Agent Actor-Critic | CoMLRL</title><link rel=icon href=../../../favicon.png><link rel=manifest href=../../../manifest.json><link rel=canonical href=../../../docs/user-guide/ac-finetuning/><link rel=stylesheet href=../../../book.min.84995bf041dbb6656f4541996b392ba4d647753aa90c42a8483168566c27189d.css><script defer src=../../../fuse.min.js></script><script defer src=../../../en.search.min.f4ef4d2c4a8d086cd4fc5fb4f035489429dd5bd0a4b57116d0a231e34dcc3e8d.js></script><link rel=stylesheet href=../../../css/sidebar.css><link rel=icon type=image/svg+xml href=../../../img/comlrl-icon.svg><link rel=icon type=image/png href=../../../img/comlrl-icon.png><link rel="shortcut icon" type=image/png href=../../../img/comlrl-icon.png><link rel=apple-touch-icon href=../../../img/comlrl-icon.png><script>document.addEventListener("DOMContentLoaded",function(){try{var t,n,s,o,e=document.querySelector("article.book-article");if(!e)return;if(n=document.querySelector(".book-header h3"),t=n?n.textContent.trim():(document.title||"").split("|")[0].trim(),!t)return;s=e.querySelector("h1"),(!s||s.textContent.trim()!==t)&&(o=document.createElement("h1"),o.textContent=t,e.insertBefore(o,e.firstChild)),function(){var t,e=document.getElementById("omlrl-footer");e||(e=document.createElement("div"),e.id="omlrl-footer",e.className="omlrl-footer",document.body.appendChild(e)),e.textContent="",t=document.createElement("strong"),t.textContent="© OpenMLRL. All rights reserved.",e.appendChild(t)}()}catch{}}),document.addEventListener("keydown",function(e){if(!e.metaKey&&!e.ctrlKey)return;var n,t=(e.key||"").toLowerCase();if(t==="k"){e.preventDefault(),n=document.querySelector(".book-search input"),n&&n.focus();return}if(t==="o"){e.preventDefault(),window.location.href="https://openmlrl.github.io";return}if(t==="g"){e.preventDefault(),window.location.href="https://github.com/OpenMLRL/CoMLRL";return}}),document.addEventListener("DOMContentLoaded",function(){var e,t=document.querySelectorAll("pre");t.forEach(function(e){var t=document.createElement("button");t.className="copy-code-button",t.textContent="Copy",t.addEventListener("click",function(){var n=e.querySelector("code"),s=n.textContent;navigator.clipboard.writeText(s).then(function(){t.textContent="Copied!",setTimeout(function(){t.textContent="Copy"},2e3)})}),e.style.position="relative",e.appendChild(t)}),e=document.createElement("a"),e.href="https://github.com/OpenMLRL/CoMLRL",e.className="github-corner",e.target="_blank",e.rel="noopener noreferrer",e.setAttribute("aria-label","View source on GitHub"),e.innerHTML='<svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><defs><linearGradient id="github-gradient" x1="0%" y1="0%" x2="100%" y2="100%"><stop offset="0%" style="stop-color:#9555af;stop-opacity:1" /><stop offset="100%" style="stop-color:#e091c4;stop-opacity:1" /></linearGradient></defs><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z" fill="url(#github-gradient)"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="#fff" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="#fff" class="octo-body"></path></svg>',document.body.appendChild(e)})</script></head><body dir=ltr class="book-kind-page book-type-docs"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=../../../><img src=../../../img/comlrl-logo.png alt=Logo><span>CoMLRL</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a>User Guide</a><ul><li><a href=../../../docs/user-guide/installation/>Installation</a></li><li><a href=../../../docs/user-guide/reinforce-finetuning/>Multi-Agent REINFORCE</a></li><li><a href=../../../docs/user-guide/ac-finetuning/ class=active>Multi-Agent Actor-Critic</a></li><li><a href=../../../docs/user-guide/multi-turn/>Multi-Turn Training</a></li></ul></li><li class=book-section-flat><a>Examples</a><ul><li><a href=../../../docs/examples/quick-demo/>CoMLRL Quick Demo</a></li></ul></li><li class=book-section-flat><a>Developers</a><ul><li><a href=../../../docs/dev/support/>Support</a></li><li><a href=../../../docs/dev/contributing/>Contributing</a></li><li><a href=../../../docs/dev/changelog/>Changelog</a></li></ul></li><li class=book-section-flat><a href=../../../docs/env/>Environments</a><ul><li><a href=https://github.com/OpenMLRL/LLM_Collab_Writing target=_blank rel=noopener>Article Writing</a></li><li><a href=https://github.com/OpenMLRL/LLM_Collab_Code_Generation target=_blank rel=noopener>Code Generation</a></li><li><a href=https://github.com/OpenMLRL/LLM_Collab_Code_Completion target=_blank rel=noopener>Code Completion</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=../../../icons/menu.svg class=book-icon alt=Menu></label><h3>Multi-Agent Actor-Critic</h3><label for=toc-control></label></div></header><article class="markdown book-article"><p>Actor-Critic methods are widely used policy gradient approaches that employ generalized advantage estimation to estimate advantages, reducing the high variance and long rollout times in Monte Carlo methods, e.g., REINFORCE. Many LLM fine-tuning frameworks implement actor-critic training (e.g., <a href=https://huggingface.co/docs/trl>trl</a>, <a href=https://verl.readthedocs.io/en/latest/>verl</a>, <a href=https://llamafactory.readthedocs.io/en/latest/advanced/trainers.html>LLaMA Factory</a>).</p><h2 id=iac>IAC<a class=anchor href=#iac>#</a></h2><p>Independent Actor-Critic (IAC) optimizes each agent&rsquo;s policy independently while using joint returns from multiple agents. Each agent maintains its own actor and critic, other agents serve as part of the environment. The policy objective is:</p><div class=book-katex>\[
J(\theta_i) = \mathbb{E}_{o_{i,0} \sim \mathcal{D}, h_i \sim \pi_{\theta_i}}\left[\log \pi_{\theta_i}(a_{i,t}|h_{i,t}) \cdot \delta_{i,t} + \beta \mathcal{H}(\pi_{\theta_i})\right]
\]</div><link rel=stylesheet href=../../../katex/katex.min.css><script defer src=../../../katex/katex.min.js></script><script defer src=../../../katex/auto-render.min.js onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"\\begin{gather}",right:"\\end{gather}",display:!0}],throwOnError:!1})'></script><p>where <span class=book-katex>\( \delta_{i,t} = r_{i,t} + \gamma V_{\phi_i}(h_{i,t+1}) - V_{\phi_i}(h_{i,t}) \)</span> is the (single-step) temporal difference error, <span class=book-katex>\( \gamma \)</span> is the discount factor, and <span class=book-katex>\( \mathcal{H}(\pi_{\theta_i}) \)</span> is the entropy bonus with coefficient <span class=book-katex>\( \beta \)</span>.</p><p>CoMLRL supports two IAC architectures for critic implementation:</p><ul><li><p><strong>Separate Critic</strong>: Uses an independent model dedicated to value estimation, completely separate from the actor. It provides more stable training but requires longer training time and larger VRAM usage.</p></li><li><p><strong>Value Head</strong>: Attaches a small value prediction head directly to the actor model, sharing the base model&rsquo;s representations. It reduces VRAM usage, but since both actor and critic share the same model, gradient errors can be amplified during training.</p></li></ul><blockquote class="book-hint info"><p><strong>IACConfig</strong> provides parameters for configuring Independent Actor-Critic training:</p><ul><li><code>output_dir</code>: Directory to save outputs</li><li><code>actor_learning_rate</code>: Learning rate for actor</li><li><code>critic_learning_rate</code>: Learning rate for critic</li><li><code>weight_decay</code>: Weight decay for AdamW optimizer</li><li><code>adam_beta1</code>, <code>adam_beta2</code>, <code>adam_epsilon</code>: Adam optimizer parameters</li><li><code>max_grad_norm</code>: Maximum gradient norm for clipping</li><li><code>rollout_buffer_size</code>: Number of samples to collect before update</li><li><code>mini_batch_size</code>: Mini-batch size for policy updates</li><li><code>ac_epochs</code>: Number of optimization epochs per rollout</li><li><code>value_clip_range</code>: Clipping range for value function</li><li><code>value_loss_coef</code>: Coefficient for value loss</li><li><code>entropy_coef</code>: Coefficient for entropy bonus</li><li><code>advantage_normalization</code>: Whether to normalize advantages</li><li><code>max_new_tokens</code>: Maximum new tokens to generate</li><li><code>temperature</code>: Temperature for sampling</li><li><code>top_p</code>: Top-p for nucleus sampling</li><li><code>top_k</code>: Top-k for sampling</li><li><code>do_sample</code>: Whether to use sampling</li><li><code>num_train_epochs</code>: Number of training epochs</li><li><code>per_device_train_batch_size</code>: Batch size per device, must be 1</li><li><code>use_separate_critic</code>: Whether to use separate critic model</li><li><code>critic_model_name_or_path</code>: Model identifier for separate critic</li><li><code>critic_value_head_hidden_dim</code>: Hidden dimension for critic value head</li><li><code>value_head_hidden_dim</code>: Hidden dimension for actor value head</li><li><code>num_agents</code>: Number of agents</li><li><code>num_turns</code>: Number of turns, currently only supports 1</li><li><code>reward_norm_eps</code>: Epsilon for reward normalization</li></ul></blockquote><blockquote class="book-hint info"><p><strong>IACTrainer</strong> trains agents using Independent Actor-Critic:</p><ul><li><code>model</code>: Model string or PreTrainedModel instance (required for single-agent, must be string for multi-agent)</li><li><code>tokenizer</code>: The tokenizer (required)</li><li><code>reward_func</code>: Callable that returns a list of floats (required)</li><li><code>reward_processor</code>: Optional processor to apply to rewards</li><li><code>formatters</code>: Single callable or list of callables for each agent to format dataset items into prompts</li><li><code>args</code>: Instance of <code>IACConfig</code> (optional)</li><li><code>train_dataset</code>: Training dataset (required)</li><li><code>eval_dataset</code>: Evaluation dataset (optional)</li><li><code>model_config</code>: Model configuration dict (optional)</li><li><code>wandb_config</code>: Configuration for Weights & Biases logging (optional)</li><li><code>metrics_callback</code>: Optional callback for custom metrics</li></ul></blockquote><blockquote class="book-hint warning"><p>For simplicity, IAC computes the policy gradient using the current policy&rsquo;s samples without importance sampling or ratio clipping.</p></blockquote><blockquote class="book-hint warning"><p>The trainer enforces <code>per_device_train_batch_size=1</code> and currently only supports single-turn training (<code>num_turns=1</code>).</p></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=../../../docs/user-guide/reinforce-finetuning/ class="flex align-center"><img src=../../../icons/backward.svg class=book-icon alt=Backward>
<span>Multi-Agent REINFORCE</span>
</a></span><span><a href=../../../docs/user-guide/multi-turn/ class="flex align-center"><span>Multi-Turn Training</span>
<img src=../../../icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div></main></body></html>