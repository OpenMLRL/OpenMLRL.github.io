<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Actor-Critic (AC) methods are widely-used policy gradient that employ critics to facilitate training. AC methods can achieve lower variance and better sample efficiency than REINFORCE, but this requires careful design and tuning of the critic to ensure stable training. In Multi-Agent Reinforcement Learning (MARL), Actor-Critic methods can be instantiated as Multi-Agent Actor-Critic (MAAC) and Independent Actor-Critic (IAC).
MAAC# Multi-Agent Actor-Critic (MAAC) uses a Centralized Critic (CC) across agents to evaluate the values of joint histories \( V_{\boldsymbol{\phi}}(\mathbf{h}_t) \) or joint history-action pairs \( Q_{\boldsymbol{\psi}}(\mathbf{h}_t, \mathbf{a}_t) \). The policy gradient of each agent is:
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="/docs/user-guide/multi-agent-actor-critic/"><meta property="og:site_name" content="CoMLRL"><meta property="og:title" content="Multi-Agent Actor-Critic"><meta property="og:description" content="Actor-Critic (AC) methods are widely-used policy gradient that employ critics to facilitate training. AC methods can achieve lower variance and better sample efficiency than REINFORCE, but this requires careful design and tuning of the critic to ensure stable training. In Multi-Agent Reinforcement Learning (MARL), Actor-Critic methods can be instantiated as Multi-Agent Actor-Critic (MAAC) and Independent Actor-Critic (IAC).
MAAC# Multi-Agent Actor-Critic (MAAC) uses a Centralized Critic (CC) across agents to evaluate the values of joint histories \( V_{\boldsymbol{\phi}}(\mathbf{h}_t) \) or joint history-action pairs \( Q_{\boldsymbol{\psi}}(\mathbf{h}_t, \mathbf{a}_t) \). The policy gradient of each agent is:"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2026-02-13T16:23:08-05:00"><meta itemprop=name content="Multi-Agent Actor-Critic"><meta itemprop=description content="Actor-Critic (AC) methods are widely-used policy gradient that employ critics to facilitate training. AC methods can achieve lower variance and better sample efficiency than REINFORCE, but this requires careful design and tuning of the critic to ensure stable training. In Multi-Agent Reinforcement Learning (MARL), Actor-Critic methods can be instantiated as Multi-Agent Actor-Critic (MAAC) and Independent Actor-Critic (IAC).
MAAC# Multi-Agent Actor-Critic (MAAC) uses a Centralized Critic (CC) across agents to evaluate the values of joint histories \( V_{\boldsymbol{\phi}}(\mathbf{h}_t) \) or joint history-action pairs \( Q_{\boldsymbol{\psi}}(\mathbf{h}_t, \mathbf{a}_t) \). The policy gradient of each agent is:"><meta itemprop=dateModified content="2026-02-13T16:23:08-05:00"><meta itemprop=wordCount content="974"><title>Multi-Agent Actor-Critic | CoMLRL</title><link rel=icon href=../../../favicon.png><link rel=manifest href=../../../manifest.json><link rel=canonical href=../../../docs/user-guide/multi-agent-actor-critic/><link rel=stylesheet href=../../../book.min.cf77f74a94d5d5797fce47774a1371342c8a7d81e76bbc31dd6c3ba4e268e9a2.css><script defer src=../../../fuse.min.js></script><script defer src=../../../en.search.min.a2d091a143a3683cc0ac8052c8fc4c5efb5598a776bc019e77cd12eceaa93413.js></script><link rel=stylesheet href=../../../css/sidebar.css><link rel=icon type=image/svg+xml href=../../../img/comlrl-icon.svg><link rel=icon type=image/png href=../../../img/comlrl-icon.png><link rel="shortcut icon" type=image/png href=../../../img/comlrl-icon.png><link rel=apple-touch-icon href=../../../img/comlrl-icon.png><script>document.addEventListener("DOMContentLoaded",function(){try{var t,n,s,o,e=document.querySelector("article.book-article");if(!e)return;if(n=document.querySelector(".book-header h3"),t=n?n.textContent.trim():(document.title||"").split("|")[0].trim(),!t)return;s=e.querySelector("h1"),(!s||s.textContent.trim()!==t)&&(o=document.createElement("h1"),o.textContent=t,e.insertBefore(o,e.firstChild)),function(){var t,e=document.getElementById("omlrl-footer");e||(e=document.createElement("div"),e.id="omlrl-footer",e.className="omlrl-footer",document.body.appendChild(e)),e.textContent="",t=document.createElement("strong"),t.textContent="Â© OpenMLRL. All rights reserved.",e.appendChild(t)}()}catch{}}),document.addEventListener("keydown",function(e){if(!e.metaKey&&!e.ctrlKey)return;var n,t=(e.key||"").toLowerCase();if(t==="k"){e.preventDefault(),n=document.querySelector(".book-search input"),n&&n.focus();return}if(t==="o"){e.preventDefault(),window.location.href="https://openmlrl.github.io";return}if(t==="g"){e.preventDefault(),window.location.href="https://github.com/OpenMLRL/CoMLRL";return}}),document.addEventListener("DOMContentLoaded",function(){var e,t=document.querySelectorAll("pre");t.forEach(function(e){var t=document.createElement("button");t.className="copy-code-button",t.textContent="Copy",t.addEventListener("click",function(){var n=e.querySelector("code"),s=n.textContent;navigator.clipboard.writeText(s).then(function(){t.textContent="Copied!",setTimeout(function(){t.textContent="Copy"},2e3)})}),e.style.position="relative",e.appendChild(t)}),e=document.createElement("a"),e.href="https://github.com/OpenMLRL/CoMLRL",e.className="github-corner",e.target="_blank",e.rel="noopener noreferrer",e.setAttribute("aria-label","View source on GitHub"),e.innerHTML='<svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><defs><linearGradient id="github-gradient" x1="0%" y1="0%" x2="100%" y2="100%"><stop offset="0%" style="stop-color:#9555af;stop-opacity:1" /><stop offset="100%" style="stop-color:#e091c4;stop-opacity:1" /></linearGradient></defs><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z" fill="url(#github-gradient)"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="#fff" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="#fff" class="octo-body"></path></svg>',document.body.appendChild(e)})</script></head><body dir=ltr class="book-kind-page book-type-docs"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=../../../><img src=../../../img/comlrl-logo.png alt=Logo><span>CoMLRL</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a>User Guide</a><ul><li><a href=../../../docs/user-guide/installation/>Installation</a></li><li><a href=../../../docs/user-guide/model-loading/>Model Loading</a></li><li><a href=../../../docs/user-guide/multi-agent-reinforce/>Multi-Agent REINFORCE</a></li><li><a href=../../../docs/user-guide/multi-agent-actor-critic/ class=active>Multi-Agent Actor-Critic</a></li><li><a href=../../../docs/user-guide/multi-turn-training/>Multi-Turn Training</a></li></ul></li><li class=book-section-flat><a>Examples</a><ul><li><a href=../../../docs/examples/comlrl-quick-start/>CoMLRL Quick Start</a></li></ul></li><li class=book-section-flat><a>Developers</a><ul><li><a href=../../../docs/dev/support/>Support</a></li><li><a href=../../../docs/dev/contributing/>Contributing</a></li><li><a href=../../../docs/dev/changelog/>Changelog</a></li></ul></li><li class=book-section-flat><a href=../../../docs/env/>Environments</a><ul><li><a href=https://github.com/OpenMLRL/LLM_Collab_Writing target=_blank rel=noopener>Writing</a></li><li><a href=https://github.com/OpenMLRL/LLM_Collab_Code_Generation target=_blank rel=noopener>Coding</a></li><li><a href=https://github.com/OpenMLRL/LLM_Collab_Minecraft target=_blank rel=noopener>Minecraft</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=../../../icons/menu.svg class=book-icon alt=Menu></label><h3>Multi-Agent Actor-Critic</h3><label for=toc-control></label></div></header><article class="markdown book-article"><p>Actor-Critic (AC) methods are widely-used policy gradient that employ critics to facilitate training.
AC methods can achieve lower variance and better sample efficiency than REINFORCE, but this requires careful design and tuning of the critic to ensure stable training.
In Multi-Agent Reinforcement Learning (MARL), Actor-Critic methods can be instantiated as Multi-Agent Actor-Critic (MAAC) and Independent Actor-Critic (IAC).</p><h2 id=maac>MAAC<a class=anchor href=#maac>#</a></h2><p>Multi-Agent Actor-Critic (MAAC) uses a Centralized Critic (CC) across agents to evaluate the values of joint histories <span class=book-katex>\( V_{\boldsymbol{\phi}}(\mathbf{h}_t) \)</span><link rel=stylesheet href=../../../katex/katex.min.css><script defer src=../../../katex/katex.min.js></script><script defer src=../../../katex/auto-render.min.js onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"\\begin{gather}",right:"\\end{gather}",display:!0}],throwOnError:!1})'></script> or joint history-action pairs <span class=book-katex>\( Q_{\boldsymbol{\psi}}(\mathbf{h}_t, \mathbf{a}_t) \)</span>.
The policy gradient of each agent is:</p><div class=book-katex>\[
\nabla_{\theta_i} J(\theta_i) = \mathbb{E}_{\boldsymbol{\pi}}\left[\sum_{t=0}^{H-1} \nabla_{\theta_i} \log \pi_{\theta_i}(a_{i,t}|h_{i,t})\,\boldsymbol{\delta}_t\right]
\]</div><p>where <span class=book-katex>\( \boldsymbol{\delta}_t = r_t + \gamma V_{\boldsymbol{\phi}}(\mathbf{h}_{t+1}) - V_{\boldsymbol{\phi}}(\mathbf{h}_{t}) \)</span>, and the critic is updated by:</p><div class=book-katex>\[
\mathcal{L}(\boldsymbol{\phi}) = \mathbb{E}_{\boldsymbol{\pi}}\left[\sum_{t=0}^{H-1} \big(r_t + \gamma V_{\phi}(\mathbf{h}_{t+1}) - V_{\phi}(\mathbf{h}_{t})\big)^2\right].
\]</div><blockquote class="book-hint info"><p><strong>MAACConfig</strong> parameters:</p><ul><li><code>num_agents</code>: Number of agents</li><li><code>num_turns</code>: Number of turns</li><li><code>critic_type</code>: Critic target type (<code>v</code> for V(h), <code>q</code> for Q(h,a))</li><li><code>num_train_epochs</code>: Number of training epochs</li><li><code>agent_learning_rate</code>: Learning rate for agents</li><li><code>critic_learning_rate</code>: Learning rate for shared critic</li><li><code>value_loss_coef</code>: Weight on critic loss</li><li><code>advantage_normalization</code>: Whether to normalize advantages before updates</li><li><code>rollout_buffer_size</code>: Number of samples to collect per agent before an update</li><li><code>train_batch_size</code>: Mini-batch size within each update</li><li><code>max_new_tokens</code>: Maximum tokens to generate per completion</li><li><code>temperature</code>: Temperature for sampling</li><li><code>top_p</code>: Top-p for nucleus sampling</li><li><code>top_k</code>: Top-k for sampling</li><li><code>num_generations</code>: Number of generations per prompt per agent</li><li><code>external_prompt_passthrough</code>: Use external prompts directly in multi-turn</li><li><code>discount</code>: Discount factor for multi-turn returns</li><li><code>early_termination_threshold</code>: Optional early-stop threshold for multi-turn</li><li><code>eval_interval</code>: Evaluation interval (in training batches)</li><li><code>eval_num_samples</code>: Number of evaluation samples per interval</li><li><code>eval_batch_size</code>: Eval dataloader batch size</li><li><code>logging_steps</code>: Log every N training batches</li></ul></blockquote><blockquote class="book-hint info"><p><strong>MAACTrainer</strong> setup:</p><ul><li><code>agent_model</code> or <code>agents</code>: Actor model identifier string for homogeneous agents, or list of agent models (multi-agent <code>agent_model</code> must be a string)</li><li><code>critic_model</code> or <code>critics</code>: Required single shared critic (either one identifier or a 1-element list)</li><li><code>tokenizer</code>: Tokenizer (required)</li><li><code>reward_func</code>: Callable returning rewards (required)</li><li><code>reward_processor</code>: Optional reward post-processor</li><li><code>formatters</code>: Single callable or list for per-agent prompt formatting</li><li><code>args</code>: Instance of <code>MAACConfig</code> (optional)</li><li><code>train_dataset</code>: Training dataset (required)</li><li><code>eval_dataset</code>: Optional evaluation dataset</li><li><code>model_config</code>: Extra model kwargs (optional)</li><li><code>wandb_config</code>: Weights & Biases logging config (optional)</li><li><code>metrics_callback</code>: Optional callback for custom metrics</li></ul></blockquote><blockquote class="book-hint warning"><p>For simplicity, IAC computes the policy gradient using the current policy&rsquo;s samples without importance sampling or ratio clipping. The <code>value_clip_range</code> is not applicable in MAAC.</p></blockquote><blockquote class="book-hint warning"><p>The trainer uses a fixed training DataLoader batch size of 1. For <code>num_turns > 1</code>, provide an <code>external_transition</code> and set <code>num_generations=1</code>. The training use batch gradient descent by default, where <code>train_batch_size</code>=<code>rollout_buffer_size</code>.</p></blockquote><h2 id=iac>IAC<a class=anchor href=#iac>#</a></h2><p>Independent Actor-Critic (IAC) optimizes each agent&rsquo;s policy independently while using joint returns from multiple agents. Each agent maintains its own actor and critic, other agents serve as part of the environment. The policy gradient for each agent is:</p><div class=book-katex>\[
\nabla_{\theta_i} J(\theta_i) = \mathbb{E}_{\boldsymbol{\pi}}\left[\sum_{t=0}^{H-1} \nabla_{\theta_i} \log \pi_{\theta_i}(a_{i,t}|h_{i,t})\,\delta_{i,t}\right]
\]</div><p>where <span class=book-katex>\( \delta_{i,t} = r_t + \gamma V_{\phi_i}(h_{i,t+1}) - V_{\phi_i}(h_{i,t}) \)</span>.</p><p>CoMLRL supports two IAC variants:</p><ul><li><p><strong>Separate Critic</strong>: Uses an independent model for value estimation separate from the actor. It provides more stable training but occupies larger storage and VRAM usage.</p></li><li><p><strong>Shared Model</strong>: Attaches a small value-head to the transformer backbone, sharing the actor model&rsquo;s history (or history-action) representations to reduce the space costs.</p></li></ul><p>The critics are updated by minimizing the TD error:</p><div class=book-katex>\[
\mathcal{L}(\phi_i) = \mathbb{E}_{\boldsymbol{\pi}}\left[\sum_{t=0}^{H-1} \big(r_t + \gamma V_{\phi_i}(h_{i,t+1}) - V_{\phi_i}(h_{i,t})\big)^2\right].
\]</div><p>When using shared model <code>use_separate_critic=false</code>, a value clip <code>value_clip_range</code> can be applied to improve training stability.</p><div class=book-katex>\[
L(\phi_i) = \max\Big( (V_{\phi_i}(h_t) - \hat{V}_t)^2,\ (V_{\phi_i}^{\text{clip}}(h_t) - \hat{V}_t)^2 \Big)
\\ V_{\phi_i}^{\text{clip}}(h_t) = V_{\phi_i}^{\text{old}}(h_t) + \mathrm{clip}(V_{\phi_i}(h_t) - V_{\phi_i}^{\text{old}}(h_t), -\epsilon, \epsilon),
\]</div><blockquote class="book-hint info"><p><strong>IACConfig</strong> provides parameters for configuring Independent Actor-Critic training:</p><ul><li><code>num_agents</code>: Number of agents</li><li><code>num_turns</code>: Number of turns</li><li><code>num_train_epochs</code>: Number of training epochs</li><li><code>agent_learning_rate</code>: Learning rate for agents</li><li><code>critic_learning_rate</code>: Learning rate for critic</li><li><code>value_loss_coef</code>: Coefficient for value loss</li><li><code>value_clip_range</code>: Clipping range for value function</li><li><code>advantage_normalization</code>: Whether to normalize advantages</li><li><code>rollout_buffer_size</code>: Number of samples to collect before update</li><li><code>train_batch_size</code>: Mini-batch size for policy updates</li><li><code>max_new_tokens</code>: Maximum new tokens to generate</li><li><code>temperature</code>: Temperature for sampling</li><li><code>top_p</code>: Top-p for nucleus sampling</li><li><code>top_k</code>: Top-k for sampling</li><li><code>num_generations</code>: Number of generations per prompt per agent</li><li><code>use_separate_critic</code>: Whether to use separate critic model</li><li><code>critic_type</code>: Critic target type (<code>v</code> for V(h), <code>q</code> for Q(h,a))</li><li><code>critic_value_head_hidden_dim</code>: Hidden dimension for critic value head</li><li><code>value_head_hidden_dim</code>: Hidden dimension for value head in shared-critic mode</li><li><code>external_prompt_passthrough</code>: Use external prompts directly in multi-turn</li><li><code>discount</code>: Discount factor for multi-turn returns</li><li><code>early_termination_threshold</code>: Optional early-stop threshold for multi-turn</li><li><code>eval_interval</code>: Evaluation interval (in training batches)</li><li><code>eval_num_samples</code>: Number of evaluation samples per interval</li><li><code>eval_batch_size</code>: Eval dataloader batch size</li><li><code>logging_steps</code>: Log every N training batches</li></ul></blockquote><blockquote class="book-hint info"><p><strong>IACTrainer</strong> trains agents using Independent Actor-Critic:</p><ul><li><code>agent_model</code> or <code>agents</code>: Model identifier string for homogeneous agents, or list of agent models (multi-agent <code>agent_model</code> must be a string)</li><li><code>critic_model</code> or <code>critics</code>: Critic identifier or list of critic models when <code>use_separate_critic=true</code></li><li><code>tokenizer</code>: The tokenizer (required)</li><li><code>reward_func</code>: Callable that returns a list of floats (required)</li><li><code>reward_processor</code>: Optional processor to apply to rewards</li><li><code>formatters</code>: Single callable or list of callables for each agent to format dataset items into prompts</li><li><code>args</code>: Instance of <code>IACConfig</code> (optional)</li><li><code>train_dataset</code>: Training dataset (required)</li><li><code>eval_dataset</code>: Evaluation dataset (optional)</li><li><code>model_config</code>: Model configuration dict (optional)</li><li><code>wandb_config</code>: Configuration for Weights & Biases logging (optional)</li><li><code>metrics_callback</code>: Optional callback for custom metrics</li><li><code>external_transition</code>: Optional transition function required for multi-turn training</li></ul></blockquote><blockquote class="book-hint warning"><p>For simplicity, IAC computes the policy gradient using the current policy&rsquo;s samples without importance sampling or ratio clipping. In shared-critic mode (<code>use_separate_critic=false</code>), value heads are attached to the actor models (do not pass <code>critic_model</code>/<code>critics</code>; passing them raises an error), and agents may be homogeneous or heterogeneous; this mode can be less stable, and <code>value_clip_range</code> only applies there. In separate-critic mode (<code>use_separate_critic=true</code>), pass a <code>critics</code> list with length equal to <code>num_agents</code> or a single <code>critic_model</code> to be broadcast; critic models may differ from actor models.</p></blockquote><blockquote class="book-hint warning"><p>The trainer uses a fixed training DataLoader batch size of 1. For <code>num_turns > 1</code>, provide an <code>external_transition</code> and set <code>num_generations=1</code>. The training use batch gradient descent by default, where <code>train_batch_size</code>=<code>rollout_buffer_size</code>.</p></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=../../../docs/user-guide/multi-agent-reinforce/ class="flex align-center"><img src=../../../icons/backward.svg class=book-icon alt=Backward>
<span>Multi-Agent REINFORCE</span>
</a></span><span><a href=../../../docs/user-guide/multi-turn-training/ class="flex align-center"><span>Multi-Turn Training</span>
<img src=../../../icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div></main></body></html>