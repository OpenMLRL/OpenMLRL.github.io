<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="REINFORCE optimizes the policy directly using sampled returns. An action-independent baseline can be included to reduce variance for REINFORCE methods. REINFORCE methods have been widely used to fine-tune LLMs because of their simplicity and effectiveness, e.g., GRPO, Dr. GRPO, RLOO, ReMax, TreeRPO, and REINFORCE++.
MAREINFORCE# In the LLM collaboration setting, REINFORCE can be extended to optimize each agent’s policy with joint returns from multiple agents.
MAREINFORCE: The naive Multi‑Agent REINFORCE without a baseline can be expressed by: \[ J(\theta_i) = \mathbb{E}_{\mathbf{o}_0 \sim \mathcal{D}, \mathbf{h}^\mathcal{G} \sim \mathbf{\pi}_{\mathbf{\theta}}} \Bigg[\frac{1}{|\mathcal{G}|}\sum_{g \in \mathcal{G}} R^{(g)}_t \cdot \log \pi_{\theta_i}(a^{(g)}_{i,t}\mid h_{i,t})\Bigg]. \] These classes are derived from comlrl.trainers.magrpo.MAGRPOTrainer. Interfaces for the trainer and configuration classes are the same as MAGRPOTrainer and MAGRPOConfig.
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="/docs/user-guide/reinforce-finetuning/"><meta property="og:site_name" content="CoMLRL"><meta property="og:title" content="Multi-Agent REINFORCE"><meta property="og:description" content="REINFORCE optimizes the policy directly using sampled returns. An action-independent baseline can be included to reduce variance for REINFORCE methods. REINFORCE methods have been widely used to fine-tune LLMs because of their simplicity and effectiveness, e.g., GRPO, Dr. GRPO, RLOO, ReMax, TreeRPO, and REINFORCE++.
MAREINFORCE# In the LLM collaboration setting, REINFORCE can be extended to optimize each agent’s policy with joint returns from multiple agents.
MAREINFORCE: The naive Multi‑Agent REINFORCE without a baseline can be expressed by: \[ J(\theta_i) = \mathbb{E}_{\mathbf{o}_0 \sim \mathcal{D}, \mathbf{h}^\mathcal{G} \sim \mathbf{\pi}_{\mathbf{\theta}}} \Bigg[\frac{1}{|\mathcal{G}|}\sum_{g \in \mathcal{G}} R^{(g)}_t \cdot \log \pi_{\theta_i}(a^{(g)}_{i,t}\mid h_{i,t})\Bigg]. \] These classes are derived from comlrl.trainers.magrpo.MAGRPOTrainer. Interfaces for the trainer and configuration classes are the same as MAGRPOTrainer and MAGRPOConfig."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2025-11-24T21:47:32-05:00"><meta itemprop=name content="Multi-Agent REINFORCE"><meta itemprop=description content="REINFORCE optimizes the policy directly using sampled returns. An action-independent baseline can be included to reduce variance for REINFORCE methods. REINFORCE methods have been widely used to fine-tune LLMs because of their simplicity and effectiveness, e.g., GRPO, Dr. GRPO, RLOO, ReMax, TreeRPO, and REINFORCE++.
MAREINFORCE# In the LLM collaboration setting, REINFORCE can be extended to optimize each agent’s policy with joint returns from multiple agents.
MAREINFORCE: The naive Multi‑Agent REINFORCE without a baseline can be expressed by: \[ J(\theta_i) = \mathbb{E}_{\mathbf{o}_0 \sim \mathcal{D}, \mathbf{h}^\mathcal{G} \sim \mathbf{\pi}_{\mathbf{\theta}}} \Bigg[\frac{1}{|\mathcal{G}|}\sum_{g \in \mathcal{G}} R^{(g)}_t \cdot \log \pi_{\theta_i}(a^{(g)}_{i,t}\mid h_{i,t})\Bigg]. \] These classes are derived from comlrl.trainers.magrpo.MAGRPOTrainer. Interfaces for the trainer and configuration classes are the same as MAGRPOTrainer and MAGRPOConfig."><meta itemprop=dateModified content="2025-11-24T21:47:32-05:00"><meta itemprop=wordCount content="544"><title>Multi-Agent REINFORCE | CoMLRL</title><link rel=icon href=../../../favicon.png><link rel=manifest href=../../../manifest.json><link rel=canonical href=../../../docs/user-guide/reinforce-finetuning/><link rel=stylesheet href=../../../book.min.84995bf041dbb6656f4541996b392ba4d647753aa90c42a8483168566c27189d.css><script defer src=../../../fuse.min.js></script><script defer src=../../../en.search.min.3353e481074e74bda0d567bbb27b88ffbb2e6c2f212428c10f4de607550a2a0e.js></script><link rel=stylesheet href=../../../css/sidebar.css><link rel=icon type=image/svg+xml href=../../../img/comlrl-icon.svg><link rel=icon type=image/png href=../../../img/comlrl-icon.png><link rel="shortcut icon" type=image/png href=../../../img/comlrl-icon.png><link rel=apple-touch-icon href=../../../img/comlrl-icon.png><script>document.addEventListener("DOMContentLoaded",function(){try{var t,n,s,o,e=document.querySelector("article.book-article");if(!e)return;if(n=document.querySelector(".book-header h3"),t=n?n.textContent.trim():(document.title||"").split("|")[0].trim(),!t)return;s=e.querySelector("h1"),(!s||s.textContent.trim()!==t)&&(o=document.createElement("h1"),o.textContent=t,e.insertBefore(o,e.firstChild)),function(){var t,e=document.getElementById("omlrl-footer");e||(e=document.createElement("div"),e.id="omlrl-footer",e.className="omlrl-footer",document.body.appendChild(e)),e.textContent="",t=document.createElement("strong"),t.textContent="© OpenMLRL. All rights reserved.",e.appendChild(t)}()}catch{}}),document.addEventListener("keydown",function(e){if(!e.metaKey&&!e.ctrlKey)return;var n,t=(e.key||"").toLowerCase();if(t==="k"){e.preventDefault(),n=document.querySelector(".book-search input"),n&&n.focus();return}if(t==="o"){e.preventDefault(),window.location.href="https://openmlrl.github.io";return}if(t==="g"){e.preventDefault(),window.location.href="https://github.com/OpenMLRL/CoMLRL";return}}),document.addEventListener("DOMContentLoaded",function(){var e,t=document.querySelectorAll("pre");t.forEach(function(e){var t=document.createElement("button");t.className="copy-code-button",t.textContent="Copy",t.addEventListener("click",function(){var n=e.querySelector("code"),s=n.textContent;navigator.clipboard.writeText(s).then(function(){t.textContent="Copied!",setTimeout(function(){t.textContent="Copy"},2e3)})}),e.style.position="relative",e.appendChild(t)}),e=document.createElement("a"),e.href="https://github.com/OpenMLRL/CoMLRL",e.className="github-corner",e.target="_blank",e.rel="noopener noreferrer",e.setAttribute("aria-label","View source on GitHub"),e.innerHTML='<svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><defs><linearGradient id="github-gradient" x1="0%" y1="0%" x2="100%" y2="100%"><stop offset="0%" style="stop-color:#9555af;stop-opacity:1" /><stop offset="100%" style="stop-color:#e091c4;stop-opacity:1" /></linearGradient></defs><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z" fill="url(#github-gradient)"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="#fff" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="#fff" class="octo-body"></path></svg>',document.body.appendChild(e)})</script></head><body dir=ltr class="book-kind-page book-type-docs"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=../../../><img src=../../../img/comlrl-logo.png alt=Logo><span>CoMLRL</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a>User Guide</a><ul><li><a href=../../../docs/user-guide/installation/>Installation</a></li><li><a href=../../../docs/user-guide/reinforce-finetuning/ class=active>Multi-Agent REINFORCE</a></li><li><a href=../../../docs/user-guide/ac-finetuning/>Multi-Agent Actor-Critic</a></li><li><a href=../../../docs/user-guide/multi-turn/>Multi-Turn Training</a></li></ul></li><li class=book-section-flat><a>Examples</a><ul><li><a href=../../../docs/examples/quick-demo/>CoMLRL Quick Demo</a></li></ul></li><li class=book-section-flat><a>Developers</a><ul><li><a href=../../../docs/dev/support/>Support</a></li><li><a href=../../../docs/dev/contributing/>Contributing</a></li><li><a href=../../../docs/dev/changelog/>Changelog</a></li></ul></li><li class=book-section-flat><a href=../../../docs/env/>Environments</a><ul><li><a href=https://github.com/OpenMLRL/LLM_Collab_Writing target=_blank rel=noopener>Article Writing</a></li><li><a href=https://github.com/OpenMLRL/LLM_Collab_Code_Generation target=_blank rel=noopener>Code Generation</a></li><li><a href=https://github.com/OpenMLRL/LLM_Collab_Code_Completion target=_blank rel=noopener>Code Completion</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=../../../icons/menu.svg class=book-icon alt=Menu></label><h3>Multi-Agent REINFORCE</h3><label for=toc-control></label></div></header><article class="markdown book-article"><p>REINFORCE optimizes the policy directly using sampled returns. An action-independent baseline can be included to reduce variance for REINFORCE methods. REINFORCE methods have been widely used to fine-tune LLMs because of their simplicity and effectiveness, e.g., <a href=https://arxiv.org/pdf/2402.03300>GRPO</a>, <a href=https://arxiv.org/abs/2503.20783>Dr. GRPO</a>, <a href="https://openreview.net/forum?id=r1lgTGL5DE">RLOO</a>, <a href=https://arxiv.org/abs/2310.1050>ReMax</a>, <a href=https://arxiv.org/abs/2506.05183>TreeRPO</a>, and <a href=https://arxiv.org/abs/2501.03262>REINFORCE++</a>.</p><h2 id=mareinforce>MAREINFORCE<a class=anchor href=#mareinforce>#</a></h2><p>In the LLM collaboration setting, REINFORCE can be extended to optimize each agent&rsquo;s policy with joint returns from multiple agents.</p><ul><li><strong>MAREINFORCE</strong>: The naive Multi‑Agent REINFORCE without a baseline can be expressed by:</li></ul><div class=book-katex>\[
J(\theta_i) = \mathbb{E}_{\mathbf{o}_0 \sim \mathcal{D}, \mathbf{h}^\mathcal{G} \sim \mathbf{\pi}_{\mathbf{\theta}}}
\Bigg[\frac{1}{|\mathcal{G}|}\sum_{g \in \mathcal{G}} R^{(g)}_t \cdot \log \pi_{\theta_i}(a^{(g)}_{i,t}\mid h_{i,t})\Bigg].
\]</div><link rel=stylesheet href=../../../katex/katex.min.css><script defer src=../../../katex/katex.min.js></script><script defer src=../../../katex/auto-render.min.js onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"\\begin{gather}",right:"\\end{gather}",display:!0}],throwOnError:!1})'></script><blockquote class="book-hint success"><p>These classes are derived from <code>comlrl.trainers.magrpo.MAGRPOTrainer</code>. Interfaces for the trainer and configuration classes are the same as <code>MAGRPOTrainer</code> and <code>MAGRPOConfig</code>.</p></blockquote><h2 id=magrpo>MAGRPO<a class=anchor href=#magrpo>#</a></h2><p>Multi‑Agent Group‑Relative Policy Optimization optimizes each agent with a group‑relative baseline computed among sibling joint actions at the same node.</p><div class=book-katex>\[
J(\theta_i) = \mathbb{E}_{\mathbf{o}_0 \sim \mathcal{D}, \mathbf{h}^\mathcal{G} \sim \mathbf{\pi}_{\mathbf{\theta}}}\left[ \frac{1}{|\mathcal{G}|}\sum_{g \in \mathcal{G}}
\Big(R^{(g)}_t - \operatorname{mean}(R^{\mathcal{G}}_t)\Big)
\cdot \log \pi_{\theta_i}\big(a^{(g)}_{i,t} \mid h_{i,t}\big) \right].
\]</div><blockquote class="book-hint info"><p><strong>MAGRPOConfig</strong> inherits from <code>TrainingArguments</code> and provides parameters for both single-turn and multi-turn training:</p><ul><li><code>num_agents</code>: Number of agents (default: 2)</li><li><code>num_generations</code>: Number of generations to sample per prompt for each agent (default: 4)</li><li><code>max_new_tokens</code>: Maximum number of new tokens to generate (default: 256)</li><li><code>temperature</code>: Temperature for sampling (default: 0.7)</li><li><code>top_p</code>: Top-p for sampling (default: 0.9)</li><li><code>num_turns</code>: Number of turns per episode; set >1 for multi-turn (default: 1)</li><li><code>discount</code>: Discount factor gamma over turns for returns (default: 0.9)</li><li><code>joint_mode</code>: Joint action composition - <code>'aligned'</code> (index-aligned, default) or <code>'cross'</code> (Cartesian product)</li><li><code>termination_threshold</code>: Early stop a branch if mean reward exceeds this threshold (default: None)</li><li><code>eval_interval</code>: Run evaluation every N training batches (default: 4)</li><li><code>eval_num_samples</code>: Number of samples to evaluate per evaluation run (default: 4)</li></ul></blockquote><blockquote class="book-hint info"><p><strong>MAGRPOTrainer</strong> accepts either a model string/object for homogeneous agents or a list of <code>agents</code> for heterogeneous setups:</p><ul><li><code>model</code> or <code>agents</code>: Model string/object for homogeneous agents, or list of agent models</li><li><code>num_agents</code>: Number of agents (default: 2)</li><li><code>tokenizer</code>: The tokenizer (required)</li><li><code>train_dataset</code>: Training dataset (required)</li><li><code>reward_func</code>: Callable that returns a list of floats (required)</li><li><code>reward_processor</code>: Optional processor to apply to rewards (e.g., scaling)</li><li><code>formatters</code>: Single callable or list of callables for each agent to format dataset items into prompts</li><li><code>external_transition</code>: Function providing transitions between turns (required for multi-turn training)</li><li><code>eval_dataset</code>: Evaluation dataset (optional)</li><li><code>eval_logger</code>: Evaluation logger function (optional)</li><li><code>eval_aggregator</code>: Evaluation aggregator function (optional)</li><li><code>wandb_config</code>: Configuration for Weights & Biases logging (optional)</li><li><code>model_config</code>: Model configuration dict (optional)</li><li><code>args</code>: Instance of <code>MAGRPOConfig</code> (optional)</li></ul></blockquote><blockquote class="book-hint warning"><p>For simplicity, MAGRPO computes the policy gradient using the current policy&rsquo;s samples without importance sampling or ratio clipping.</p></blockquote><blockquote class="book-hint warning"><p>The trainer enforces <code>per_device_train_batch_size=1</code> and requires at least 2 generations for group baseline computation.</p></blockquote><h2 id=other-variants>Other Variants<a class=anchor href=#other-variants>#</a></h2><p>CoMLRL also implements other Multi-Agent REINFORCE variants with different baselines:</p><ul><li><strong>MARLOO</strong>: Multi‑Agent REINFORCE Leave‑One‑Out. Baseline is the mean return of other agents (leave‑one‑out) at the same step.</li></ul><div class=book-katex>\[
J(\theta_i) = \mathbb{E}_{\mathbf{o}_0 \sim \mathcal{D}, \mathbf{h}^\mathcal{G} \sim \mathbf{\pi}_{\mathbf{\theta}}}
\Bigg[\frac{1}{|\mathcal{G}|}\sum_{g \in \mathcal{G}} \Big( R^{(g)}_t - \sum_{k\in \mathcal{G},\, k\neq g}\tfrac{R^{(k)}_t}{|\mathcal{G}|-1} \Big) \cdot \log \pi_{\theta_i}(a^{(g)}_{i,t}\mid h_{i,t}) \Bigg];
\]</div><ul><li><strong>MAREMAX</strong>: Multi‑Agent REINFORCE with Group Max. Baseline is the maximum group return at the step.</li></ul><div class=book-katex>\[
J(\theta_i) = \mathbb{E}_{\mathbf{o}_0 \sim \mathcal{D}, \mathbf{h}^\mathcal{G} \sim \mathbf{\pi}_{\mathbf{\theta}}}
\Bigg[\frac{1}{|\mathcal{G}|}\sum_{g \in \mathcal{G}} \Big( R^{(g)}_t - \max(R_t^{\mathcal{G}}) \Big) \cdot \log \pi_{\theta_i}(a^{(g)}_{i,t}\mid h_{i,t}) \Bigg].
\]</div><blockquote class="book-hint success"><p>These classes are derived from <code>comlrl.trainers.magrpo.MAGRPOTrainer</code>. Interfaces for the trainer and configuration classes are the same as <code>MAGRPOTrainer</code> and <code>MAGRPOConfig</code>.</p></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=../../../docs/user-guide/installation/ class="flex align-center"><img src=../../../icons/backward.svg class=book-icon alt=Backward>
<span>Installation</span>
</a></span><span><a href=../../../docs/user-guide/ac-finetuning/ class="flex align-center"><span>Multi-Agent Actor-Critic</span>
<img src=../../../icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div></main></body></html>