<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="CoMLRL supports fine-tuning multi-LLM systems with larger models and more agents when multiple GPUs are available. Users can configure the parallelization training with iac.parallel_training. Currently, parallel_training supports two modes: none or null is the default mode for single-device training; mpis the model parallel scheduling across explicit agent/critic devices.
We will support more parallelization modes (e.g., data parallelization, multi-node training) in the future.
Model Parallelization# When parallel_training=mp, CoMLRL requires explicit agent_devices / critic_devices configuration and deploys the agents and critics accordingly. The training and inference for each model (agent/critic) are running separately on its assigned device. The responses are aggregated on the CPU and pass to the reward function. The reward is then broadcast back to all devices for training. MP supports training larger and more models than a single GPU can hold, but the training throughput is limited by the slowest model.
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="/docs/user-guide/training-parallelization/"><meta property="og:site_name" content="CoMLRL"><meta property="og:title" content="Training Parallelization"><meta property="og:description" content="CoMLRL supports fine-tuning multi-LLM systems with larger models and more agents when multiple GPUs are available. Users can configure the parallelization training with iac.parallel_training. Currently, parallel_training supports two modes: none or null is the default mode for single-device training; mpis the model parallel scheduling across explicit agent/critic devices.
We will support more parallelization modes (e.g., data parallelization, multi-node training) in the future.
Model Parallelization# When parallel_training=mp, CoMLRL requires explicit agent_devices / critic_devices configuration and deploys the agents and critics accordingly. The training and inference for each model (agent/critic) are running separately on its assigned device. The responses are aggregated on the CPU and pass to the reward function. The reward is then broadcast back to all devices for training. MP supports training larger and more models than a single GPU can hold, but the training throughput is limited by the slowest model."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2026-02-17T10:23:54-05:00"><meta itemprop=name content="Training Parallelization"><meta itemprop=description content="CoMLRL supports fine-tuning multi-LLM systems with larger models and more agents when multiple GPUs are available. Users can configure the parallelization training with iac.parallel_training. Currently, parallel_training supports two modes: none or null is the default mode for single-device training; mpis the model parallel scheduling across explicit agent/critic devices.
We will support more parallelization modes (e.g., data parallelization, multi-node training) in the future.
Model Parallelization# When parallel_training=mp, CoMLRL requires explicit agent_devices / critic_devices configuration and deploys the agents and critics accordingly. The training and inference for each model (agent/critic) are running separately on its assigned device. The responses are aggregated on the CPU and pass to the reward function. The reward is then broadcast back to all devices for training. MP supports training larger and more models than a single GPU can hold, but the training throughput is limited by the slowest model."><meta itemprop=dateModified content="2026-02-17T10:23:54-05:00"><meta itemprop=wordCount content="177"><title>Training Parallelization | CoMLRL</title><link rel=icon href=../../../favicon.png><link rel=manifest href=../../../manifest.json><link rel=canonical href=../../../docs/user-guide/training-parallelization/><link rel=stylesheet href=../../../book.min.cf77f74a94d5d5797fce47774a1371342c8a7d81e76bbc31dd6c3ba4e268e9a2.css><script defer src=../../../fuse.min.js></script><script defer src=../../../en.search.min.1f5f10685cd39b978475c0da4b9af85a3c0a3dc2abc7423a72aab5b249bad45f.js></script><link rel=stylesheet href=../../../katex/katex.min.css><script defer src=../../../katex/katex.min.js></script><script defer src=../../../katex/auto-render.min.js onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"\\begin{gather}",right:"\\end{gather}",display:!0}],throwOnError:!1})'></script><link rel=stylesheet href=../../../css/sidebar.css><link rel=icon type=image/svg+xml href=../../../img/comlrl-icon.svg><link rel=icon type=image/png href=../../../img/comlrl-icon.png><link rel="shortcut icon" type=image/png href=../../../img/comlrl-icon.png><link rel=apple-touch-icon href=../../../img/comlrl-icon.png><script>document.addEventListener("DOMContentLoaded",function(){try{var t,n,s,o,e=document.querySelector("article.book-article");if(!e)return;if(n=document.querySelector(".book-header h3"),t=n?n.textContent.trim():(document.title||"").split("|")[0].trim(),!t)return;s=e.querySelector("h1"),(!s||s.textContent.trim()!==t)&&(o=document.createElement("h1"),o.textContent=t,e.insertBefore(o,e.firstChild)),function(){var t,e=document.getElementById("omlrl-footer");e||(e=document.createElement("div"),e.id="omlrl-footer",e.className="omlrl-footer",document.body.appendChild(e)),e.textContent="",t=document.createElement("strong"),t.textContent="Â© OpenMLRL. All rights reserved.",e.appendChild(t)}()}catch{}}),document.addEventListener("keydown",function(e){if(!e.metaKey&&!e.ctrlKey)return;var n,t=(e.key||"").toLowerCase();if(t==="k"){e.preventDefault(),n=document.querySelector(".book-search input"),n&&n.focus();return}if(t==="o"){e.preventDefault(),window.location.href="https://openmlrl.github.io";return}if(t==="g"){e.preventDefault(),window.location.href="https://github.com/OpenMLRL/CoMLRL";return}}),document.addEventListener("DOMContentLoaded",function(){var e,t=document.querySelectorAll("pre");t.forEach(function(e){var t=document.createElement("button");t.className="copy-code-button",t.textContent="Copy",t.addEventListener("click",function(){var n=e.querySelector("code"),s=n.textContent;navigator.clipboard.writeText(s).then(function(){t.textContent="Copied!",setTimeout(function(){t.textContent="Copy"},2e3)})}),e.style.position="relative",e.appendChild(t)}),e=document.createElement("a"),e.href="https://github.com/OpenMLRL/CoMLRL",e.className="github-corner",e.target="_blank",e.rel="noopener noreferrer",e.setAttribute("aria-label","View source on GitHub"),e.innerHTML='<svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><defs><linearGradient id="github-gradient" x1="0%" y1="0%" x2="100%" y2="100%"><stop offset="0%" style="stop-color:#9555af;stop-opacity:1" /><stop offset="100%" style="stop-color:#e091c4;stop-opacity:1" /></linearGradient></defs><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z" fill="url(#github-gradient)"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="#fff" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="#fff" class="octo-body"></path></svg>',document.body.appendChild(e)})</script></head><body dir=ltr class="book-kind-page book-type-docs"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=../../../><img src=../../../img/comlrl-logo.png alt=Logo><span>CoMLRL</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a>User Guide</a><ul><li><a href=../../../docs/user-guide/installation/>Installation</a></li><li><a href=../../../docs/user-guide/model-loading/>Model Loading</a></li><li><a href=../../../docs/user-guide/multi-agent-reinforce/>Multi-Agent REINFORCE</a></li><li><a href=../../../docs/user-guide/multi-agent-actor-critic/>Multi-Agent Actor-Critic</a></li><li><a href=../../../docs/user-guide/multi-turn-training/>Multi-Turn Interaction</a></li><li><a href=../../../docs/user-guide/training-parallelization/ class=active>Training Parallelization</a></li></ul></li><li class=book-section-flat><a>Examples</a><ul><li><a href=../../../docs/examples/comlrl-quick-start/>CoMLRL Quick Start</a></li></ul></li><li class=book-section-flat><a>Developers</a><ul><li><a href=../../../docs/dev/support/>Support</a></li><li><a href=../../../docs/dev/contributing/>Contributing</a></li><li><a href=../../../docs/dev/changelog/>Changelog</a></li></ul></li><li class=book-section-flat><a href=../../../docs/env/>Environments</a><ul><li><a href=https://github.com/OpenMLRL/LLM_Collab_Writing target=_blank rel=noopener>Writing</a></li><li><a href=https://github.com/OpenMLRL/LLM_Collab_Code_Generation target=_blank rel=noopener>Coding</a></li><li><a href=https://github.com/OpenMLRL/LLM_Collab_Minecraft target=_blank rel=noopener>Minecraft</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=../../../icons/menu.svg class=book-icon alt=Menu></label><h3>Training Parallelization</h3><label for=toc-control></label></div></header><article class="markdown book-article"><p>CoMLRL supports fine-tuning multi-LLM systems with larger models and more agents when multiple GPUs are available.
Users can configure the parallelization training with <code>iac.parallel_training</code>.
Currently, <code>parallel_training</code> supports two modes: <code>none</code> or <code>null</code> is the default mode for single-device training; <code>mp</code>is the model parallel scheduling across explicit agent/critic devices.</p><blockquote class="book-hint success"><p>We will support more parallelization modes (e.g., <a href=https://docs.pytorch.org/docs/stable/elastic/run.html>data parallelization</a>, <a href=ray.io>multi-node training</a>) in the future.</p></blockquote><h2 id=model-parallelization>Model Parallelization<a class=anchor href=#model-parallelization>#</a></h2><p>When <code>parallel_training=mp</code>, CoMLRL requires explicit <code>agent_devices</code> / <code>critic_devices</code> configuration and deploys the agents and critics accordingly.
The training and inference for each model (agent/critic) are running separately on its assigned device.
The responses are aggregated on the CPU and pass to the reward function. The reward is then broadcast back to all devices for training.
MP supports training larger and more models than a single GPU can hold, but the training throughput is limited by the slowest model.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#bb60d5>CUDA_VISIBLE_DEVICES</span><span style=color:#666>=</span>0,1,2,3 python train_iac.py
</span></span><span style=display:flex><span>  --config configs/iac_xxx.yaml
</span></span><span style=display:flex><span>  --override
</span></span><span style=display:flex><span>    <span style=color:#bb60d5>agent_model</span><span style=color:#666>=</span><span style=color:#4070a0>&#34;model_a&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#bb60d5>agents</span><span style=color:#666>=</span>None
</span></span><span style=display:flex><span>    <span style=color:#bb60d5>critic_model</span><span style=color:#666>=</span><span style=color:#4070a0>&#34;model_b&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#bb60d5>critics</span><span style=color:#666>=</span>None
</span></span><span style=display:flex><span>    iac.use_separate_critic<span style=color:#666>=</span><span style=color:#007020>true</span>
</span></span><span style=display:flex><span>    iac.parallel_training<span style=color:#666>=</span>mp
</span></span><span style=display:flex><span>    iac.agent_devices<span style=color:#666>=</span><span style=color:#4070a0>&#39;[&#34;cuda:0&#34;,&#34;cuda:1&#34;]&#39;</span>
</span></span><span style=display:flex><span>    iac.critic_devices<span style=color:#666>=</span><span style=color:#4070a0>&#39;[&#34;cuda:2&#34;,&#34;cuda:3&#34;]&#39;</span></span></span></code></pre></div><blockquote class="book-hint note"><p>Note that when devices are changed, the training is not deterministic due to the non-deterministic GPU scheduling and aggregation on CPU.</p></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class="flex flex-wrap justify-between"><span><a href=../../../docs/user-guide/multi-turn-training/ class="flex align-center"><img src=../../../icons/backward.svg class=book-icon alt=Backward>
<span>Multi-Turn Interaction</span>
</a></span><span><a href=../../../docs/examples/comlrl-quick-start/ class="flex align-center"><span>CoMLRL Quick Start</span>
<img src=../../../icons/forward.svg class=book-icon alt=Forward></a></span></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div></main></body></html>