<!doctype html><html lang=en-us dir=ltr><head><meta name=generator content="Hugo 0.152.2"><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Welcome to CoMLRL's documentation Â ðŸ‘‹
Cooperative Multi-LLM Reinforcement Learning (CoMLRL) is an open-source library for training multiple LLMs to collaborate using Multi-Agent Reinforcement Learning (MARL). It provides implementations of various MARL algorithms for LLM collaboration and supports different environments and benchmarks.
About# Q&amp;A â€œWhat are the differences between CoMLRL and other multi-LLM training frameworks?&#34;
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="/"><meta property="og:site_name" content="CoMLRL"><meta property="og:title" content="CoMLRL"><meta property="og:description" content="Welcome to CoMLRL's documentation Â ðŸ‘‹
Cooperative Multi-LLM Reinforcement Learning (CoMLRL) is an open-source library for training multiple LLMs to collaborate using Multi-Agent Reinforcement Learning (MARL). It provides implementations of various MARL algorithms for LLM collaboration and supports different environments and benchmarks.
About# Q&amp;A â€œWhat are the differences between CoMLRL and other multi-LLM training frameworks?&#34;"><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><meta itemprop=name content="CoMLRL"><meta itemprop=description content="Welcome to CoMLRL's documentation Â ðŸ‘‹
Cooperative Multi-LLM Reinforcement Learning (CoMLRL) is an open-source library for training multiple LLMs to collaborate using Multi-Agent Reinforcement Learning (MARL). It provides implementations of various MARL algorithms for LLM collaboration and supports different environments and benchmarks.
About# Q&amp;A â€œWhat are the differences between CoMLRL and other multi-LLM training frameworks?&#34;"><meta itemprop=dateModified content="2025-11-20T10:22:18-05:00"><meta itemprop=wordCount content="712"><title>CoMLRL</title><link rel=icon href=./favicon.png><link rel=manifest href=./manifest.json><link rel=canonical href=./><link rel=stylesheet href=./book.min.84995bf041dbb6656f4541996b392ba4d647753aa90c42a8483168566c27189d.css><script defer src=./fuse.min.js></script><script defer src=./en.search.min.13bee499e9e027813480ea37ab20b2546ad23bae05ac60acf56bbf6c02da1fa6.js></script><link rel=alternate type=application/rss+xml href=./index.xml title=CoMLRL><link rel=stylesheet href=./katex/katex.min.css><script defer src=./katex/katex.min.js></script><script defer src=./katex/auto-render.min.js onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"\\begin{gather}",right:"\\end{gather}",display:!0}],throwOnError:!1})'></script><link rel=stylesheet href=./css/sidebar.css><link rel=icon type=image/svg+xml href=./img/comlrl-icon.svg><link rel=icon type=image/png href=./img/comlrl-icon.png><link rel="shortcut icon" type=image/png href=./img/comlrl-icon.png><link rel=apple-touch-icon href=./img/comlrl-icon.png><script>document.addEventListener("DOMContentLoaded",function(){try{var t,n,s,o,e=document.querySelector("article.book-article");if(!e)return;if(n=document.querySelector(".book-header h3"),t=n?n.textContent.trim():(document.title||"").split("|")[0].trim(),!t)return;s=e.querySelector("h1"),(!s||s.textContent.trim()!==t)&&(o=document.createElement("h1"),o.textContent=t,e.insertBefore(o,e.firstChild)),function(){var t,e=document.getElementById("omlrl-footer");e||(e=document.createElement("div"),e.id="omlrl-footer",e.className="omlrl-footer",document.body.appendChild(e)),e.textContent="",t=document.createElement("strong"),t.textContent="Â© OpenMLRL. All rights reserved.",e.appendChild(t)}()}catch{}}),document.addEventListener("keydown",function(e){if(!e.metaKey&&!e.ctrlKey)return;var n,t=(e.key||"").toLowerCase();if(t==="k"){e.preventDefault(),n=document.querySelector(".book-search input"),n&&n.focus();return}if(t==="o"){e.preventDefault(),window.location.href="https://openmlrl.github.io";return}if(t==="g"){e.preventDefault(),window.location.href="https://github.com/OpenMLRL/CoMLRL";return}}),document.addEventListener("DOMContentLoaded",function(){var e,t=document.querySelectorAll("pre");t.forEach(function(e){var t=document.createElement("button");t.className="copy-code-button",t.textContent="Copy",t.addEventListener("click",function(){var n=e.querySelector("code"),s=n.textContent;navigator.clipboard.writeText(s).then(function(){t.textContent="Copied!",setTimeout(function(){t.textContent="Copy"},2e3)})}),e.style.position="relative",e.appendChild(t)}),e=document.createElement("a"),e.href="https://github.com/OpenMLRL/CoMLRL",e.className="github-corner",e.target="_blank",e.rel="noopener noreferrer",e.setAttribute("aria-label","View source on GitHub"),e.innerHTML='<svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><defs><linearGradient id="github-gradient" x1="0%" y1="0%" x2="100%" y2="100%"><stop offset="0%" style="stop-color:#9555af;stop-opacity:1" /><stop offset="100%" style="stop-color:#e091c4;stop-opacity:1" /></linearGradient></defs><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z" fill="url(#github-gradient)"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="#fff" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="#fff" class="octo-body"></path></svg>',document.body.appendChild(e)})</script></head><body dir=ltr class="book-kind-home book-type-page"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=./><img src=./img/comlrl-logo.png alt=Logo><span>CoMLRL</span></a></h2><div class="book-search hidden"><input id=book-search-input type=text placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a>User Guide</a><ul><li><a href=./docs/user-guide/installation/>Installation</a></li><li><a href=./docs/user-guide/reinforce-finetuning/>Multi-Agent REINFORCE</a></li><li><a href=./docs/user-guide/ac-finetuning/>Multi-Agent Actor-Critic</a></li><li><a href=./docs/user-guide/multi-turn/>Multi-Turn Training</a></li></ul></li><li class=book-section-flat><a>Examples</a><ul><li><a href=./docs/examples/quick-demo/>CoMLRL Quick Demo</a></li></ul></li><li class=book-section-flat><a>Developers</a><ul><li><a href=./docs/dev/support/>Support</a></li><li><a href=./docs/dev/contributing/>Contributing</a></li><li><a href=./docs/dev/changelog/>Changelog</a></li></ul></li><li class=book-section-flat><a href=./docs/env/>Environments</a><ul><li><a href=https://github.com/OpenMLRL/LLM_Collab_Writing target=_blank rel=noopener>Article Writing</a></li><li><a href=https://github.com/OpenMLRL/LLM_Collab_Code_Generation target=_blank rel=noopener>Code Generation</a></li><li><a href=https://github.com/OpenMLRL/LLM_Collab_Code_Completion target=_blank rel=noopener>Code Completion</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class="book-header hidden"><div class="flex align-center justify-between"><label for=menu-control><img src=./icons/menu.svg class=book-icon alt=Menu></label><h3></h3><label for=toc-control></label></div></header><article class="markdown book-article"><p style="font-family:futura,futura pt,avenir next,segoe ui,Arial,sans-serif;font-weight:700;font-size:1.7rem;letter-spacing:0.em;line-height:1;margin-top:1.8em 0">Welcome to CoMLRL's documentation &nbsp;ðŸ‘‹</p><p><strong>Co</strong>operative <strong>M</strong>ulti-<strong>L</strong>LM <strong>R</strong>einforcement <strong>L</strong>earning (<strong>CoMLRL</strong>) is an open-source library for training multiple LLMs to collaborate using Multi-Agent Reinforcement Learning (MARL). It provides implementations of various MARL algorithms for LLM collaboration and supports different environments and benchmarks.</p><h2 id=about>About<a class=anchor href=#about>#</a></h2><div class=book-tabs><input type=radio class=toggle name=tabs-0 id=tabs-0-0 checked><label for=tabs-0-0>Q&amp;A</label><div class="book-tabs-content markdown-inner"><p><em style=font-weight:600;color:#9555af;margin-bottom:-.5rem;display:block>&ldquo;What are the differences between CoMLRL and other multi-LLM training frameworks?"</em></p><p>A lot of works use role-conditioned, parameter-sharing approaches built on single-agent RL training framework to implement multi-agent training. Using truly distinct agents provides a better modeling where highly heterogeneous LLMs possess fundamentally different capabilities and aligns better with the concept of study â€” &ldquo;multi-agent&rdquo;.</p><p>Compare with other multi-LLM training frameworks, agents can either be trained centralizedly or decentralizedly with CoMLRL, while their execution is always fully decentralized to ensure efficient inference. CoMLRL implements standard MARL algorithms from scratch to maximize flexibility and customizability while maintaining simplicity for usage.</p><p><em style=font-weight:600;color:#9555af;margin-bottom:-.5rem;display:block>&ldquo;Does CoMLRL support single-agent fine-tuning?"</em></p><p><strong>Yes!</strong> The simplest way is to set <code>num_agents=1</code> in your trainer. But since we omit fancy optimizations for simplicity of multi-agent training, you may not find the single-agent trainers optimal. <a href=https://github.com/hiyouga/LLaMA-Factory>LLaMA-Factory</a>, <a href=https://github.com/huggingface/trl>trl</a>, <a href=https://github.com/OpenRLHF/OpenRLHF>OpenRLHF</a>, and <a href=https://github.com/volcengine/verl>verl</a> are good choices for single-agent fine-tuning.</p><p><em style=font-weight:600;color:#9555af;margin-bottom:-.5rem;display:block>&ldquo;Does CoMLRL support self-play/self-improvement/self-evolving by MARL?"</em></p><p><strong>Yes!</strong> Although we focus on LLM collaboration formalized as <a href=https://www.fransoliehoek.net/docs/OliehoekAmato16book.pdf>Dec-POMDP</a>, users can still customize the interactions with environment to implement pipeline like self-play (<a href=https://github.com/spiral-rl/spiral>Spiral</a>) and self-improvement (<a href=https://github.com/vsubramaniam851/multiagent-ft/tree/main>MAFT</a>). Users can refer to our <a href=docs/user-guide/multi-turn>multi-turn training</a> for more details.</p><p><em style=font-weight:600;color:#9555af;margin-bottom:-.5rem;display:block>&ldquo;Does CoMLRL support distributed training?"</em></p><p><strong>Not yet.</strong> We are currently focusing on <a href=https://arxiv.org/abs/2409.03052>CTDE</a> on light-weighted training small-scale LLMs with cooperative MARL for proof of the concept. Resource-consuming distributed training with slow and complex gradient accumulation is under development and will be open-sourced in the near future.</p></div><input type=radio class=toggle name=tabs-0 id=tabs-0-1><label for=tabs-0-1>LLM Collaboration</label><div class="book-tabs-content markdown-inner"><p><em style=font-weight:600;color:#9555af>&ldquo;What is LLM collaboration?"</em></p><p>LLM collaboration refers to the problems where LLM agents cooperatively solve tasks in multi-agent systems. The tasks are specified in language and provided to each agent as a prompt, and the agent generates a response synchronously based on its instructions. The set of all agents&rsquo; responses jointly forms a solution. Users and systems may validate the solutions to provide additional requirements or suggestions for LLMs. These components form part
of the environment for LLM collaboration, with states that may be updated based on the agentsâ€™ outputs. The updates are embedded into prompts for subsequent turns. This process iterates until the task is completed or a turn limit is reached.</p></div><input type=radio class=toggle name=tabs-0 id=tabs-0-2><label for=tabs-0-2>MARL Fine-Tuning</label><div class="book-tabs-content markdown-inner"><p><em style=font-weight:600;color:#9555af>&ldquo;Why should we fine-tune multi-LLM systems with MARL?"</em></p><p>Many studies have explored LLM-based multi-agent systems for completing tasks with multiple interacting agents. However, most of these models are pretrained separately and are not specifically optimized for coordination, which would limit their performance. In addition, designing effective prompts remains difficult and unclear. Cooperative MARL methods have been extensively studied for years, which optimize a team of agents towards a shared objective. They naturally fit LLM collaboration and motivate us to bring advances from the well-established MARL community to LLM-based MAS.</p></div><input type=radio class=toggle name=tabs-0 id=tabs-0-3><label for=tabs-0-3>Decentralization</label><div class="book-tabs-content markdown-inner"><p><em style=font-weight:600;color:#9555af>&ldquo;What are the benefits of decentralized reasoning?"</em></p><p>Cooperative MARL methods are grounded in the theory of <a href=https://www.fransoliehoek.net/docs/OliehoekAmato16book.pdf>Dec-POMDP</a>. The agents execute in a decentralized manner, which has many advantages. Unlike knowledge distillation, pruning, or quantization, it accelerates LLM inference without incurring information loss. Moreover, decentralization reduces the computational and memory burden of maintaining long-context dependencies and conducting joint decision-making within a single model. By assigning specific subtasks to individual agents, the system achieves more modular, efficient, and lightweight reasoning. In addition, effective cooperation among small local language models can offer a safe and cost-efficient solution for offline and edge intelligence.</p></div></div><h2 id=features>Features<a class=anchor href=#features>#</a></h2><ul><li><strong>MARL trainers to optimize LLM collaboration:</strong><ul><li><strong><em>Multi-Agent REINFORCE</em>:</strong> Critic-free policy gradient methods, including <a href=https://github.com/OpenMLRL/CoMLRL/blob/main/comlrl/trainers/mareinforce.py>MAREINFORCE</a>, <a href=https://github.com/OpenMLRL/CoMLRL/blob/main/comlrl/trainers/magrpo.py>MAGRPO</a>, <a href=https://github.com/OpenMLRL/CoMLRL/blob/main/comlrl/trainers/marloo.py>MARLOO</a>, <a href=https://github.com/OpenMLRL/CoMLRL/blob/main/comlrl/trainers/maremax.py>MAREMAX</a>.<ul><li>Aligned individual response joint with <code>joint_mode='align'</code>.</li><li>Memory-efficient cross joint with <code>joint_mode='cross'</code>.</li></ul></li><li><strong><em>Multi-Agent Actor-Critic:</em></strong> Critic-based policy gradient methods, including <a href=https://github.com/OpenMLRL/CoMLRL/blob/main/comlrl/trainers/iac.py>IAC</a>.<ul><li>Canonical IAC with a separate critic with <code>use_separate_critic=True</code>.</li><li>Memory-efficient critic with value-head over actor with <code>use_separate_critic=False</code>.</li></ul></li></ul></li><li><strong>Environments that simulate real-world tasks for training and evaluating LLM collaboration:</strong><ul><li><a href=https://github.com/OpenMLRL/LLM_Collab_Writing><strong><em>Writing Collaboration</em></strong></a>: Multiple LLM agents collaborate on processing articles.<ul><li><a href=https://huggingface.co/datasets/trl-lib/tldr>TLDR</a> - Summarizing Reddit posts.</li><li><a href=http://arxiv.org/abs/1905.00075>ArXiv</a> - Expanding abstracts into introductions.</li></ul></li><li><a href=https://github.com/OpenMLRL/LLM_Collab_Code_Generation><strong><em>Code Generation</em></strong></a>: Generate code solutions for programming problems.<ul><li><a href=https://arxiv.org/abs/2108.07732>MBPP</a> - Mostly basic python problems.</li><li><a href=https://arxiv.org/abs/2107.03374>HumanEval</a> - Handwritten evaluation problems</li><li><a href=https://huggingface.co/datasets/OpenMLRL/CoopHumanEval>CoopHumanEval</a> - HumanEval with cooperative nature.</li></ul></li><li><a href=https://github.com/OpenMLRL/LLM_Collab_Code_Completion><strong><em>Code Completion</em></strong></a>: Complete code snippets based on given contexts.<ul><li><a href=https://conf.researchr.org/details/icse-2024/icse-2024-research-track/219/Evaluating-Large-Language-Models-in-Class-Level-Code-Generation>ClassEval</a> - Complete class-level code based on method stubs and docstrings.</li></ul></li></ul></li></ul><p align=center><img src=./img/demo.gif width=800px; alt></p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class=book-comments></div><script>(function(){document.querySelectorAll("pre:has(code)").forEach(e=>{e.addEventListener("click",e.focus),e.addEventListener("copy",function(t){if(t.preventDefault(),navigator.clipboard){const t=window.getSelection().toString()||e.textContent;navigator.clipboard.writeText(t)}})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div></main></body></html>