[{"id":0,"href":"/docs/examples/comlrl-quick-start/","title":"CoMLRL Quick Start","section":"Examples","content":"This tutorial demonstrates how to train two LLM agents to collaborate to tell a story. The first agent generates a compact story setup, while the second agent produces a longer version. The reward function encourages the second agent\u0026rsquo;s output to be 2–3× longer than the first agent\u0026rsquo;s.\nTo run this demo, please have at least 24 GB of GPU memory available. You can also visualize the training process by setting up your WandB dashboard.\nImport Libraries# import math from functools import partial from datasets import Dataset from transformers import AutoModelForCausalLM, AutoTokenizer from comlrl.utils.reward_processor import RewardProcessors from comlrl.trainers.reinforce import MAGRPOConfig, MAGRPOTrainerDataset Preparation# We first create a dataset of creative prompts for the agents to work on.\ntrain_data = { \u0026#34;prompt\u0026#34;: [ \u0026#34;Describe a city in the clouds:\u0026#34;, \u0026#34;Invent a new holiday and explain it:\u0026#34;, \u0026#34;Write a bedtime story for a dragon:\u0026#34;, \u0026#34;Explain how teleportation might work:\u0026#34;, \u0026#34;Tell a joke about dinosaurs:\u0026#34;, \u0026#34;Describe a world without electricity:\u0026#34;, \u0026#34;Create a superhero with a unique power:\u0026#34;, \u0026#34;Write a scene where the moon talks:\u0026#34;, \u0026#34;Invent a new type of fruit:\u0026#34;, \u0026#34;Design a playground on Mars:\u0026#34;, ] } train_dataset = Dataset.from_dict(train_data)Agent Initialization# We load a tokenizer to convert text into tokens that the model can process and initialize two separate instances.\nmodel_name = \u0026#34;Qwen/Qwen2.5-0.5B\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_name) agents = [AutoModelForCausalLM.from_pretrained(model_name) for _ in range(2)]Define the Reward Function# The reward function measures how well the agents collaborate. It gives maximum reward (1.0) when the second agent\u0026rsquo;s output is 2–3× longer than the first agent\u0026rsquo;s. If the length ratio falls outside this range, the reward decays exponentially based on how far it deviates.\ndef proper_length_ratio_reward( completions1, completions2, target_min=2.0, target_max=3.0 ): rewards = [] for c1, c2 in zip(completions1, completions2): len1, len2 = len(c1), len(c2) if len1 == 0: rewards.append(0.0) continue ratio = len2 / len1 if target_min \u0026lt;= ratio \u0026lt;= target_max: reward = 1.0 else: if ratio \u0026lt; target_min: distance = target_min - ratio else: distance = ratio - target_max reward = math.exp(-distance) rewards.append(float(reward)) return rewardsConfigure Training# We set up the training configuration with hyperparameters like learning rate, batch size, and the number of generations each agent produces per prompt.\noutput_dir = \u0026#34;./magrpo_multi_reward_output\u0026#34; config = MAGRPOConfig( num_train_epochs=3, agent_learning_rate=5e-5, logging_steps=10, num_generations=8, num_turns=1, max_new_tokens=128, )Create the Trainer# We instantiate the MAGRPO trainer with our agents, reward function, and configuration. The reward is scaled by 100× to provide a stronger learning signal.\nwandb_config = { \u0026#34;project\u0026#34;: \u0026#34;your-project-name\u0026#34;, \u0026#34;entity\u0026#34;: \u0026#34;your-entity-name\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;length-ratio-demo\u0026#34;, } configured_reward_func = partial( proper_length_ratio_reward, target_min=2, target_max=3 ) trainer = MAGRPOTrainer( agents=agents, reward_func=configured_reward_func, reward_processor=RewardProcessors.scale(factor=100.0), args=config, train_dataset=train_dataset, tokenizer=tokenizer, wandb_config=wandb_config, )Run Training# Finally, we start the training process. The trainer will optimize both agents to maximize the collaborative reward, then save the trained models.\ntrainer.train() trainer.save_model(f\u0026#34;{output_dir}/models\u0026#34;)"},{"id":1,"href":"/docs/user-guide/installation/","title":"Installation","section":"User Guide","content":"CoMLRL provides several different ways for installation.\nInstall from PyPI# pip install comlrl # install PyTorch compatible with your deviceInstall from conda-forge# conda install -c conda-forge comlrl # install PyTorch compatible with your deviceInstall from source# To access the latest features of CoMLRL or to develop CoMLRL, clone this repository and install in editable mode:\ngit clone https://github.com/OpenMLRL/CoMLRL.git cd CoMLRL pip install -e . # install PyTorch compatible with your device"},{"id":2,"href":"/docs/dev/support/","title":"Support","section":"Developers","content":"We are willing to help you with any issues you encounter while using CoMLRL.\nReport Issues# If you are stuck with a problem using CoMLRL, please follow this procedure:\nRead the documentation first, including using the search feature (Ctrl + K).\nSearch the GitHub Issues archives to see if someone else already had the same problem.\nBefore writing, try to create a minimal example that reproduces the problem. You\u0026rsquo;ll get the fastest response if you can send just a handful of lines of code that show what isn\u0026rsquo;t working.\n"},{"id":3,"href":"/docs/env/writing/","title":"Writing","section":"Environments","content":"Collaborative summarization and expansion tasks for pairs (or teams) of LLMs. The reference implementation lives in the LLM_Collab_Writing repository and includes:\nTLDR – distills Reddit threads into concise summaries. ArXiv Introductions – grows short abstracts into multi-paragraph drafts. "},{"id":4,"href":"/docs/env/coding/","title":"Coding","section":"Environments","content":"A suite of cooperative programming benchmarks where agents propose, critique, and refine solutions. The environments shipped in LLM_Collab_Code_Generation cover:\nMBPP – mostly basic Python problems for rapid iteration. HumanEval – handwritten tasks from OpenAI for exact-match grading. CoopHumanEval – HumanEval variants that explicitly require collaboration. "},{"id":5,"href":"/docs/dev/contributing/","title":"Contributing","section":"Developers","content":"Thanks for your interest in helping build CoMLRL! This guide walks you through reporting issues, contributing changes, and keeping the codebase healthy.\nDevelopment Guidelines# Fork the upstream repository. Clone your fork and synchronize with upstream: git clone https://github.com/\u0026lt;your-username\u0026gt;/CoMLRL.git cd CoMLRL git remote add upstream https://github.com/OpenMLRL/CoMLRL.git git fetch upstream git checkout -b feature/\u0026lt;short-description\u0026gt; upstream/main git fetch upstream \u0026amp;\u0026amp; git rebase upstream/main Implement new features or fix bugs, updating documentation as needed. Open a pull request to the upstream repository and wait for review. "},{"id":6,"href":"/docs/user-guide/model-loading/","title":"Model Loading","section":"User Guide","content":"CoMLRL supports both homogeneous and heterogeneous models. Users can assign agent_model/critic_model with HuggingFace model identifiers for homogeneous setups, or provide agents/critics lists for heterogeneous setups.\nHomogeneous Agents# The easiest way to start the journey of CoMLRL is to load num_agents homogeneous agents with a single model identifier. Users can set agent_model.name to a single model identifier while keeping agents: null.\nFor example, to load 3 Qwen/Qwen2.5-1.5B agents:\ntrainer = MAGRPOTrainer( agent_model=\u0026#34;Qwen/Qwen2.5-1.5B\u0026#34;, agents=None, num_agents=3, )Heterogeneous Agents# Although homogeneous LLM agents can be specified into different roles by prompting, using heterogeneous LLMs with different skills can further unleash the potential of multi-agent collaboration. Users can load a list of heterogeneous agents in agents, where the length of the list should match num_agents. Each entry should specify a model identifier and optional tokenizer/model kwargs. When agents is provided, agent_model should be set to null or ignored; if both are provided, they must match (same names, correct length) or training will raise an error.\nFor example, to load a Qwen/Qwen2.5-Coder-3B and a Qwen/Qwen2.5-Coder-7B:\ntrainer = MAGRPOTrainer( agent_model=None, agents=[\u0026#34;Qwen/Qwen2.5-Coder-3B\u0026#34;, \u0026#34;Qwen/Qwen2.5-Coder-7B\u0026#34;], num_agents=2, )Loading LLM Critics# The loading of critics depends on the algorithm and the use_separate_critic setting. In Multi-Agent Actor-Critic (MAAC), a single separated centralized critic is used, so one model should be provided in critic_model or critics without any constraints on model types.\nFor example, to load a Qwen/Qwen2.5-Coder-3B and a Qwen/Qwen2.5-Coder-1.5B with a centralized Qwen/Qwen2.5-Coder-7B critic:\ntrainer = MAACTrainer( agent_model=None, agents=[\u0026#34;Qwen/Qwen2.5-Coder-3B\u0026#34;, \u0026#34;Qwen/Qwen2.5-Coder-1.5B\u0026#34;], critic_model=\u0026#34;Qwen/Qwen2.5-Coder-7B\u0026#34;, critics=None, num_agents=2, )In Independent Actor-Critic (IAC), each agent can have its own critic. When use_separate_critic=true, users should provide critic_model.name or critics with length num_agents to load separate critics for each agent. When use_separate_critic=false, each agent shares its LLM agent backbone with its critic, and the critic is loaded at the same time as the agent. In this case, critic_model and critics should not be provided and set to null or None. Similarly to agents, if both critic_model and critics are provided, they must match (same names, correct length) or training will raise an error.\nFor example, to load a Qwen/Qwen2.5-Coder-3B and a Qwen/Qwen2.5-Coder-7B with separate critics of the same models:\ntrainer = IACTrainer( agent_model=None, agents=[\u0026#34;Qwen/Qwen2.5-Coder-3B\u0026#34;, \u0026#34;Qwen/Qwen2.5-Coder-7B\u0026#34;], critic_model=None, critics=[\u0026#34;Qwen/Qwen2.5-Coder-3B\u0026#34;, \u0026#34;Qwen/Qwen2.5-Coder-7B\u0026#34;], num_agents=2, )Or actor can share the same model with its critic:\ntrainer = IACTrainer( agent_model=None, agents=[\u0026#34;Qwen/Qwen2.5-Coder-3B\u0026#34;, \u0026#34;Qwen/Qwen2.5-Coder-7B\u0026#34;], critic_model=None, critics=None, num_agents=2, ) Internally, trainers always work with agents/critics lists. agent_model and critic_model are convenience shortcuts for homogeneous settings; if both are provided, they must be consistent.\nTokenizers are loaded per agent by default. If your models use incompatible vocabularies, training may fail (e.g., in shared-critic settings). Prefer models from the same family or ensure tokenizer compatibility.\n"},{"id":7,"href":"/docs/dev/changelog/","title":"Changelog","section":"Developers","content":" Version 1.3.6# Fixed the bug of loading heterogeneous models and reform the loading logics Reconstruct the docs Version 1.3.5# Add unit tests for hyperparameter constraints. Clean legacy interfaces. Version 1.3.4# Fix the bug of loading heterogeneous models and reform the loading logics. Enable MBGD in MAGRPO to align with MAAC and IAC. Remove redundant and legacy hyperparameters (e.g., model kwargs, patching hyperparameters). Clean multi-device legacy, like drop last and num_workers. Add unit tests for model loading and separate it from CI as a badge. Clean short functions. Reorganize the docs and align the parameters. Version 1.3.3# Compact MAREINFORCETrainer derivation, and move to the new folder. Unify the interface for different trainers. Remove redundant patches and wrappers. Reorganize the variables in the config yamls. Version 1.3.2# Fix wandb logging issue in MAGRPOTrainer Version 1.3.1# Allow batch training in MAGRPOTrainer, IACTrainer and MAACTrainer Allow multi-turn training in IACTrainer and MAACTrainer Change the x-axis from data_step to env_step Version 1.3.0# Use TD error as critic update target in IACTrainer and MAACTrainer.\nVersion 1.2.9# Add MAACTrainer (separated centralized critic), now both IACTrainer and MAACTrainer can support single-turn training.\nVersion 1.2.8# The critic in IACTrainer now estimate V rather than Q.\nVersion 1.2.7# Change the IPPOTrainer to be IACTrainer.\nVersion 1.2.6# The first release of CoMLRL:\nIncluding MAGRPO, MAREINFORCE, MARLOO, MAREMAX, and IPPO trainers for multi-agent reinforcement learning with LLMs. Support for multi-turn training with custom external feedback mechanisms. LLM collaboration environments for various tasks. Comprehensive documentation and examples for getting started. "},{"id":8,"href":"/docs/env/code-completion/","title":"Code Completion","section":"Environments","content":"Multi-agent autocompletion tasks where each model fills in part of a codebase. The LLM_Collab_Code_Completion project currently focuses on ClassEval, which asks teams of LLMs to finish class skeletons based on docstrings and partially implemented methods.\n"},{"id":9,"href":"/docs/user-guide/multi-agent-reinforce/","title":"Multi-Agent REINFORCE","section":"User Guide","content":"REINFORCE is a class of policy gradient methods that optimize the policy directly using sampled returns. It has been widely used to fine-tune LLMs because of its simplicity and efficiency, e.g., GRPO, Dr. GRPO, RLOO, ReMax, TreeRPO, and REINFORCE++. REINFORCE can be extended to multi-agent settings, where multiple LLM agents response synchronously and their joint responses form a solution at each turn to receive a shared reward at each turn.\nMA-REINFORCE# The naive Multi‑Agent REINFORCE (MA-REINFORCE) can be expressed as:\n\\[ J(\\theta_i) = \\mathbb{E}_{\\mathbf{o}_0 \\sim \\mathcal{D}, \\mathbf{h}_t \\sim \\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}} \\Bigg[\\sum_{t=0}^{H-1} R_t \\cdot \\log \\pi_{\\theta_i}(a_{i,t}\\mid h_{i,t})\\Bigg], \\] where \\( R_t \\) is the return at turn \\( t \\) and \\( H \\) is the horizon (i.e., number of dialog turns). The expectation is taken over initial observations from the dataset \\( \\mathcal{D} \\) and the joint action history of all episodes following policy \\( \\boldsymbol{\\pi}_{\\boldsymbol{\\theta}} \\).\nREINFORCE methods do not use a critic model for value estimation. Their policy gradients estimation can have high variance, due to the stochasticity of the environment and the long-term credit assignment. There are two common approaches to reduce the variance: using an action-independent baseline or update with more samples, e.g., using \\( K \\) samples for value estimation of each joint history \\( \\mathbf{h}_t \\).\n\\[ J(\\theta_i) = \\mathbb{E}_{\\mathbf{o}_0 \\sim \\mathcal{D}, \\mathbf{h}_t \\sim \\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}} \\Bigg[\\frac{1}{K}\\sum_{k=1}^{K} \\sum_{t=0}^{H-1} \\left(R^{k}_t - b(\\mathbf{h}_t)\\right) \\cdot \\log \\pi_{\\theta_i}(a^{k}_{i,t}\\mid h_{i,t})\\Bigg], \\] where the baseline \\( b(\\mathbf{h}_t) \\) is action-independent.\nMAGRPO# Multi‑Agent Group‑Relative Policy Optimization (MAGRPO) is an instantiation of MA-REINFORCE inspired from GRPO, where the group-average baseline is the mean return of \\( K \\) samples:\n\\[ J(\\theta_i) = \\mathbb{E}_{\\mathbf{o}_0 \\sim \\mathcal{D}, \\mathbf{h}_t \\sim \\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}} \\Bigg[\\frac{1}{K}\\sum_{k=1}^{K}\\sum_{t=0}^{H-1} \\left(R^{k}_t - \\frac{1}{K}\\sum_{l=1}^{K}R^{l}_t\\right) \\cdot \\log \\pi_{\\theta_i}(a^{k}_{i,t}\\mid h_{i,t})\\Bigg]. \\] MAGRPOConfig parameters:\nnum_agents: Number of agents num_turns: Number of turns per episode num_train_epochs: Number of training epochs agent_learning_rate: Learning rate logging_steps: Log every N steps num_generations: Number of generations to sample per prompt for each agent max_new_tokens: Maximum number of new tokens to generate temperature: Temperature for sampling top_p: Top-p for sampling top_k: Top-k for sampling discount: Discount factor gamma over turns for returns joint_mode: Joint action composition (aligned for index-aligned, cross for Cartesian product) early_termination_threshold: Stop rollouts with mean reward exceeds a threshold rollout_buffer_size: Number of node samples to buffer before update train_batch_size: Mini-batch size within each update advantage_normalization: Whether to normalize advantages eval_interval: Run evaluation every N training batches eval_num_samples: Number of samples to evaluate per evaluation run eval_batch_size: Eval dataloader batch size external_prompt_passthrough: Use external prompts directly in multi-turn advantage_mode: Baseline mode (mean, max, rloo, raw) MAGRPOTrainer setup:\nagent_model or agents: Model identifier string for homogeneous agents, or list of agent models (multi-agent agent_model must be a string) num_agents: Number of agents tokenizer: The tokenizer (required) reward_func: Callable that returns a list of floats (required) reward_processor: Optional processor to apply to rewards (e.g., scaling) formatters: Single callable or list of callables for each agent to format prompts args: Instance of MAGRPOConfig (optional) train_dataset: Training dataset (required) eval_dataset: Evaluation dataset (optional) model_config: Model configuration dict (optional) wandb_config: Configuration for Weights \u0026amp; Biases logging (optional) external_transition: Function providing transitions between turns eval_logger: Evaluation logger function (optional) eval_aggregator: Evaluation aggregator function (optional) For simplicity, MAGRPO computes the policy gradient using the current policy\u0026rsquo;s samples without importance sampling or ratio clipping. And since it does not use a critic model, there is no value_clip_range applicable.\nThe trainer uses a fixed training DataLoader batch size of 1 and requires at least num_generations=2 generations for group baseline computation. The training use batch gradient descent by default, where train_batch_size=rollout_buffer_size.\nOther Variants# CoMLRL also provides other MA-REINFORCE variants with different baselines:\nMARLOO: Multi‑Agent REINFORCE Leave‑One‑Out. Baseline is the mean return of other agents (leave‑one‑out) at the same step. \\[ J(\\theta_i) = \\mathbb{E}_{\\mathbf{o}_0 \\sim \\mathcal{D}, \\mathbf{h}_t \\sim \\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}} \\Bigg[\\frac{1}{K}\\sum_{k=1}^{K}\\sum_{t=0}^{H-1} \\left(R^{k}_t - \\frac{1}{K-1}\\sum_{l=1, l\\neq k}^{K}R^{l}_t\\right) \\cdot \\log \\pi_{\\theta_i}(a^{k}_{i,t}\\mid h_{i,t})\\Bigg]. \\] MAREMAX: Multi‑Agent REINFORCE with Group Max. Baseline is the maximum group return at the step. \\[ J(\\theta_i) = \\mathbb{E}_{\\mathbf{o}_0 \\sim \\mathcal{D}, \\mathbf{h}_t \\sim \\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}} \\Bigg[\\frac{1}{K}\\sum_{k=1}^{K}\\sum_{t=0}^{H-1} \\left(R^{k}_t - \\mathrm{max}_l\\, R^l_t \\right) \\cdot \\log \\pi_{\\theta_i}(a^{k}_{i,t}\\mid h_{i,t})\\Bigg]. \\] These classes and MA-REINFORCE are derived from comlrl.trainers.reinforce.MAGRPOTrainer. Interfaces for the trainer and configuration classes are the same as MAGRPOTrainer and MAGRPOConfig.\n"},{"id":10,"href":"/docs/env/minecraft/","title":"Minecraft","section":"Environments","content":"Multi-agent building environments in Minecraft. The LLM_Collab_Minecraft repository includes:\nStrBuild – agents collaboratively build structured designs from text. HouseBuild – agents coordinate materials and steps to construct houses. "},{"id":11,"href":"/docs/user-guide/multi-agent-actor-critic/","title":"Multi-Agent Actor-Critic","section":"User Guide","content":"Actor-Critic (AC) methods are widely-used policy gradient that employ critics to facilitate training. AC methods can achieve lower variance and better sample efficiency than REINFORCE, but this requires careful design and tuning of the critic to ensure stable training. In Multi-Agent Reinforcement Learning (MARL), Actor-Critic methods can be instantiated as Multi-Agent Actor-Critic (MAAC) and Independent Actor-Critic (IAC).\nMAAC# Multi-Agent Actor-Critic (MAAC) uses a Centralized Critic (CC) across agents to evaluate the values of joint histories \\( V_{\\boldsymbol{\\phi}}(\\mathbf{h}_t) \\) or joint history-action pairs \\( Q_{\\boldsymbol{\\psi}}(\\mathbf{h}_t, \\mathbf{a}_t) \\). The policy gradient of each agent is:\n\\[ \\nabla_{\\theta_i} J(\\theta_i) = \\mathbb{E}_{\\boldsymbol{\\pi}}\\left[\\sum_{t=0}^{H-1} \\nabla_{\\theta_i} \\log \\pi_{\\theta_i}(a_{i,t}|h_{i,t})\\,\\boldsymbol{\\delta}_t\\right] \\] where \\( \\boldsymbol{\\delta}_t = r_t \u0026#43; \\gamma V_{\\boldsymbol{\\phi}}(\\mathbf{h}_{t\u0026#43;1}) - V_{\\boldsymbol{\\phi}}(\\mathbf{h}_{t}) \\), and the critic is updated by:\n\\[ \\mathcal{L}(\\boldsymbol{\\phi}) = \\mathbb{E}_{\\boldsymbol{\\pi}}\\left[\\sum_{t=0}^{H-1} \\big(r_t \u0026#43; \\gamma V_{\\phi}(\\mathbf{h}_{t\u0026#43;1}) - V_{\\phi}(\\mathbf{h}_{t})\\big)^2\\right]. \\] MAACConfig parameters:\nnum_agents: Number of agents num_turns: Number of turns critic_type: Critic target type (v for V(h), q for Q(h,a)) num_train_epochs: Number of training epochs agent_learning_rate: Learning rate for agents critic_learning_rate: Learning rate for shared critic value_loss_coef: Weight on critic loss advantage_normalization: Whether to normalize advantages before updates rollout_buffer_size: Number of samples to collect per agent before an update train_batch_size: Mini-batch size within each update max_new_tokens: Maximum tokens to generate per completion temperature: Temperature for sampling top_p: Top-p for nucleus sampling top_k: Top-k for sampling num_generations: Number of generations per prompt per agent external_prompt_passthrough: Use external prompts directly in multi-turn discount: Discount factor for multi-turn returns early_termination_threshold: Optional early-stop threshold for multi-turn eval_interval: Evaluation interval (in training batches) eval_num_samples: Number of evaluation samples per interval eval_batch_size: Eval dataloader batch size logging_steps: Log every N training batches MAACTrainer setup:\nagent_model or agents: Actor model identifier string for homogeneous agents, or list of agent models (multi-agent agent_model must be a string) critic_model or critics: Required single shared critic (either one identifier or a 1-element list) tokenizer: Tokenizer (required) reward_func: Callable returning rewards (required) reward_processor: Optional reward post-processor formatters: Single callable or list for per-agent prompt formatting args: Instance of MAACConfig (optional) train_dataset: Training dataset (required) eval_dataset: Optional evaluation dataset model_config: Extra model kwargs (optional) wandb_config: Weights \u0026amp; Biases logging config (optional) metrics_callback: Optional callback for custom metrics For simplicity, IAC computes the policy gradient using the current policy\u0026rsquo;s samples without importance sampling or ratio clipping. The value_clip_range is not applicable in MAAC.\nThe trainer uses a fixed training DataLoader batch size of 1. For num_turns \u0026gt; 1, provide an external_transition and set num_generations=1. The training use batch gradient descent by default, where train_batch_size=rollout_buffer_size.\nIAC# Independent Actor-Critic (IAC) optimizes each agent\u0026rsquo;s policy independently while using joint returns from multiple agents. Each agent maintains its own actor and critic, other agents serve as part of the environment. The policy gradient for each agent is:\n\\[ \\nabla_{\\theta_i} J(\\theta_i) = \\mathbb{E}_{\\boldsymbol{\\pi}}\\left[\\sum_{t=0}^{H-1} \\nabla_{\\theta_i} \\log \\pi_{\\theta_i}(a_{i,t}|h_{i,t})\\,\\delta_{i,t}\\right] \\] where \\( \\delta_{i,t} = r_t \u0026#43; \\gamma V_{\\phi_i}(h_{i,t\u0026#43;1}) - V_{\\phi_i}(h_{i,t}) \\).\nCoMLRL supports two IAC variants:\nSeparate Critic: Uses an independent model for value estimation separate from the actor. It provides more stable training but occupies larger storage and VRAM usage.\nShared Model: Attaches a small value-head to the transformer backbone, sharing the actor model\u0026rsquo;s history (or history-action) representations to reduce the space costs.\nThe critics are updated by minimizing the TD error:\n\\[ \\mathcal{L}(\\phi_i) = \\mathbb{E}_{\\boldsymbol{\\pi}}\\left[\\sum_{t=0}^{H-1} \\big(r_t \u0026#43; \\gamma V_{\\phi_i}(h_{i,t\u0026#43;1}) - V_{\\phi_i}(h_{i,t})\\big)^2\\right]. \\] When using shared model use_separate_critic=false, a value clip value_clip_range can be applied to improve training stability.\n\\[ L(\\phi_i) = \\max\\Big( (V_{\\phi_i}(h_t) - \\hat{V}_t)^2,\\ (V_{\\phi_i}^{\\text{clip}}(h_t) - \\hat{V}_t)^2 \\Big) \\\\ V_{\\phi_i}^{\\text{clip}}(h_t) = V_{\\phi_i}^{\\text{old}}(h_t) \u0026#43; \\mathrm{clip}(V_{\\phi_i}(h_t) - V_{\\phi_i}^{\\text{old}}(h_t), -\\epsilon, \\epsilon), \\] IACConfig provides parameters for configuring Independent Actor-Critic training:\nnum_agents: Number of agents num_turns: Number of turns num_train_epochs: Number of training epochs agent_learning_rate: Learning rate for agents critic_learning_rate: Learning rate for critic value_loss_coef: Coefficient for value loss value_clip_range: Clipping range for value function advantage_normalization: Whether to normalize advantages rollout_buffer_size: Number of samples to collect before update train_batch_size: Mini-batch size for policy updates max_new_tokens: Maximum new tokens to generate temperature: Temperature for sampling top_p: Top-p for nucleus sampling top_k: Top-k for sampling num_generations: Number of generations per prompt per agent use_separate_critic: Whether to use separate critic model critic_type: Critic target type (v for V(h), q for Q(h,a)) critic_value_head_hidden_dim: Hidden dimension for critic value head value_head_hidden_dim: Hidden dimension for value head in shared-critic mode external_prompt_passthrough: Use external prompts directly in multi-turn discount: Discount factor for multi-turn returns early_termination_threshold: Optional early-stop threshold for multi-turn eval_interval: Evaluation interval (in training batches) eval_num_samples: Number of evaluation samples per interval eval_batch_size: Eval dataloader batch size logging_steps: Log every N training batches IACTrainer trains agents using Independent Actor-Critic:\nagent_model or agents: Model identifier string for homogeneous agents, or list of agent models (multi-agent agent_model must be a string) critic_model or critics: Critic identifier or list of critic models when use_separate_critic=true tokenizer: The tokenizer (required) reward_func: Callable that returns a list of floats (required) reward_processor: Optional processor to apply to rewards formatters: Single callable or list of callables for each agent to format dataset items into prompts args: Instance of IACConfig (optional) train_dataset: Training dataset (required) eval_dataset: Evaluation dataset (optional) model_config: Model configuration dict (optional) wandb_config: Configuration for Weights \u0026amp; Biases logging (optional) metrics_callback: Optional callback for custom metrics external_transition: Optional transition function required for multi-turn training For simplicity, IAC computes the policy gradient using the current policy\u0026rsquo;s samples without importance sampling or ratio clipping. In shared-critic mode (use_separate_critic=false), value heads are attached to the actor models (do not pass critic_model/critics; passing them raises an error), and agents may be homogeneous or heterogeneous; this mode can be less stable, and value_clip_range only applies there. In separate-critic mode (use_separate_critic=true), pass a critics list with length equal to num_agents or a single critic_model to be broadcast; critic models may differ from actor models.\nThe trainer uses a fixed training DataLoader batch size of 1. For num_turns \u0026gt; 1, provide an external_transition and set num_generations=1. The training use batch gradient descent by default, where train_batch_size=rollout_buffer_size.\n"},{"id":12,"href":"/docs/user-guide/multi-turn-training/","title":"Multi-Turn Training","section":"User Guide","content":"Many complex problems cannot be solved in a single turn. LLM agents need to interact with the environment to obtain useful feedback from other models or tools involved in the system.\nMulti-Turn MAGRPO# MAGRPO in the multi-turn setting forms a tree-structured rollout expansion where branches represent different joint responses (TreeRPO).\nIn each episode, a task is sampled from the dataset to construct initial observations \\( \\mathbf{o}_0=\\{o_{1, 0}, \\cdots, o_{n, 0}\\} \\) and histories \\( \\mathbf{h}_0=\\{h_{1, 0}, \\cdots, h_{n, 0}\\} \\) for all agents. At each turn, agents generate a group of joint responses \\( \\mathbf{a}^{\\mathcal{G}}_t\\gets\\boldsymbol{\\pi}^{\\mathcal{G}}(\\cdot|\\mathbf{h}_t) \\) from their current observation-action history \\( \\mathbf{h}_t \\), with each response initiating a distinct rollout. Agents receive joint rewards \\( r^{(g)}_{t} \\) for each response based on the accumulated history \\( \\mathbf{a}^{(g)}_{t} \\in \\mathbf{a}^{\\mathcal{G}}_{t} \\) and current action. Each rollout then evolves independently, producing new joint observations \\( \\mathbf{o}^{\\mathcal{G}}_{t\u0026#43;1} \\) as the environment dynamics unfold and spawning more rollouts at the next turn \\( t\u0026#43;1 \\). This process continues until the terminal turn is reached \\( H \\).\nJoint Mode# MAGRPO supports two modes for forming joint responses at each turn:\nAlign: Provides flexibility in the number of joint responses generated per turn, allowing any number of generations at each turn. However, generations are not fully utilized since only aligned responses across agents are combined. As training progresses over \\( T \\) turns with \\( N \\) agents, the total number of leaves grows as \\( G^T \\), where \\( G \\) is the number of generations per turn. Cross: Maximizes the utilization of generations and provides more accurate value estimation with more samples by forming the Cartesian product of all agent responses. As training progresses over \\( T \\) turns with \\( N \\) agents, the total number of leaves grows as \\( G^{N \\cdot T} \\), where each node has \\( G^N \\) sibling joint actions. Note that only responses originating from the same rollout can be combined, as rollouts evolve independently.\nMulti-Turn MAAC/IAC# Different from MAGRPO rollout tree that generates numerous samples in each episode, MAAC/IAC with multi-turn training only generates one sample per agent at each turn in the multi-turning setting, and the episode continues for multiple turns until termination. Since the value estimation in MAAC/IAC is updated based on temporal difference error, the agents don\u0026rsquo;t need to wait until the end of the episode to update their policies, and can learn online. This feature is especially useful in the coordination optimization in long-horizon settings. The external feedback mechanism controls how environment observations are incorporated into prompts for subsequent turns, which is usually implemented in CoMLRL\u0026rsquo;s downstreaming environments.\nEnvironment Transition# External feedback mechanisms control how environment observations are incorporated into prompts for subsequent turns, this is usually implemented in CoMLRL\u0026rsquo;s downstreaming environments.\nCustom External Feedback# Users can implement custom external feedback by defining a function with the following interface:\nCustom External Feedback Interface:\nprompt: Original task prompt/problem description (required) agent_completions: List or tuple of completions from the previous turn, one per agent (required) num_agents: Number of agents in the system (required) prompt_history_per_agent: List of prompt histories for each agent, where each history is a list of prompts from previous turns (optional) response_history_per_agent: List of response histories for each agent, where each history is a list of responses from previous turns (optional) The function must return a list or tuple of prompts for the next turn, one for each agent. The trainer only passes the arguments above (no extra kwargs), so any mode-specific parameters should be captured via closure or functools.partial.\nBy default, returned prompts are inserted as the new prompt field and then passed through each agent\u0026rsquo;s formatter. If external_prompt_passthrough=true, the returned prompts are used directly without re-formatting. In MAGRPO, the external transition is called per rollout branch with that branch\u0026rsquo;s histories.\nFor example:\ndef custom_external( prompt: str, agent_completions: List[str], num_agents: int, prompt_history_per_agent: Optional[List[List[str]]] = None, response_history_per_agent: Optional[List[List[str]]] = None, ) -\u0026gt; List[str]: # Custom logic to format next-turn prompts # Access environment feedback, tool outputs, etc. next_turn_prompts = [] for i in range(num_agents): # Format prompt for agent i based on history and feedback next_prompt = f\u0026#34;{prompt}\\nPrevious attempt: {agent_completions[i]}\\nPlease revise.\u0026#34; next_turn_prompts.append(next_prompt) return next_turn_prompts For IAC/MAAC multi-turn training, num_generations must be set to 1.\nExample Modes (Expert, Diagnosis, and Self-Improvement)# An environment for code generation includes 3 example external transition modes:\nexternal.mode=expert_edits: Uses an external LLM (default: DeepSeek-Coder) to propose code edits. Follow-up prompts include edit suggestions with context from previous turns. It can be configured via expert_model for different experts (e.g., Claude, GPT) when API keys are available.\nexternal.mode=level_feedback: Static AST checks and dynamically executes code to provide diagnosis. The default sandbox test includes the first test; configurable via sandbox_slice to include all tests (0, None, or \u0026lsquo;all\u0026rsquo;), specific number of tests (negative values enabled).\nexternal.mode=plain: Self-improvement mode that just includes prompts and responses in the previous turns and a revision instruction.\n"},{"id":13,"href":"/docs/env/","title":"Environments","section":"Docs","content":"Environments that simulate real-world tasks for training and evaluating LLM collaboration:\nWriting: Multiple LLM agents collaborate on processing articles.\nTLDR - Summarizing Reddit posts. ArXiv - Expanding abstracts into introductions. Coding: Generate code solutions for programming problems.\nMBPP - Mostly basic python problems. HumanEval - Handwritten evaluation problems. CoopHumanEval - HumanEval with cooperative nature. Minecraft: Multi-agent building environments.\nStrBuild - Structured builds from text. HouseBuild - Coordinated house construction. "}]