<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>CoMLRL</title><link>/</link><description>Recent content on CoMLRL</description><generator>Hugo</generator><language>en-us</language><atom:link href="/index.xml" rel="self" type="application/rss+xml"/><item><title>CoMLRL Quick Start</title><link>/docs/examples/comlrl-quick-start/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/examples/comlrl-quick-start/</guid><description>&lt;p&gt;This tutorial demonstrates how to train two LLM agents to collaborate to tell a story. The first agent generates a compact story setup, while the second agent produces a longer version. The reward function encourages the second agent&amp;rsquo;s output to be 2–3× longer than the first agent&amp;rsquo;s.&lt;/p&gt;
&lt;p&gt;To run this demo, please have at least 24 GB of GPU memory available. You can also visualize the training process by setting up your WandB dashboard.&lt;/p&gt;</description></item><item><title>Installation</title><link>/docs/user-guide/installation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/user-guide/installation/</guid><description>&lt;p&gt;CoMLRL provides several different ways for installation.&lt;/p&gt;
&lt;h2 id="install-from-pypi"&gt;Install from PyPI&lt;a class="anchor" href="#install-from-pypi"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;pip install comlrl
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;# install PyTorch compatible with your device&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="install-from-conda-forge"&gt;Install from conda-forge&lt;a class="anchor" href="#install-from-conda-forge"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;conda install -c conda-forge comlrl
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;# install PyTorch compatible with your device&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="install-from-source"&gt;Install from source&lt;a class="anchor" href="#install-from-source"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;To access the latest features of CoMLRL or to develop CoMLRL, clone this repository and install in editable mode:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;git clone https://github.com/OpenMLRL/CoMLRL.git
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;cd&lt;/span&gt; CoMLRL
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;pip install -e .
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;# install PyTorch compatible with your device&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description></item><item><title>Support</title><link>/docs/dev/support/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/dev/support/</guid><description>&lt;p&gt;We are willing to help you with any issues you encounter while using CoMLRL.&lt;/p&gt;
&lt;h2 id="report-issues"&gt;Report Issues&lt;a class="anchor" href="#report-issues"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;If you are stuck with a problem using CoMLRL, please follow this procedure:&lt;/p&gt;
&lt;div class="book-steps"&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Read the documentation first, including using the search feature (Ctrl + K).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Search the &lt;a href="https://github.com/OpenMLRL/CoMLRL/issues"&gt;GitHub Issues&lt;/a&gt; archives to see if someone else already had the same problem.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Before writing, try to create a minimal example that reproduces the problem. You&amp;rsquo;ll get the fastest response if you can send just a handful of lines of code that show what isn&amp;rsquo;t working.&lt;/p&gt;</description></item><item><title>Writing</title><link>/docs/env/writing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/env/writing/</guid><description>&lt;p&gt;Collaborative summarization and expansion tasks for pairs (or teams) of LLMs.
The reference implementation lives in the
&lt;a href="https://github.com/OpenMLRL/LLM_Collab_Writing"&gt;LLM_Collab_Writing&lt;/a&gt; repository
and includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;TLDR&lt;/strong&gt; – distills Reddit threads into concise summaries.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ArXiv Introductions&lt;/strong&gt; – grows short abstracts into multi-paragraph drafts.&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Coding</title><link>/docs/env/coding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/env/coding/</guid><description>&lt;p&gt;A suite of cooperative programming benchmarks where agents propose, critique, and
refine solutions. The environments shipped in
&lt;a href="https://github.com/OpenMLRL/LLM_Collab_Code_Generation"&gt;LLM_Collab_Code_Generation&lt;/a&gt;
cover:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MBPP&lt;/strong&gt; – mostly basic Python problems for rapid iteration.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HumanEval&lt;/strong&gt; – handwritten tasks from OpenAI for exact-match grading.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CoopHumanEval&lt;/strong&gt; – HumanEval variants that explicitly require collaboration.&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Contributing</title><link>/docs/dev/contributing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/dev/contributing/</guid><description>&lt;p&gt;Thanks for your interest in helping build CoMLRL! This guide walks you through reporting issues, contributing changes, and keeping the codebase healthy.&lt;/p&gt;
&lt;h2 id="development-guidelines"&gt;Development Guidelines&lt;a class="anchor" href="#development-guidelines"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;div class="book-steps"&gt;
&lt;ol&gt;
&lt;li&gt;Fork the upstream repository.&lt;/li&gt;
&lt;li&gt;Clone your fork and synchronize with upstream:
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; git clone https://github.com/&amp;lt;your-username&amp;gt;/CoMLRL.git
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020"&gt;cd&lt;/span&gt; CoMLRL
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; git remote add upstream https://github.com/OpenMLRL/CoMLRL.git
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; git fetch upstream
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; git checkout -b feature/&amp;lt;short-description&amp;gt; upstream/main
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; git fetch upstream &lt;span style="color:#666"&gt;&amp;amp;&amp;amp;&lt;/span&gt; git rebase upstream/main&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;Implement new features or fix bugs, updating documentation as needed.&lt;/li&gt;
&lt;li&gt;Open a pull request to the upstream repository and wait for review.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description></item><item><title>Model Loading</title><link>/docs/user-guide/model-loading/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/user-guide/model-loading/</guid><description>&lt;p&gt;CoMLRL supports both homogeneous and heterogeneous models.
Users can assign &lt;code&gt;agent_model&lt;/code&gt;/&lt;code&gt;critic_model&lt;/code&gt; with &lt;a href="https://huggingface.co/models"&gt;HuggingFace model identifiers&lt;/a&gt; for homogeneous setups, or provide &lt;code&gt;agents&lt;/code&gt;/&lt;code&gt;critics&lt;/code&gt; lists for heterogeneous setups.&lt;/p&gt;
&lt;h2 id="homogeneous-agents"&gt;Homogeneous Agents&lt;a class="anchor" href="#homogeneous-agents"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The easiest way to start the journey of CoMLRL is to load &lt;code&gt;num_agents&lt;/code&gt; homogeneous agents with a single model identifier.
Users can set &lt;code&gt;agent_model.name&lt;/code&gt; to a single model identifier while keeping &lt;code&gt;agents: null&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example, to load 3 &lt;em&gt;Qwen/Qwen2.5-1.5B&lt;/em&gt; agents:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;trainer &lt;span style="color:#666"&gt;=&lt;/span&gt; MAGRPOTrainer(
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; agent_model&lt;span style="color:#666"&gt;=&lt;/span&gt;&lt;span style="color:#4070a0"&gt;&amp;#34;Qwen/Qwen2.5-1.5B&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; agents&lt;span style="color:#666"&gt;=&lt;/span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; num_agents&lt;span style="color:#666"&gt;=&lt;/span&gt;&lt;span style="color:#40a070"&gt;3&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="heterogeneous-agents"&gt;Heterogeneous Agents&lt;a class="anchor" href="#heterogeneous-agents"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Although homogeneous LLM agents can be specified into different roles by prompting, using heterogeneous LLMs with different skills can further unleash the potential of multi-agent collaboration.
Users can load a list of heterogeneous agents in &lt;code&gt;agents&lt;/code&gt;, where the length of the list should match &lt;code&gt;num_agents&lt;/code&gt;. Each entry should specify a model identifier and optional tokenizer/model kwargs.
When &lt;code&gt;agents&lt;/code&gt; is provided, &lt;code&gt;agent_model&lt;/code&gt; should be set to null or ignored; if both are provided, they must match (same names, correct length) or training will raise an error.&lt;/p&gt;</description></item><item><title>Changelog</title><link>/docs/dev/changelog/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/dev/changelog/</guid><description>&lt;hr&gt;
&lt;h2 id="version-136"&gt;Version 1.3.6&lt;a class="anchor" href="#version-136"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fixed the bug of loading heterogeneous models and reform the loading logics&lt;/li&gt;
&lt;li&gt;Reconstruct the docs&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="version-135"&gt;Version 1.3.5&lt;a class="anchor" href="#version-135"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Add unit tests for hyperparameter constraints.&lt;/li&gt;
&lt;li&gt;Clean legacy interfaces.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="version-134"&gt;Version 1.3.4&lt;a class="anchor" href="#version-134"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fix the bug of loading heterogeneous models and reform the loading logics.&lt;/li&gt;
&lt;li&gt;Enable MBGD in MAGRPO to align with MAAC and IAC.&lt;/li&gt;
&lt;li&gt;Remove redundant and legacy hyperparameters (e.g., model kwargs, patching hyperparameters).&lt;/li&gt;
&lt;li&gt;Clean multi-device legacy, like drop last and num_workers.&lt;/li&gt;
&lt;li&gt;Add unit tests for model loading and separate it from CI as a badge.&lt;/li&gt;
&lt;li&gt;Clean short functions.&lt;/li&gt;
&lt;li&gt;Reorganize the docs and align the parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="version-133"&gt;Version 1.3.3&lt;a class="anchor" href="#version-133"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Compact MAREINFORCETrainer derivation, and move to the new folder.&lt;/li&gt;
&lt;li&gt;Unify the interface for different trainers.&lt;/li&gt;
&lt;li&gt;Remove redundant patches and wrappers.&lt;/li&gt;
&lt;li&gt;Reorganize the variables in the config yamls.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="version-132"&gt;Version 1.3.2&lt;a class="anchor" href="#version-132"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fix wandb logging issue in MAGRPOTrainer&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="version-131"&gt;Version 1.3.1&lt;a class="anchor" href="#version-131"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Allow batch training in MAGRPOTrainer, IACTrainer and MAACTrainer&lt;/li&gt;
&lt;li&gt;Allow multi-turn training in IACTrainer and MAACTrainer&lt;/li&gt;
&lt;li&gt;Change the x-axis from data_step to env_step&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="version-130"&gt;Version 1.3.0&lt;a class="anchor" href="#version-130"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Use TD error as critic update target in IACTrainer and MAACTrainer.&lt;/p&gt;</description></item><item><title>Code Completion</title><link>/docs/env/code-completion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/env/code-completion/</guid><description>&lt;p&gt;Multi-agent autocompletion tasks where each model fills in part of a codebase.
The &lt;a href="https://github.com/OpenMLRL/LLM_Collab_Code_Completion"&gt;LLM_Collab_Code_Completion&lt;/a&gt;
project currently focuses on &lt;strong&gt;ClassEval&lt;/strong&gt;, which asks teams of LLMs to finish
class skeletons based on docstrings and partially implemented methods.&lt;/p&gt;</description></item><item><title>Multi-Agent REINFORCE</title><link>/docs/user-guide/multi-agent-reinforce/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/user-guide/multi-agent-reinforce/</guid><description>&lt;p&gt;REINFORCE is a class of policy gradient methods that optimize the policy directly using sampled returns.
It has been widely used to fine-tune LLMs because of its simplicity and efficiency, e.g., &lt;a href="https://arxiv.org/pdf/2402.03300"&gt;GRPO&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2503.20783"&gt;Dr. GRPO&lt;/a&gt;, &lt;a href="https://openreview.net/forum?id=r1lgTGL5DE"&gt;RLOO&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2310.1050"&gt;ReMax&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2506.05183"&gt;TreeRPO&lt;/a&gt;, and &lt;a href="https://arxiv.org/abs/2501.03262"&gt;REINFORCE++&lt;/a&gt;.
REINFORCE can be extended to multi-agent settings, where multiple LLM agents response synchronously and their joint responses form a solution at each turn to receive a shared reward at each turn.&lt;/p&gt;
&lt;h2 id="ma-reinforce"&gt;MA-REINFORCE&lt;a class="anchor" href="#ma-reinforce"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The naive Multi‑Agent REINFORCE (MA-REINFORCE) can be expressed as:&lt;/p&gt;</description></item><item><title>Minecraft</title><link>/docs/env/minecraft/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/env/minecraft/</guid><description>&lt;p&gt;Multi-agent building environments in Minecraft. The
&lt;a href="https://github.com/OpenMLRL/LLM_Collab_Minecraft"&gt;LLM_Collab_Minecraft&lt;/a&gt;
repository includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/OpenMLRL/LLM_Collab_Minecraft/tree/main/str_build"&gt;StrBuild&lt;/a&gt;&lt;/strong&gt; – agents collaboratively build structured designs from text.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/OpenMLRL/LLM_Collab_Minecraft/tree/main/house_build"&gt;HouseBuild&lt;/a&gt;&lt;/strong&gt; – agents coordinate materials and steps to construct houses.&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Multi-Agent Actor-Critic</title><link>/docs/user-guide/multi-agent-actor-critic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/user-guide/multi-agent-actor-critic/</guid><description>&lt;p&gt;Actor-Critic (AC) methods are widely-used policy gradient that employ critics to facilitate training.
AC methods can achieve lower variance and better sample efficiency than REINFORCE, but this requires careful design and tuning of the critic to ensure stable training.
In Multi-Agent Reinforcement Learning (MARL), Actor-Critic methods can be instantiated as Multi-Agent Actor-Critic (MAAC) and Independent Actor-Critic (IAC).&lt;/p&gt;
&lt;h2 id="maac"&gt;MAAC&lt;a class="anchor" href="#maac"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Multi-Agent Actor-Critic (MAAC) uses a Centralized Critic (CC) across agents to evaluate the values of joint histories &lt;span class="book-katex"&gt;\( V_{\boldsymbol{\phi}}(\mathbf{h}_t) \)&lt;/span&gt;&lt;link rel="stylesheet" href="./katex/katex.min.css" /&gt;&lt;script defer src="./katex/katex.min.js"&gt;&lt;/script&gt;&lt;script defer src="./katex/auto-render.min.js" onload="renderMathInElement(document.body, {&amp;#34;delimiters&amp;#34;:[{&amp;#34;left&amp;#34;:&amp;#34;$$&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;$$&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\(&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\)&amp;#34;,&amp;#34;display&amp;#34;:false},{&amp;#34;left&amp;#34;:&amp;#34;\\[&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\]&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\begin{equation}&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\end{equation}&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\begin{align}&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\end{align}&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\begin{gather}&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\end{gather}&amp;#34;,&amp;#34;display&amp;#34;:true}],&amp;#34;throwOnError&amp;#34;:false});"&gt;&lt;/script&gt; or joint history-action pairs &lt;span class="book-katex"&gt;\( Q_{\boldsymbol{\psi}}(\mathbf{h}_t, \mathbf{a}_t) \)&lt;/span&gt;.
The policy gradient of each agent is:&lt;/p&gt;</description></item><item><title>Multi-Turn Training</title><link>/docs/user-guide/multi-turn-training/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/user-guide/multi-turn-training/</guid><description>&lt;p&gt;Many complex problems cannot be solved in a single turn. LLM agents need to interact with the environment to obtain useful feedback from other models or tools involved in the system.&lt;/p&gt;
&lt;h2 id="multi-turn-magrpo"&gt;Multi-Turn MAGRPO&lt;a class="anchor" href="#multi-turn-magrpo"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;MAGRPO in the multi-turn setting forms a tree-structured rollout expansion where branches represent different joint responses (&lt;a href="https://arxiv.org/abs/2506.05183"&gt;TreeRPO&lt;/a&gt;).&lt;/p&gt;
&lt;p align="center"&gt;
 &lt;img src="./img/joint-tree.svg" width="450px;" alt=""/&gt;
&lt;/p&gt;
&lt;p&gt;In each episode, a task is sampled from the dataset to construct initial observations &lt;span class="book-katex"&gt;\( \mathbf{o}_0=\{o_{1, 0}, \cdots, o_{n, 0}\} \)&lt;/span&gt;&lt;link rel="stylesheet" href="./katex/katex.min.css" /&gt;&lt;script defer src="./katex/katex.min.js"&gt;&lt;/script&gt;&lt;script defer src="./katex/auto-render.min.js" onload="renderMathInElement(document.body, {&amp;#34;delimiters&amp;#34;:[{&amp;#34;left&amp;#34;:&amp;#34;$$&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;$$&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\(&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\)&amp;#34;,&amp;#34;display&amp;#34;:false},{&amp;#34;left&amp;#34;:&amp;#34;\\[&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\]&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\begin{equation}&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\end{equation}&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\begin{align}&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\end{align}&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\begin{gather}&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\end{gather}&amp;#34;,&amp;#34;display&amp;#34;:true}],&amp;#34;throwOnError&amp;#34;:false});"&gt;&lt;/script&gt; and histories &lt;span class="book-katex"&gt;\( \mathbf{h}_0=\{h_{1, 0}, \cdots, h_{n, 0}\} \)&lt;/span&gt; for all agents. At each turn, agents generate a group of joint responses &lt;span class="book-katex"&gt;\( \mathbf{a}^{\mathcal{G}}_t\gets\boldsymbol{\pi}^{\mathcal{G}}(\cdot|\mathbf{h}_t) \)&lt;/span&gt; from their current observation-action history &lt;span class="book-katex"&gt;\( \mathbf{h}_t \)&lt;/span&gt;, with each response initiating a distinct rollout. Agents receive joint rewards &lt;span class="book-katex"&gt;\( r^{(g)}_{t} \)&lt;/span&gt; for each response based on the accumulated history &lt;span class="book-katex"&gt;\( \mathbf{a}^{(g)}_{t} \in \mathbf{a}^{\mathcal{G}}_{t} \)&lt;/span&gt; and current action. &lt;strong&gt;Each rollout then evolves independently&lt;/strong&gt;, producing new joint observations &lt;span class="book-katex"&gt;\( \mathbf{o}^{\mathcal{G}}_{t&amp;#43;1} \)&lt;/span&gt; as the environment dynamics unfold and spawning more rollouts at the next turn &lt;span class="book-katex"&gt;\( t&amp;#43;1 \)&lt;/span&gt;. This process continues until the terminal turn is reached &lt;span class="book-katex"&gt;\( H \)&lt;/span&gt;.&lt;/p&gt;</description></item></channel></rss>