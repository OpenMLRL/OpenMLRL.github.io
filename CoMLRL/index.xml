<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>CoMLRL</title><link>https://openmlrl.github.io/CoMLRL/</link><description>Recent content on CoMLRL</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://openmlrl.github.io/CoMLRL/index.xml" rel="self" type="application/rss+xml"/><item><title>CoMLRL Quick Demo</title><link>https://openmlrl.github.io/CoMLRL/docs/examples/quick-demo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openmlrl.github.io/CoMLRL/docs/examples/quick-demo/</guid><description>&lt;p&gt;This tutorial demonstrates how to train two LLM agents to collaborate to tell a story. The first agent generates a compact story setup, while the second agent produces a longer version. The reward function encourages the second agent&amp;rsquo;s output to be 2–3× longer than the first agent&amp;rsquo;s.&lt;/p&gt;
&lt;p&gt;To run this demo, please have at least 24 GB of GPU memory available. You can also visualize the training process by setting up your WandB dashboard.&lt;/p&gt;</description></item><item><title>Installation</title><link>https://openmlrl.github.io/CoMLRL/docs/user-guide/installation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openmlrl.github.io/CoMLRL/docs/user-guide/installation/</guid><description>&lt;p&gt;You can create a venv or conda environment with Python 3.10+ and install CoMLRL as follows.&lt;/p&gt;
&lt;h2 id="install-from-pypi"&gt;Install from PyPI&lt;a class="anchor" href="#install-from-pypi"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;pip install comlrl
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;# Install PyTorch compatible with your device&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="install-from-conda-forge"&gt;Install from conda-forge&lt;a class="anchor" href="#install-from-conda-forge"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;conda install -c conda-forge comlrl
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;# Install PyTorch compatible with your device&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="install-from-source"&gt;Install from source&lt;a class="anchor" href="#install-from-source"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;To access the latest features of CoMLRL, clone this repository and install in editable mode:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;git clone https://github.com/OpenMLRL/CoMLRL.git
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;cd&lt;/span&gt; CoMLRL
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;pip install -e .
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;# Install PyTorch compatible with your device&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description></item><item><title>Support</title><link>https://openmlrl.github.io/CoMLRL/docs/dev/support/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openmlrl.github.io/CoMLRL/docs/dev/support/</guid><description>&lt;p&gt;We are willing to help you with any issues you encounter while using CoMLRL.&lt;/p&gt;
&lt;h2 id="report-issues"&gt;Report Issues&lt;a class="anchor" href="#report-issues"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;If you are stuck with a problem using CoMLRL, please follow this procedure:&lt;/p&gt;
&lt;div class="book-steps"&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Read the documentation first, including using the search feature (Ctrl + K).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Search the &lt;a href="https://github.com/OpenMLRL/CoMLRL/issues"&gt;GitHub Issues&lt;/a&gt; archives to see if someone else already had the same problem.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Before writing, try to create a minimal example that reproduces the problem. You&amp;rsquo;ll get the fastest response if you can send just a handful of lines of code that show what isn&amp;rsquo;t working.&lt;/p&gt;</description></item><item><title>Contributing</title><link>https://openmlrl.github.io/CoMLRL/docs/dev/contributing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openmlrl.github.io/CoMLRL/docs/dev/contributing/</guid><description>&lt;p&gt;Thanks for your interest in helping build CoMLRL! This guide walks you through reporting issues, contributing changes, and keeping the codebase healthy.&lt;/p&gt;
&lt;h2 id="development-guidelines"&gt;Development Guidelines&lt;a class="anchor" href="#development-guidelines"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;div class="book-steps"&gt;
&lt;ol&gt;
&lt;li&gt;Fork the upstream repository.&lt;/li&gt;
&lt;li&gt;Clone your fork and synchronize with upstream:
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; git clone https://github.com/&amp;lt;your-username&amp;gt;/CoMLRL.git
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020"&gt;cd&lt;/span&gt; CoMLRL
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; git remote add upstream https://github.com/OpenMLRL/CoMLRL.git
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; git fetch upstream
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; git checkout -b feature/&amp;lt;short-description&amp;gt; upstream/main
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; git fetch upstream &lt;span style="color:#666"&gt;&amp;amp;&amp;amp;&lt;/span&gt; git rebase upstream/main&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;Implement new features or fix bugs, updating documentation as needed.&lt;/li&gt;
&lt;li&gt;Open a pull request to the upstream repository and wait for review.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description></item><item><title>Multi-Agent REINFORCE</title><link>https://openmlrl.github.io/CoMLRL/docs/user-guide/reinforce-finetuning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openmlrl.github.io/CoMLRL/docs/user-guide/reinforce-finetuning/</guid><description>&lt;p&gt;REINFORCE optimizes the policy directly using sampled returns. An action-independent baseline can be included to reduce variance for REINFORCE methods. REINFORCE methods have been widely used to fine-tune LLMs because of their simplicity and effectiveness, e.g., &lt;a href="https://arxiv.org/pdf/2402.03300"&gt;GRPO&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2503.20783"&gt;Dr. GRPO&lt;/a&gt;, &lt;a href="https://openreview.net/forum?id=r1lgTGL5DE"&gt;RLOO&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2310.1050"&gt;ReMax&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2506.05183"&gt;TreeRPO&lt;/a&gt;, and &lt;a href="https://arxiv.org/abs/2501.03262"&gt;REINFORCE++&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="mareinforce"&gt;MAREINFORCE&lt;a class="anchor" href="#mareinforce"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In the LLM collaboration setting, REINFORCE can be extended to optimize each agent&amp;rsquo;s policy with joint returns from multiple agents.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MAREINFORCE&lt;/strong&gt;: The naive Multi‑Agent REINFORCE without a baseline can be expressed by:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="book-katex"&gt;\[ 
J(\theta_i) = \mathbb{E}_{\mathbf{o}_0 \sim \mathcal{D}, \mathbf{h}^\mathcal{G} \sim \mathbf{\pi}_{\mathbf{\theta}}}
\Bigg[\frac{1}{|\mathcal{G}|}\sum_{g \in \mathcal{G}} R^{(g)}_t \cdot \log \pi_{\theta_i}(a^{(g)}_{i,t}\mid h_{i,t})\Bigg].
 \]&lt;/div&gt;&lt;link rel="stylesheet" href="./CoMLRL/katex/katex.min.css" /&gt;&lt;script defer src="./CoMLRL/katex/katex.min.js"&gt;&lt;/script&gt;&lt;script defer src="./CoMLRL/katex/auto-render.min.js" onload="renderMathInElement(document.body, {&amp;#34;delimiters&amp;#34;:[{&amp;#34;left&amp;#34;:&amp;#34;$$&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;$$&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\(&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\)&amp;#34;,&amp;#34;display&amp;#34;:false},{&amp;#34;left&amp;#34;:&amp;#34;\\[&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\]&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\begin{equation}&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\end{equation}&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\begin{align}&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\end{align}&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\begin{gather}&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\end{gather}&amp;#34;,&amp;#34;display&amp;#34;:true}],&amp;#34;throwOnError&amp;#34;:false});"&gt;&lt;/script&gt;
&lt;blockquote class="book-hint success"&gt;
&lt;p&gt;These classes are derived from &lt;code&gt;comlrl.trainers.magrpo.MAGRPOTrainer&lt;/code&gt;. Interfaces for the trainer and configuration classes are the same as &lt;code&gt;MAGRPOTrainer&lt;/code&gt; and &lt;code&gt;MAGRPOConfig&lt;/code&gt;.&lt;/p&gt;</description></item><item><title>Changelog</title><link>https://openmlrl.github.io/CoMLRL/docs/dev/changelog/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openmlrl.github.io/CoMLRL/docs/dev/changelog/</guid><description>&lt;hr&gt;
&lt;h2 id="version-200"&gt;Version 2.0.0&lt;a class="anchor" href="#version-200"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;First release of CoMLRL.&lt;/p&gt;</description></item><item><title>Multi-Agent PPO</title><link>https://openmlrl.github.io/CoMLRL/docs/user-guide/ppo-finetuning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openmlrl.github.io/CoMLRL/docs/user-guide/ppo-finetuning/</guid><description>&lt;p&gt;PPO is a widely used policy gradient method that employs generalized advantage estimation to estimate advantages, reducing the high variance and long rollout times in Monte Carlo methods, e.g., REINFORCE. PPO has also been used for LLM fine-tuning, e.g., &lt;a href="https://huggingface.co/docs/trl/main/en/ppo_trainer"&gt;trl&lt;/a&gt;, &lt;a href="https://verl.readthedocs.io/en/latest/algo/ppo.html"&gt;verl&lt;/a&gt;, &lt;a href="https://llamafactory.readthedocs.io/en/latest/advanced/trainers.html#ppo"&gt;LLaMA Factory&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="ippo"&gt;IPPO&lt;a class="anchor" href="#ippo"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Independent PPO (&lt;a href="https://arxiv.org/abs/2011.09533"&gt;IPPO&lt;/a&gt;) optimizes each agent&amp;rsquo;s policy independently while using joint returns from multiple agents. Each agent maintains its own actor and critic, other agents serve as part of the environment. The policy objective is:&lt;/p&gt;</description></item><item><title>Multi-Turn Training</title><link>https://openmlrl.github.io/CoMLRL/docs/user-guide/multi-turn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://openmlrl.github.io/CoMLRL/docs/user-guide/multi-turn/</guid><description>&lt;p&gt;Many complex problems cannot be solved in a single turn. Agents need to interact with the environment to obtain useful feedback from other models or tools involved in the system, enabling iterative refinement and exploration of multiple solution paths.&lt;/p&gt;
&lt;h2 id="multi-turn-magrpo"&gt;Multi-Turn MAGRPO&lt;a class="anchor" href="#multi-turn-magrpo"&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;MAGRPO in the multi-turn setting (&lt;strong&gt;MAGRPO-MT&lt;/strong&gt;) forms a tree-structured rollout expansion where branches represent different joint responses (&lt;a href="https://arxiv.org/abs/2506.05183"&gt;TreeRPO&lt;/a&gt;).&lt;/p&gt;
&lt;p align="center"&gt;
 &lt;img src="./img/joint-tree.svg" width="400"/&gt;
&lt;/p&gt;
&lt;p&gt;In each episode, a task is sampled from the dataset to construct initial observations &lt;span class="book-katex"&gt;\( \mathbf{o}_0=\{o_{1, 0}, \cdots, o_{n, 0}\} \)&lt;/span&gt;&lt;link rel="stylesheet" href="./CoMLRL/katex/katex.min.css" /&gt;&lt;script defer src="./CoMLRL/katex/katex.min.js"&gt;&lt;/script&gt;&lt;script defer src="./CoMLRL/katex/auto-render.min.js" onload="renderMathInElement(document.body, {&amp;#34;delimiters&amp;#34;:[{&amp;#34;left&amp;#34;:&amp;#34;$$&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;$$&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\(&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\)&amp;#34;,&amp;#34;display&amp;#34;:false},{&amp;#34;left&amp;#34;:&amp;#34;\\[&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\]&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\begin{equation}&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\end{equation}&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\begin{align}&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\end{align}&amp;#34;,&amp;#34;display&amp;#34;:true},{&amp;#34;left&amp;#34;:&amp;#34;\\begin{gather}&amp;#34;,&amp;#34;right&amp;#34;:&amp;#34;\\end{gather}&amp;#34;,&amp;#34;display&amp;#34;:true}],&amp;#34;throwOnError&amp;#34;:false});"&gt;&lt;/script&gt; and histories &lt;span class="book-katex"&gt;\( \mathbf{h}_0=\{h_{1, 0}, \cdots, h_{n, 0}\} \)&lt;/span&gt; for all agents. At each turn, agents generate a group of joint responses &lt;span class="book-katex"&gt;\( \mathbf{a}^{\mathcal{G}}_t\gets\boldsymbol{\pi}^{\mathcal{G}}(\cdot|\mathbf{h}_t) \)&lt;/span&gt; from their current observation-action history &lt;span class="book-katex"&gt;\( \mathbf{h}_t \)&lt;/span&gt;, with each response initiating a distinct rollout. Agents receive joint rewards &lt;span class="book-katex"&gt;\( r^{(g)}_{t} \)&lt;/span&gt; for each response based on the accumulated history &lt;span class="book-katex"&gt;\( \mathbf{a}^{(g)}_{t} \in \mathbf{a}^{\mathcal{G}}_{t} \)&lt;/span&gt; and current action. &lt;strong&gt;Each rollout then evolves independently&lt;/strong&gt;, producing new joint observations &lt;span class="book-katex"&gt;\( \mathbf{o}^{\mathcal{G}}_{t&amp;#43;1} \)&lt;/span&gt; as the environment dynamics unfold and spawning more rollouts at the next turn &lt;span class="book-katex"&gt;\( t&amp;#43;1 \)&lt;/span&gt;. This process continues until the terminal turn is reached &lt;span class="book-katex"&gt;\( H \)&lt;/span&gt;.&lt;/p&gt;</description></item></channel></rss>